[
    {
        "ref": "/blog/2022-06-25-ecca-2022/",
        "title": "ECCA 2022",
        "section": "blog",
        "tags": [],
        "date" : "2022.06.25",
        "body": "Last week I was in Harrogate, UK for the 30th Environmental Criminology and Crime Analysis (ECCA) meeting. I haven\u0026rsquo;t been to an in-person conference since ASC 2019. Usually to help me focus, I take notes, and the ASC notes (linked above) were appreciated by some, so I decided to type up also my ECCA notes. Here are some of my thoughts/ what I learned grouped them into some key themes for a summary. *(Disclaimer: these are my notes, possibly incorrect, please do find the speakers for any questions/detail/clarification and if you find I wrote something incorrect just tell me and I shall amend!) *\nI grouped the talks into those about risky places, theory building, methodological advances, emerging crimes, policing, and covid + crime.\nRisky places Many of the talks discussed risky places for various crimes.\nVania Ceccato talked about crime in libraries, such as unpleasant/aggressive behaviour and theft. She described a conceptual model to investigate spatio-tempral nature of crime in libraries, and used 3D mapping in CAD software to think about micro environments and settings withinin the library and how these can promote or limit crime. For example the book supply area lends itself to theft, while the transition area (corridoor/toilet) lends itself to alcohol/ drug abuse. She mapped these crime hotspots within the library based on fieldwork inspections and email reports from the libraries\u0026rsquo; incident report system. She found the temporal patterns reflect routine activities of young people (who mostly use the library?) and what happens in the neighbourhood where the library is situated. In the Q\u0026amp;A Kate made the observation that there are more crimes more in the outside/entrance, and then less as you go into the library, and that Vania\u0026rsquo;s results show an \u0026ldquo;infiltration\u0026rdquo; into the library.\nMangai Natarajan focused on understanding drug trafficking via airports and applied ECCA techniques of risky facilities, crime places, crime script, crimes of specialised access, ESEER model. She analysed court transcripts of drug trafficking cases and looked at the fit with these approaches, for example modelling a J-curve of risky airports, or mapping a script and applying the ESEER (Easy, Safe, Excusable, Enticing, Rewarding) framework. Her results provide some insight into the role of airports in drug smuggling, and a development of some typologies, but she recommends a systematic safety audit to take this research forward.\nJames Hunter, Toby Davies, and Lucia Summers all discussed illegitimate businesses and their spatial concentrations. James Hunter focused on the UK, specifically car washes and nail bars which are involved with wage theft, modern slavery and human trafficking. His team used Google maps (62% of these businesses have presence on google maps/ other social media presence) and Google street view to identify these and then looked at what are features associated with location of these hand car washes e.g. visibility/attractiveness to customers, neighbourhood characteristics, etc. used POI database, census, etc to develop an informal economy index to identify where most likely those working in informal economy might be located. They developed this into a policy tool which makes predictive hand car wash index and brings up profile.\nToby and Lucia both looked at illegal massage businesses (IMBs) in the United States. Toby used betweenness estimates, measures of accessibility and permeability, and found that higher betweenness street segments are less likely, but higher local betweenness segments are more likely to host IMBs. In the Q\u0026amp;A John Eck suggested that rent prices might be something important to consider. Lucia went through reviews of these IMDs on an online platform and used this to try to develop a tool for law enforcement of a repository of sex trafficking signatures. She found that IMBs do not cluster as much as previous research suggests, and this might be due to her careful data cleaning which identified many duplicate venues, which seem to suggets that if one is shut down, it re-opens in the same place and same info (e.g. phone number). Again a question about landlords or to who the businesses are registered, but an issue is that they are registered to accountants who have many businesses registered to them, but are just the accountants.\nTheory building The first and last talk were both those that I think fit this bill. The conference opened with Shannon Linning discussing her new book written with John Eck: Whose \u0026lsquo;Eyes on the Street\u0026rsquo; Control Crime?. Following a re-reading of Jane Jacobs, she presented 3 turning points:\n⁃\tPrimary source of informal ctrl should be place managers (as well as/ instead of residents?) ⁃\tMovement of people is manipulated by outsiders from neighboutrhoods who build areas to fit their needs, so the movement of different resident types etc is more orchestrated by external people/companies than we\u0026rsquo;d think (eg by zoning, govt housing, building infrastructure, urban renewal, etc - creating population mobility through building transport infrastructure) ⁃\tConsider what is the most effective unit of analysis? - places - start at proprietary places, then build out form these (bottom up?) proprietary place \u0026gt; street \u0026gt; area\nAnother re-reading of books came from the presentation by Jason Roach, who delivered a very entertaining recap of psychology research from the lens of crime prevention, specifically considering influencing and nudges. Although revisiting old psychology textbooks is a bit risky (a few of the studies have since been discredited), he raised interesting points about incremental influence (which he also called nudge snooker) - the need to nudge people to a place where they can be better nudged, using the example of the 12-step programme of alcoholics anonymous to illustrate.\nA critical reading of the Home Office\u0026rsquo;s Beating Crime Plan was delivered by Scott Keay standing in for Ken Pease which demonstrated that despite this being called \u0026ldquo;the most comprehensive, far-reaching strategy of its kind yet devised\u0026rdquo; by our PM, it only included a small subset of the crime crime prevention initiatives shown to reduce crime by the What Works Crime Reduction Toolkit (which the home office paid for, by the way), and also included about 5 which show mixed findings or no overall change\u0026hellip; He ended with a call for action for criminologists, analysits, researchers, etc to:\n⁃\tincrease exposure to direct practice in their our areas of research ⁃\tongoing collaboration in crime reduction projects ⁃\twrite evidence-based crime reduction strategy\nThere was also a book recommendation thrown in for the honest politician\u0026rsquo;s guide to crime control. In the Q\u0026amp;A it was raised that there is probably a difference between what analysts produce and what gets published and the filtering that it goes through which is performed by the specialist advisors who are not experts and not elected. Therefore the much great work by analysts which does not make it to reports is not seen, and this is probably a large part of the problem.\nGraham Farrell presented on a part of his large body of work on the international crime drop and the security hypothesis, looking at adolescent involvement and continuance in offending. His thesis is that as crime dropped, offender age increased over time - so age crime curve also shifted - most of the crime drop is accounted for by the change in the teens. In 40s there is more offending - these are the people who were in their teens in the 80s - so we\u0026rsquo;re looking at cohort effects. He referenced the \u0026ldquo;Debuts and legacies\u0026rdquo; 2015 paper with Nick Tilly and Gloria Laycock. In 2012 the offenders whi started un their teens in the 80s are still offending because learned this in their teens when crime was easy. Thesis: Opportunities affect both adolescent-limited and persistent offending. In the Q\u0026amp;A someone raised whether it is possible that offending is the same but they are much better - so the data captures those who are caught. Because they grow up when committing crime is harder so they are better at it (when they do offend) and so they\u0026rsquo;re not in the UCR data because they\u0026rsquo;re not caught?\nThen the conference closed with what was one of my favourite talks, Andy Newton presenting a Framework for Analysing Crime Events in Time and Space (of course there\u0026rsquo;s the acronym: FACETS). His thesis is that all key environmental crim theories talk about crime events - something which happens at a particular place and a particular time - but what what is meant by place and crime? Questions to ask ourselves: what is temporal framework (how long does crime take/last?) and spatial framework (what is the extent of the place?). Building on dynamic/ static crimes, use this framework to think about different cases. Regarding how to think about time he suggested we revisit the work of Amos Hawley when we think about time, to think more about rhythm, tempo, and timing. Also to think about the denominator problem: if crime is dynamic then the denominator also has to be dynamic. He described also thinking around the 168 hour crime week and work using street sensor data as a dynamic denominator to measure this. Spatially he urged us to think of crime events using a fuzzy approach (eg bayesian approach) or think about the work of John Hipp on Egohoods. The Q\u0026amp;A had some interesting discussion around data quality with time element in that time recorded may not necessarily reflect when the crime happened. and whether to some degree are we talking about spatial and temporal measurement error? Or are you interested to understand the extent of the actual events? And also whether we should link sequences that might be 2 separate events? Like breaking into the house to get the key and then stealing the car - 2 events or 1 event?\nMethods for crime analysis I always learn a lot of neat methods stuff at ECCA, and sometimes the aim maybe isn\u0026rsquo;t methodological innovation but I tend to focus on that, so I may have some bias here\u0026hellip; Anyway, this year, Kim Rossmo presented a method of Bayesian analysis for GIS in search optimisation. Specifically focusing on searches for missing persons in wilderness search and rescue. Presented the case study of Bill Ewasku a hiker who went missing in a large national park. His search was informed by his mobile phone which pinged on tower and narrowed search, but still deployed 100s of personnel, SAR teams, dogs, horses, helicoper etc etc etc 1772 person-miles searched overall (they wear GPS trackers while searching). To help, Kim and team generated an optimal probability search map - combining 3 sources of information - range of cellphone tower, reception spots, and search radius. Bayes’ theorem provides a method for combining these different types of evidence to produce an optimal search strategy. This paper is actually published in the journal of search and rescue but recently the remains were found by hikers very closer to the peak area predicted, validating the probability map.\nSophie Curtis-Ham presented her PhD researc on Lessons Learnt linking offenders\u0026rsquo; mental maps for criminal spatial signatures. In her work she realised that police hold a lot of data about offenders\u0026rsquo; activity spaces on top of their home locations. So she asked the question: when building geographic profile, should we prioritise those with activity notes near the crime location, OR does it matter the activity type? - which activity nodes are people more likely to commit crimes near? This model in her paper published in Crime Science. She also produced an R package R package: gpsmartr which allows anyone to implement this on their own data. In her testing she found that differentiating between activity nodes improves predictions compared to when just treating all activity nodes as the same (and even better than just considering home location). In the Q\u0026amp;A it emerged that this could be used for forecasting crime, and there is potential application in correctional setting for offenders what are their risky places for future offending, and putting offender management strategies in place for these locations.\nPaul Ekblom and Herve Borrion both explored illegal waste dumping/ flytipping but from different methodologies. Paul Ekblom introduced the Crime Role Grid, a wider system mapping to identify and influence a diversity of stakeholders. He called it stakeholder mapping with crime layer. The idea is that you use it to collect and organise knowledge of agents in the system, anticipate the reaction of the various agents, design and develop resources. In this way crimes can be better understood from wider system-level view of agents, entities and causes. The crime role grid can be used with scripting, dependency mapping, or built into an interactive toolkit. Herve on the other hand was exploring an agent based modelling approach. He built an ABM to explore dynamics of fly tipping and hopes to build in crime prevention measures into ABM and calculate probability of different outcomes for each intervention which leads to getting caught (getting a fixed penalty notice). The point: with more granularity in simulation it is possible to recreate situational crime prevention framework that aligns with decision making model built for the offender in the ABM.\nWouter Steenbeek considered how we characterise near-repeat incidents using the Knox test, specifically how we define spatial and temporal ranges of interest. He argued that there is no clear definition for what is near repeat in the literature, there is no set definition for correct spatial and temporal bandwidth, or about the number of bands. To address this he presented the \u0026ldquo;Knox sloping-down index\u0026rdquo; (KSI), a scale from 0 to 1 which measures the intensity of your near-repeatedness. This measure showed how sensitive results are to choice of spatial and temporal bandwidth. Some questions to then consider: do we need to agree on bandwidth and bands to use KSI? Or do we need different measure (KSI is wrong) and so need different near repeat measure? In the Q\u0026amp;A it was suggested that if the aim is crime prevention, can this be understood as flexibility (depend on context) rather than vagueness? - so is it possible that KSI is a tool that best identifies where to target the intervention: find the place where KSI is best - and then use this to inform practice to reduce crime.\nWim Bernasco talked about spatial scale in crime location choice research in relation to crime location choice. What makes offenders choose a particular location to commit the crime? The answer is in the attributes of the particular location. Follow discrete choice models (utility maximisation) - these use individual data (not crime count). Common problem - not sure what unit of analysis is most appropriate. Issue of spatial scale has not really been addressed. Looked at 33 studies on crime location choice and collected unit of analysis, average size, number of units included and number of crime events included. Found larger units common, but more recent move to smaller units. Also carried out empirical data analysis to answer is there MAUP in crime location choice? Chenai snatching study - break down into grids (1500, 1000, 500m grids) instead of wards, to test MAUP. Found that differences are small but there is MAUP with the different size grids. There was then a question about whether this decision is hierarchical (ie you choose neighbourhood first then street then property) and if this can be integrated? - Wim sugested this can be a next step, possibly achieved with nested logit models to allow for looking at this.\nMatt Ashby compared harm-spots and hot-spots. He raised the case when we cannot be crime-specific, for example to answer the question: which nhood team should we allocate more officers to? or Which district should we prioritise for extra funding?. In this case we want an overview,but of course, a hotspot map of \u0026ldquo;aLL cRiMe\u0026rdquo; is something quite useless, and if one of our crime mapping students were to produce this, we would not mark them favourably. So then what is the answer? One alternative approach is to weight crime based on some measure of harm (e.g. days in prison equivalent). Matt showed 2 KDEs, one is count of crime and one weighted by harm (so 1 weighted and unweighted KDE) and compared rank of each cell based on count and rank of each cell based on harm. Overall correlation between crime and harm is high but the correlation of where hotspots are is low - so mapping crime count means you miss areas of high harm. For example the low count high harm areas: 2 of these are prisons.\nFinally, Jose Pina Sanches presented on the recounting crime project about measurement error in police recorded crime rates. We all know police data is flawed due to under reporting/ under detection. The question: how to define measurement error mechanism? - there will be systematic measurement error, negative, and multiplicative. What they did was compare victimisation survey and police data, then look at multiplicative error - plot along different equations, and see which one they are at. Since multiplicative looks best - this is how to define measurement error. Now can think about what measurement error is in models, then substitute measurement error term for the estimate. Then take log the multiplicative measurement error transforms into an additive error terms. This means that only intercept is biased - if you do this then the regression coefficient becomes entirely unbiased. To help others implement this, they created the rcme R package. As emerged in the Q\u0026amp;A the next step is to apply this to non-linear models.\nEmerging crimes There were a number of presentations about emerging crime types we need to better understand. For example, Ben and Amy Stickle presented their work about porch pirates. They note that between delivery and package retrieval is the most vulnerable time for parcel that is being delivered, and they analysed YouTube videos from doorbell cameras to find points for intervention. Their analysis identified multiple points for interventions (e.g. packages with a lithium battery warning essentially say \u0026ldquo;please steal me\u0026rdquo; and this should be covered up), and overall they advocate for security layering, making crime prevention approaches easy, and the need for collaboration from retailer, police, delivery people, and the consumer. In the Q\u0026amp;A it emerged that different companies have different levels of knowledge/understanding of this happening, and a possible future direction to interview offenders, as in the case of this crime type, the risk/reward/effort equation the reward is a gamble, which may have implication for who is offender/ their motivation/ etc.\nThere were a collection of \u0026ldquo;cyber\u0026rdquo; presentations. Marianne Junger discussed preventive strategies of victims to avoid fraud victimisation, asking people \u0026lsquo;what did they notice that wasn\u0026rsquo;t right that stopped them from losing money/ paying?\u0026rsquo; for an online scam. Majority (67%) either recognised the fraud maybe because of the process (bank doesn\u0026rsquo;t send email like this) or because it was in media/news. 25% noted a mistake by fraud (like \u0026lsquo;i dont have children\u0026rsquo; or \u0026lsquo;i dont use that bank\u0026rsquo;) and other reasons included private knowledge (e.g. my daughter would never ask for money, my boss doesn\u0026rsquo;t correspond in this way, etc), general distrust, or (my favourite) sticking to strong principles (e.g. \u0026lsquo;i never trust english speaking person i do not know personally\u0026rsquo;). There were also some near misses, where the person clicked the link but it was not working, or changed their login details just in time. Overall fraud crime prevention advice: knowledge about scams is important - but strategies should be different for different types of fraud. The Q\u0026amp;A pressed the methodology of asking for the most memorable fraud, and recall bias. Anita Lavorgna talked about harmful sharenting which apparently means oversharing of possibly sensitive information by parents or guardians about children/grandchildren/ this sort of thing. Asier Moneva presented on hackers\u0026rsquo; \u0026ldquo;journey to crime\u0026rdquo; - defining travel to crime as achieving the task with the least effort. In the experiment, 70 software / IT security students were tasked to carry out some attack on a website and obtain the admin password. To operationalise efficiency, they collected keystroke data and built a cyber kill chain (like crime script for cyber crime) carrying out sequence analysis to produce hacking efficiency measure. Finally, Tom Meurs from Uni Twente explored how attackers using ransomware determine how much ransom to ask for, and found that the more effort was required for the attack and the yearly revenue of the target, the higher the ransom was, essentially: ransom is a function of effort and opportunity.\nPolicing and crime prevention Philipp Dau looked at police presence and survival time of crime-free windows in cases of hotspot policing. He looked at reported crime in Belgium, and have GPS data from police vehicles (cars and motorbikes) which left ~46,000 policeable crimes (eliminated indoor crimes eg DV) 201 vehicles tracked - 127 call response units and 36 motorbikes. Survival curve of hotspots by time spent in the area, and separeatly considered all streets, hotspots, and hottest hot spots. For all streets shorter visit had longer survival time, but hotspot longer visit was better.\nSam Lantgon and Tim Verlaan presented their work looking at demand for emergency police services in the Netherlands. They were looking at 112 calls to inform staffing/ training/ safety wellbeing/ and answer what is scale and composition of demand in the Netherlands? And a technical point: how to measure this demand? Their study is also pre-registered here. After some recoding to harmonise Dutch categories with the American ones used by Jerry Ratcliffe in his paper. They find that 34% of calls were for crime, considering time on scene 40% were crime related. Health calls - while fewer (6%) take a long time (11%) and median time on scene is by far highest for health calls (75 min). They also looked at by number of units attending - ones with flocking (many units going to calls) denote the more serious incidents (this also explains high median time for time on scene for health e.g. where CPR required) Suggest there are 4 reasonable ways to calculate time - will use all 4 measures and see which one has impact.\nWildlife There was also a session on wildlife crime. Andrew Lemieux presented work using SARA process to implement problem oriented wilderness protection to stop people from using \u0026rsquo;elephant paths\u0026rsquo; - unofficial trails that lead people into the conservation area (eg where deer have their babies). There are numerous problems: fence jumping, dog off lead, littering, elephant paths. Used strava data to show where people\u0026rsquo;s paths were - evidence of the unofficial movement beyond what landowner was seeing. Chose to focus on 2 elephant paths where people were accessing areas that needed protection. Intervention: signs to assist compliance, cover up trails, remove trail from open street map. Evaluation: people kept jumping fence. But where paths were closed down, people stopped using them, where it wasn\u0026rsquo;t, people continued to use. Q\u0026amp;A: \u0026ldquo;provocative counter measures\u0026rdquo; - vs when people don\u0026rsquo;t know they are being manipulated these can lead to more successful interventions.\nJen Mailley presented on her what works to reduce wildlife crime document for UNDOC. She asked: What is evidence base to decrease wildlife crime? At the moment - not super known. The evidence base currently is patchy, for example a systematic review paper by Dorothea Delpech ended up with only 5 studies that met criteria. But the next step is to build a conservation evidence database.\nFinally, Sarah Gluszek from Fauna \u0026amp; Flora International - a conservation charity presented on situational crime prevention guidance for illegal wildlife trade. She with others (e.g. Julie Viollaz, who also being at UNDOC is working also with Jen on the project above) made a toolkit, translated into different languages, and simplified to make understandable to help implement crime prevention in these diverse settings.\nCovid and crime There were two covid+crime presentations, one about spatial variation in the impacts of covid on crime by Martin Andresen. He used structural break test for analytical methods, and found differential Moran\u0026rsquo;s I to identify local changes - specifically found an increase in criminal activity in the poorer areas.\nThe other paper was by Patricio Estevez who looked at covid + personal victimisation using Mexico\u0026rsquo;s crime survey. For methods, he used societal growth curves and focused on repeat victimisation, looking at incidence, prevalence and concentration. Overall found a reduction in incidents compared to the trend (counterfactual).\nPosters There were also some brilliant posters, but I did not take notes on them so I apologise for that one\u0026hellip;\nThe end Overall like always I learned so much and feel super inspired. I didn\u0026rsquo;t present this year but I hope that I will have something I\u0026rsquo;m proud of next year. I hope I didn\u0026rsquo;t misrepresent anyone\u0026rsquo;s work in these notes, but if yes, just email me to let me know!\n"
    }
,
    {
        "ref": "/blog/2022-01-19-tips-for-academic-writing/",
        "title": "Tips for academic writing",
        "section": "blog",
        "tags": [],
        "date" : "2022.01.19",
        "body": "Writing is a big part of my job. I write lectures, papers, book chapters (\u0026hellip;book!), presentations, grant applications, blog entries, feedback, emails, reports, reference letters, and probably a bunch of other things. Yupp, a lot of my job involves writing. But just because I write a lot, doesn\u0026rsquo;t mean I\u0026rsquo;m any good at it. I can\u0026rsquo;t be that bad, I do have citations of my work, but there is always scope to improve. So how to go about improving?\nLike any good nerd, I decided that reading up on the topic is the best solution. I actually started thinking about a need to improve my writing after a friend recommended me the book Elements of Story in March 2021. Since then, I expanded my reading list with Strunk and White\u0026rsquo;s classic reference book Elements of Style, Howard Becker\u0026rsquo;s Writing for Social Scientists, and Helen Kara\u0026rsquo;s Writing for Research.\nA key ingredient for better writing suggested by all these books is editing. Everything comes down to editing and revising your writing over and over and over again. I also found some specific issues which help with key questions such as: where to start?, how to be clear? and how to be engaging? I detail these key takeaways below.\nDISCLAIMER: This is by no means a summary of the content of these books, it\u0026rsquo;s just some notes for my future self (and anyone interested) on what I thought I would focus on. For anyone with the interest and time, I would recommend reading these books for themselves, as they all offer unique and helpful insights into different but related elements of the writing process.\nSo in no particular order, here are some key things I\u0026rsquo;ve learned.\nJust write Writing for Social Scientists and Writing for Research are both clear on this.\n \u0026ldquo;If you want to be a good writer, the first thing you have to do is write.\u0026rdquo;\n This can be daunting, and many people will priorities other tasks (cleaning the house, baking cookies) because they find it hard to get started.\nOne reason might be that academics expect their writing to be flawless at the moment they put pen to paper (fingertip to keyboard?). This is a myth. The sooner you allow yourself to just write, without the pressure for it to be perfect, the sooner you can start to actually write. In Writing for Social Scientists Becker says to just \u0026ldquo;write anything\u0026rdquo;:\n \u0026ldquo;Once you know that writing a sentence down wont hurt you, know it because you have tried it, you can do what I usually ask people to try: write whatever comes into your head, as fast as you can type, without reference to outlines, notes, data, books, or any other aids. The aim is to find out what you would like to say.\u0026rdquo;\n In Writing for Research, Helen Kara recommends trying out the technique of \u0026ldquo;Freewriting\u0026rdquo;. In freewriting you set a prompt related to your project, and then write for ten minutes about it, writing anything that comes to your mind. It doesn\u0026rsquo;t have to be good, it just has to be written.\nOnce you have started writing, it is much easier to continue on.\nWhere do I start? Great, you are ready, you can start writing. But where to start?\nMy students tend to think writing is a linear process, so I find myself explaining this in dissertation meetings every year: there is no need to write in the order in which the paper will be read.\nFor data analysis coursework, I usually advise students to start with their research question, their methods, and then their results. This fits with Kara\u0026rsquo;s observation in Writing for Research that:\n \u0026ldquo;[writing] often starts in the middle.\u0026rdquo;\n But there is really no wrong place to start writing a paper. In Writing for Social Scientists, Becker suggests to:\n \u0026ldquo;Do whatever is easiest first.\u0026rdquo;\n Another approach is to tailor your writing to the environment in which you will be working. In Writing for Research, Kara gives the example of working on a train: with no access to internet or reference material, she will write a section which doesn\u0026rsquo;t need these resources. She also suggests leaving notes such as \u0026ldquo;reference this later\u0026rdquo; or \u0026ldquo;link this section to the next\u0026rdquo; as a solution not to get stuck, but be able to continue the flow of writing.\nThe first draft of everything is shit I had originally seen this quote attributed to Hemingway but I saw this quoted again in Writing for Research as \u0026ldquo;The first draft is always shit.\u0026rdquo; said by Elmore Leonard^[Side note: there is an actual book called \u0026lsquo;Hemmingway did not say that\u0026rsquo; for quotes misattributed to Hemingway apparently\u0026hellip;].\nThis snappy line drives home the point that editing is a crucial part of the writing process. Writing is not just the creation of the content, but the rearranging and deleting of that content as well. In Elements of Style Strunk and White say:\n \u0026ldquo;Revising is part of writing..\u0026rdquo;\n In Writing for Social Scientists, Becker explains that in undergraduate education, students work to tight deadlines^[often self imposed: if you are a procrastinator, I recommend a full read of Helen Kara\u0026rsquo;s Writing for Research for helpful tips]. This means that the first draft is often what is handed in, and this puts us into a bad habit of expecting a first draft to be good. Splitting with this notion will greatly help your writing.\n \u0026ldquo;Knowing that you will write many more drafts, you know that you need not worry about this one\u0026rsquo;s crudeness and lack of coherence. This one is for discovery, not for presentation.\u0026rdquo;\n Understanding this is a big step in moving from the writing process typically used for undergraduate essays to producing better writing, at all stages.\nThe schedule for editing can vary. Ideally, you want to leave as much time between writing and editing as you possibly can. Becker describes his process in Writing for Social Scientists where he writes over summer, then edits throughout the semester, when demands on his time are greater (i.e. teaching). Helen Kara gives this general guideline:\n \u0026ldquo;You need to leave your draft alone for a while so you can come back to it with fresh eyes. A month is good; six weeks is better.\u0026rdquo;\n How many iterations your paper will go through will vary. Based on my reading of Writing for Social Scientists it sounds like an upper bound on the number of revisions may not exist. More tangibly, Writing for Research suggests three drafts (although this is qualified as at least three drafts):\n \u0026ldquo;The first draft where you churn out the words, the second where you knock it into shape, and the third where you review your word choices, grammar, and structure.\u0026rdquo;\n And you don\u0026rsquo;t have to be alone in the editing process either. You can make use of your network of colleagues, friends, supervisors, mentors, etc to help with this process.\nEditing really is the key remedy to almost all issues of writing. It seems so simple, but Becker has found:\n \u0026ldquo;What the students accept less easily is that, however long it takes, such detailed editing is worth doing.\u0026rdquo;\n It really is. If you take away anything from reading this it\u0026rsquo;s this: EDIT YOUR WRITING !!!\nGive away the ending Student essays (and many academic papers) often start with vague introductions which don\u0026rsquo;t really say anything, and certainly do not contribute to the reader\u0026rsquo;s understanding of their argument. In Elements of Story Flaherty calls such an introduction the \u0026ldquo;um lead\u0026rdquo;:\n \u0026ldquo;They [writers] often spend the initial paragraphs clearing their throats (\u0026hellip;) This Um lead is not useless. The very act of writing helps a person sharpen her thoughts. But when she does arrive at the actual heart of the story, she must be sure to scroll up to the top and lop off those two and a half paragraphs of \u0026lsquo;Um\u0026rsquo;.\u0026rdquo;\n So while all this writing fluff helps organise your thoughts, it should be cut out in the editing phase.\nThis is echoed in Writing for Social Scientists:\n \u0026ldquo;Many social scientists (\u0026hellip;) think they are actually doing a good thing by beginning evasively. They reveal items of evdidence one at a time, like clues in a detective story, expecting readers to keep everything straight until they trimphantly produce the dramatic concluding paragraph \u0026hellip;I often suggest to these would-be Conan Goyles that they simply put their last triumphant paragraph first, telling readers where the argument is going\u0026hellip;\n And to those who are reluctant to give away the ending, for fear of the reader not making it to the end, Becker reassures us that \u0026ldquo;scientific papers seldom deal with material suspenseful enough to warrant this format\u0026rdquo;.\nGet to the point Related to the idea of \u0026rsquo;throat clearing\u0026rsquo; is the use of abstract, meaningless words, sentences, and even paragraphs which do not contribute to the reader\u0026rsquo;s understanding, and may even work actively against this. To use definite, specific, and concrete language is the a key principle of composition in Elements of Style:\n \u0026ldquo;If those who have studied the art of writing are in accord on any one point, it is this: the surest way to arouse and hold the reader\u0026rsquo;s attention is by being specific, definite, and concrete.\u0026rdquo;\n My students frequently complain about not having enough words to express their thoughts within the essay\u0026rsquo;s word count constraints. However these same students use up hundreds of words on indirect and vague sentences which contribute nothing to the paper\u0026rsquo;s argument. Once you delete these words, phrases, sentences, and even paragraphs, there will be plenty of words to spend on making clear arguments.\nSo why do we use these unnecessary words? Becker offers:\n \u0026ldquo;We scholars use unnecessary words because we think, like the student in my seminar, that if we say it plainly it will sound like something anybody could say, rather than the profound statement only a social scientist could make.\u0026rdquo;\n But it is better to be clear than to be fancy. Helen Kara says in Writing for Research:\n \u0026ldquo;In academic research you will have a word count, but it\u0026rsquo;s a maxiumum rather than a target. You won\u0026rsquo;t lose any marks for saying everything you need to say in fewer words, and you will always gain marks for clarity.\u0026rdquo;\n There are many reasons why we might have a build up of useless words. Becker raises two common examples:\n1: Sometimes authors recognise that readers may disagree with them, and their insecurity adds unnecessary justifications. For example, he offers the sentence: \u0026ldquo;It is important to make the theory\u0026rsquo;s steps explicit.\u0026rdquo; as something redundant; it is obvious this is important, because only important things should be in your paper. Such sentences can be removed.\n2: Social science writing loves sentences with three predicate clauses. For example: \u0026ldquo;This book excites our curiosity, gives us some answers, and convinces us that the author is right.\u0026rdquo; Like Becker, I am also guilty of using this form whether I have three things to say or not. This forced third unnecessary word does no work. \u0026ldquo;It doesn\u0026rsquo;t further an argument, state an important qualification or add a compelling detail. (See?)\u0026rdquo;. So don\u0026rsquo;t force that third unnecessary word, when two (or even better: one) is enough.\nSo how to address this? Be ruthless in your editing. Strunk and White say it best in Elements of Style :\n \u0026ldquo;Omit needless words. Vigorous writing is concuse. A sentence should contain no unnecessary words, a paragraph no unneccessary sentences \u0026hellip;\u0026rdquo;\n Becker is not stressed, he says:\n \u0026ldquo;No harm. It comes out in the editing.\u0026rdquo;\n Avoid the \u0026ldquo;classy locutions\u0026rdquo; Related to the use of too many words, but something which deserves its own theme is to avoid what Becker calls (I think ironically?) \u0026ldquo;classy locutions\u0026rdquo;. He describes having edited the work of a graduate student, going through and simplifying phrases. For example, he replaced \u0026ldquo;could afford not to have to be concerned with\u0026rdquo; with \u0026ldquo;needn\u0026rsquo;t worry\u0026rdquo;, and \u0026ldquo;unified stance\u0026rdquo; with \u0026ldquo;agreement\u0026rdquo;, and so on. Upon seeing this, the graduate student agreed this was shorter, and clearer, but insisted that \u0026ldquo;the other way was classier\u0026rdquo;. She said:\n \u0026ldquo;Somewhere along the line, probably in college, I picked up on the fact that articulate people used big words, which impressed me\u0026hellip;\u0026rdquo;\n And this is something I see throughout academic work. So it is really, really, REALLY important for me, that I can say this (quoting Becker):\n \u0026ldquo;None of these classy locutions mean anything different from the simpler ones they replace. They work cereonially, not semantically.\u0026rdquo;\n The point is also picked up in two principles of Elements of Style:\n1: \u0026ldquo;Do not overwrite\u0026rdquo;:\n \u0026ldquo;Rich, ornate prose is hard to digest, generally unwholesome, and sometimes nauseating.\u0026rdquo;\n and 2: \u0026ldquo;Avoid fancy words\u0026rdquo;:\n \u0026ldquo;Avoid the elaborate, the pretentious, the coy, and the cute. Do not be tempted by a twenty-dollar word when there is a ten-center handy, ready and able.\u0026rdquo;\n The point is echoed by Flaherty in Elements of Story:\n \u0026ldquo;Banish long, obscure or Latinate words, too. \u0026ldquo;Housing complex\u0026rdquo; and \u0026ldquo;residential development\u0026rdquo; are each two words, but the first is better. The number of syllables matters as much as the number of words.\u0026rdquo;\n We get attached to locutions and formats, but it is important to reflect on what do they really add to your paper, and if nothing, then the remedy takes us back to the point about editing. Remember: delete the excess.\nBesides classy locution, I wanted to add a point here about writing with numbers that is mentioned in Elements of Story: avoid needless complexity. Consider this example of miniature Eiffel Tower figurines:\n \u0026ldquo;The nation may have imported 340,684 miniature Eiffel Towers in the last year, but the reader will more easily absorb a round number \u0026ldquo;about 340,000.\u0026rdquo;\n When we talk about communicating statistics, we often present estimates surrounded by uncertainty, but print these estimates to 3 decimal places. This sends a message of certainty where there is none. Not to mention, sometimes decimals detract from what the numbers actually mean. The average number of burglaries per neighbourhood might be 3.752 burglaries per 100 households, but there is no such thing as three quarters of a burglary, so we might as well round up to a whole number, which better corresponds to what we are quantifying in the first place.\nAnother way to reduce complexity is by interpreting numbers for your reader. When giving this feedback to students I borrow a phrase from Blastand and Dilnot\u0026rsquo;s book The Tiger That Isn\u0026rsquo;t, which simply asks:\n \u0026ldquo;Is that a big number?\u0026rdquo;\n When presenting numbers, I expect to have some context as a reader to help me interpret the numbers presented. Flaherty in Elements of Story suggests a way to do this by finding smart ways to compare magnitudes:\n \u0026ldquo;\u0026rsquo;[A] million is to a billion as 11 days to 31 years\u0026rsquo;, and use familiar referents, like \u0026lsquo;A rhino weighs as much as 20 Michael Phelpses\u0026rsquo;.\u0026rdquo;\n Whether presenting numbers or writing about concepts and theory, keep things simple, define and interpret things in their context, and impress the reader with clarity, rather than fancy language.\nShow rather than tell \u0026ldquo;Show rather than tell\u0026rdquo; is possibly my most commonly given feedback on student essays. When I read a sentence like \u0026ldquo;There is strong support for this argument\u0026rdquo;, I immediately think these words could have been used to present evidence of this support, rather then tell me to trust their word for it. In Elements of Story, Flaherty suggests that:\n \u0026ldquo;\u0026lsquo;Show, don\u0026rsquo;t tell\u0026rsquo; works because showing is a telling, just a more vivid one.\u0026rdquo;\n In academic writing, you should be showing with evidence that the argument is supported more often than telling the reader to take your word for it. Becker raises this point in Writing for Social Scientists with the example sentence: \u0026ldquo;There is a complex relationship between A and B\u0026rdquo;. This sentence says nothing - it is a place holder, which means nothing in itself, and needs to be illustrated. Instead, we can better present our arguments by describing what we have studied in specific detail.\nOnce this evidence is presented, there is no longer a need for a sentence like \u0026ldquo;There is strong support for this argument\u0026rdquo;, because this has been illustrated. This removes the need to explain too much. In Elements of Style Strunk and White advise:\n \u0026ldquo;It is seldom advisable to tell all.\u0026rdquo;\n Once you\u0026rsquo;ve presented the evidence, and put your numbers and arguments into context (abstaining from \u0026lsquo;fancy locutions\u0026rsquo;) the reader will arrive to your conclusions on their own.\nTell a story Another issue I see in essays is the loss of the main argument in a sea of other writing. Often students are great at listing what they have read, and their data sources, and their analysis, and even their conclusions, but then miss the opportunity to tie everything together, and present it as a cohesive whole. One way to think about this is to think about telling stories.\nBecker observes in Writing for Social Scientists:\n \u0026ldquo;Perhaps as a result of my experiences in teaching, I have become more and more convinced of the importance of stories - good examples - in the presentation of ideas.\u0026rdquo;\n Stories in the form of \u0026ldquo;good examples\u0026rdquo; could be told by including actors Elements of Story:\n \u0026ldquo;Whatever your subject, give it a human face if you can.\u0026rdquo;\n This probably more relevant to journalists, but may apply to some academic papers, or conference presentations as well. Flaherty suggests to put actors, not just talkers in your story. For example: \u0026ldquo;The tight housing market is also the old lady evicted with her four cats\u0026rdquo;. But, while such a \u0026ldquo;face\u0026rdquo; might not always be possible, the importance of having a focused story is relevant to any academic paper. Flaherty suggests that all stories are divided into two parts, the action and the commentary. It is important to balance these, giving enough detail (context) but maintaining profluence - the sense that things are moving, getting somewhere, flowing forward:\n \u0026ldquo;Keep the boat moving and, at the same time, describe and explain the scenery to its pasenger.\u0026rdquo;\n To keep your story focused, you might think about it in terms of the trunk and the branches. If your paper is a tree, the main argument is the trunk, and any details are the branches. To make writing clear, don\u0026rsquo;t leave your reader wondering: \u0026ldquo;What\u0026rsquo;s the trunk and what\u0026rsquo;s the branch?\u0026rdquo;.\nThis is especially common to see in literature review sections. The source of the problem is highlighted by Becker in Writing for Social Scientists: \u0026ldquo;Scholars must say something new while connecting what they say to whats already been said, and this must be done in such a way that people will understand the point\u0026rdquo;. So while we must engage with what had been done and recognise that \u0026quot;[o]ther people have worked on your problem, or problems related to it \u0026hellip; you just have to fit them in where they belong\u0026rdquo;, we must take care, as .\u0026rdquo;..paying too much attention to [the literature] can deform the argument you want to make. It might bend your argument out of shape\u0026quot;. The literature review must engage with existing scholarship in the area, but this should be all done with the aim to motivate the main argument of the paper. To achieve this, Becker suggests:\n \u0026ldquo;The logic of your argument should guide your paper.\u0026rdquo;\n How to do this? Flaherty in Elements of Story suggests that the struggle over trunk and branch is often one of relative length.\n \u0026ldquo;Branches are slender, trunks are thick.\u0026rdquo;\n If a main idea is set out in three paragraphs, the qualifications should get about one paragraph. If the main idea is one paragraph the qualifications only one sentence. The further the topic is from the heart of the story (your main argument), the fewer words it merits.\nKeeping branches slender is achieved most effectively through exclusion.\n \u0026ldquo;To write is to choose, which is to exclude. (\u0026hellip;) Choose your main theme and position it, uncrowded, at centre stage.\u0026rdquo;\n You choose a theme, an argument, a focus which will guide the writing. This doesn\u0026rsquo;t mean to ignore all the unchosen themes, you can nod to these. But devote most of your space to the big focus.\nUse active voice over passive I have heard this writing advice repeatedly. Do not use the passive voice. Always use active voice. It makes for stronger writing. But it wasn\u0026rsquo;t until I read the arguments made in Writing for Social Scientists that it really clicked for me why this is important.\n \u0026ldquo;Active verbs almost always force you to name the person who did whatever was done (\u0026hellip;). We seldom think that things just happen all by themselves, as passive verbs suggest (\u0026hellip;) Sentences that name active agendas make our representations of social life more understandable and believable. \u0026ldquo;the criminal was sentenced\u0026rdquo; hides the judge, who we know, did the sentencing and not, incidentally make the criminals fate seem the operatoin of impersonal forces.\n Specifically for social science writing this is interesting. At a conference once, Marcus Felson suggested that many sociologists think as if people didn\u0026rsquo;t have bodies, in which they do the living, perceiving, acting, etc. Relatedly, describing social concepts in passive tense removes agency. Becker explains:\n \u0026ldquo;One problem has to do with agency: who did the things that your sentence alleges were done? Sociologists often prefer locutions that leave the answer to that question unclear, largely because many of their theories don\u0026rsquo;t tell them who is doing what.\n If you say for example: \u0026ldquo;deviants were labelled\u0026rdquo; , you don\u0026rsquo;t have to say who labelled them. And this issue is substantial:\n \u0026ldquo;That is a theoretical error, not just bad writing. (\u0026hellip;) If you leave out the actors, you missstate the theory.\u0026rdquo;\n This revelation really drives home the point to prefer active over passive voice in academic writing.\nFinal thoughts It was good to read a little about writing, and learn some techniques to implement going forward. I\u0026rsquo;m sure there are many other books I could have learned from, and I look forward to finding more resources and eventually building up even more confidence in this area in the future, but for now, taking these points on board will definitely improve my writing. Not just the quality of the writing, but my experience of the process, and hopefully my productivity as well. And I hope that it is useful for others who want to improve their writing. Now let\u0026rsquo;s start this editing process rather than ramble on trying to reach some conclusion. As Strunk and White say:\n \u0026ldquo;Omit needless words!\u0026rdquo;\n "
    }
,
    {
        "ref": "/blog/2021-11-04-peak-s-altitude-x-conference/",
        "title": "Peak's Altitude X conference",
        "section": "blog",
        "tags": [],
        "date" : "2021.11.04",
        "body": "I had the chance to attend an in-person conference organised by Peak: Altitude X. The conference aimed to bring together business and data communities, and I also snuck in as an undercover academic.\nMy sneaking in was facilitated very kindly by Her+Data Manchester, so I want to say a big big thank you for this opportunity!\nHighlights I wanted to quickly write up the main highlights (besides catching up with people in-person) for me.\nLorraine Heggessey - Lessons learnt from a career in broadcasting Lorraine Heggessey spoke about her career and reflected on the process of making big decisions. She said that when making big decisions there will always be people who find reasons to not make that decision. Reasons why it won\u0026rsquo;t work out.\n \u0026ldquo;With big decisions, there will always be reasons not to.\u0026rdquo;\n But that doesn\u0026rsquo;t mean that you should not do it. She said after an experience of taking a large risk early in her career (filming secretly in Moscow and Leningrad about mistreatment of Jewish people in the Soviet Union), which payed off, she is prone to take risk, make big decisions, and lead by doing.\nI also learned that those 10-minute segments at the end of David Attenborough nature shows exist, because they were initially created so that the BBC can fill a full hour slot despite having made 50-minute long programmes.\nLeanne Fitzpatrick - Why data alone isn’t enough Leanne Fitzpatrick is head of data science at the Financial Times. Her talk focused on the importance of context, not just around data, but also around the building and implementation of our models. This is really important because we talk a lot about data quality, constraints of the data, but this needs to be considered and monitored also for the model.\n \u0026ldquo;We need to consider the constraints the model was built under.\u0026rdquo;\n We should ask questions like: Is the model experimental? Was it built in a short amount of time? - in that case, it might need to be closely monitored. Leanne set out a list of the \u0026ldquo;dimensions of confidence\u0026rdquo; which should be considered around the model, which includes things like the whether there is monitoring in place, the \u0026ldquo;newness\u0026rdquo; of the model, model latency, and ethical constraints.\nShe concluded that we must communicate the parameters of uncertainty around our models when presenting our predictions, in order to be transparent and responsible data scientists.\nConversation with Christopher Wylie Definitely the most crowded of the talks, the conversation with data privacy expert and Cambridge Analytica whistleblower Christopher Wylie, began with a discussion on the origins of Cambridge Analytica, and covered his career trajectory, while raising very important points about what is privacy.\nHe mentioned how tech, especially early on was all about disruption, but it now appears some of the things which were being disrupted are essential societal elements, like social cohesion.\n \u0026ldquo;Society is aware that there is \u0026ldquo;a problem\u0026rdquo; but there is no clarity on what the problem is.\u0026rdquo;\n The key issue is that of regulations. With innovation, it is inherent that regulation comes after the fact. He mentioned the example of how FAA regulations came much later than the Wright brothers, and pharmaceutical regulations were toughened only after the Thalidomide scandal. But here, regulators are approaching services like Facebook as a service, but Wylie encouraged us to think about these as we would engineers. When regulating engineering products, like a bridge, there are health and safety checks. This should be applied to the virtual and tech spaces as well, to safeguard people\u0026rsquo;s privacy in the long run.\nFinal thoughts Problems in the data science world overlap very much with problems in the criminology/ crime science world. There are many opportunities to think about which translate directly. For example, thinking about virtual spaces as designs with architecture. Christopher Wylie mentioned that the virtual infratructure that is constructed is what makes people behave the way they do. The curation algorithms which promote extreme content facilitate radicalisation for example. And so it might be an idea to consider CPTED ideas and principles to design safe spaces online, as well as in the physical world.\nAnd as an experience, I was really comfortable being back to an in person conference actually. Unfortunately I couldn\u0026rsquo;t stay for the drinks reception after, but I\u0026rsquo;m looking forward to more of these as we recover from COVID-19 in the future.\n"
    }
,
    {
        "ref": "/blog/twitter-by-police-officers-in-urban-and-rural-contexts-in-sweden/",
        "title": "The Use of Twitter by Police Officers in Urban and Rural Contexts in Sweden",
        "section": "blog",
        "tags": [],
        "date" : "2021.10.14",
        "body": " This blog post is about the full paper: Ceccato V, Solymosi R, Müller O. The Use of Twitter by Police Officers in Urban and Rural Contexts in Sweden. International Criminal Justice Review. October 2021. doi:10.1177/10575677211041926 which can be accessed here: https://journals.sagepub.com/doi/10.1177/10575677211041926.\n Together with Professor Vania Ceccato from KTH and her student Oskar Müller we set out to explore how Twitter profiles of police in Sweden build “exhibitions” of policing for the public to view and interact with. We figured as people\u0026rsquo;s lives are increasingly influenced by online content, this will apply also to their \u0026ldquo;crime talk\u0026rdquo;: the process by which members of the public make sense of information around them relating to crime.\nWe collected 14,583 Tweets made over the course of just under 8 years by 20 accounts listed on the official Police Sweden website which belong to our study area “Region South” (Region Syd). Of these, 5 were official accounts representing a force (for example, @polisen_malmo is the Twitter account of the Malmö police force) and 15 personal accounts of individual police officers use social media (with personal accounts) while clearly identifying themselves as members of a police organization (as well as being listed on the official website). We further distinguished between accounts which police urban and rural areas.\nWe found that while official accounts stick closely to the guidelines, personal accounts deviate more, however personal accounts have greater engagement from the public (measured as likes and retweets). These online \u0026ldquo;exhibitions\u0026rdquo; of policing social media may inform \u0026ldquo;crime talk\u0026rdquo; more, and this might be especially important in rural contexts, where face to face interaction is likely less frequent than in urban areas.\nOfficial guidance We looked first at the Swedish Police Authority guidelines on social media use which states the main purpose is for such things as:\n sharing information, calling for support, warning messages, reflective of the organization, recruiting, establishing/ reinforcing cooperation with stakeholders and civil society, and for putting a face on the police, humanizing the police authority, supporting each other in the organization.  It further states that users should:\n “Avoid value-laden words and words that may be perceived as derogatory or offensive - be objective,” and “Show humility and respect, avoid irony, an employee in the police should never express himself condescendingly or have an attitude that can be perceived as offensive or superior,” “Be careful with humor, as this is something very personal that can easily be misunderstood” and “The police should not link to any other organization” and photographs should be avoided, “Photographs or pictures of minors must be approved by the guardian” (SPA, 2016, pp. 10–11).  With this in mind, we then identified what were the key themes which Tweets discussed.\nThemes We identified the following themes:\nCrimes, facts and dialogue Tweets from official accounts mostly focused on a priority to reveal “the facts”, such as the most common types of crime problems, or notification of individual crime events. For example:\n Detonation at [LOCATION] police station shortly after 5. The detonation caused minor damage to the door to the property but fortunately no injuries (urban, official)\n These Tweets present raw information, and do not offer any interpretation or discussion of the topics. In contrast, in personal accounts it was more common with examples of a dialogue being established between police officers and the public:\n This is how we should work against #organizedcrime also in our local police areas (rural, personal)\n We find that personal accounts present information in a way that shows dialogue (e.g. responding to the “crime talk” of the public by addressing an incorrect assumption) and demonstrate their perspective (using the word “should”).\nUse of language All Tweets from the official accounts in our sample adhere to the national guidelines, as they mostly fall within the “facts” category as presented above. However, for personal accounts, some of the wording might go against the guidelines. For example:\n Never trust social democrats. They would sell their grandmother to stay in power (urban, personal)\n or\n Can look at her hair and know what she is going to write [LINK] (urban, personal)\n This last one also contains a link to an outside source, which is discouraged in official guidelines.\nOpinions, emotions, and public engagement This theme included expressed opinions and emotions, for example about the police organization and job conditions:\n The work environment now is worse than ever in my opinion. Listen now, some colleagues in the core business are extremely tired. (urban, personal)\n or gender issues:\n Just read the Police\u0026rsquo;s action plan for increased gender equality - it looks good. Maybe we can upset the male norm?! (rural, personal)\n or other emotions ranging from public grief to humor and jokes:\n It is hardly possible to find words for the tragedy in today\u0026rsquo;s fatal shooting. Poor family and relatives. It is not possible to understand their grief. …Heavy. (urban, personal)\n Public engagement So we saw that personal accounts deviate from the guidance while the official accounts do not. How is this perceived? Other studies (1, 2) have found that public prefer a more \u0026ldquo;genuine\u0026rdquo; interaction with police online, and we see the same here.\nNumber of Retweets and number of likes per tweet are much greater for the personal accounts than the official ones in both urban and rural contexts. This suggests that personal accounts, which display Tweets like follow-ups of all sorts, replies, information, but also anecdotes about crimes, opinions about daily events and politics are more “popular” (measured by likes and Retweets) than the official accounts simply reporting crime incidents with no reflection.\nSo what? Well, why is all this important?\nThe presence of the police contributes to public reassurance. The online presence, characterised by the exhibitions built up on social media is one way to contribute to this reassurance. We found that the tone and theme of personal accounts, while may be seen to go against the guidance, received greater interaction from the public. This was the case in both urban and rural settings.\nThis is important, especially in rural areas, where the public may not see police face to face very frequently at all. There are indications of the presence of “enthusiasts” (eldsjälar) in rural communities who tend to attract followers and reactions, such as likes and Retweets. These active officers are the ones dominating the local discourse and feeding the “crime talk,” and are possibly the key faces of community policing.\nTherefore, one recommendation from our study is to consider the balance between following the Police Authority guidelines on social media use and engaging genuinely with the public. A balance is needed, as while we found increased engagement with personal accounts, we did also find examples of derogatory comments among the personal Tweets, indicating an issue where police officers are not following the principles outlined within the national guidelines in terms of when they Tweet (e.g., duty or off duty), how they Tweet (e.g., language and expression of values and emotions) or what they Tweet about (e.g., issues of privacy, disclosure).\nA methodological note Another contribution of this paper, which I\u0026rsquo;m pleased with, is the approach we took to build the sample of accounts from which to collect Tweets. We adopted the PRISMA method used for systematic reviews, as a framework for collecting appropriate Twitter accounts. The applicability for this to social media research was smooth, as many concepts translated well. For example, we could use \u0026ldquo;mention chasing\u0026rdquo; as an adaptation of \u0026ldquo;citation chasing\u0026rdquo;. The process is detailed in the PRISMA flow diagram.\n The full paper has more detail about our findings on topics and themes as well as engagement, and greater detail on our methodology too. It can be found online here: https://journals.sagepub.com/doi/10.1177/10575677211041926.\nAs always, comments and questions welcome, do not hesitate to get in touch!\n"
    }
,
    {
        "ref": "/blog/2021-07-12-understanding-knife-crime-and-trust-in-police-with-young-people-in-east-london/",
        "title": "Understanding Knife Crime and Trust in Police with Young People in East London",
        "section": "blog",
        "tags": [],
        "date" : "2021.07.12",
        "body": "\u0026ldquo;Knife crime\u0026rdquo; is a term which has come to describe a range of offences where sharp objects, usually knives are involved, which has generated much discourse in media (e.g. BBC), policy (e.g. Home Office), and in academic research.\nA commonly highlighted theme in this topic is the lack of trust between young people and police, which leads to young people\u0026rsquo;s worry about knife crime, which impacts on their everyday lives, and on issues of communication between young eople and police.\nIn 2019 Artemis Skarlatidou, Froi Legaspi, and I received funding from the Not Equal network, a UKRI funded network, that aims to foster collaborations to create the conditions for digital technology to support social justice.\nOur project sought to gain insight into how young people perceive and experience knife crime, how it influences their quality of lives, and how their perceptions match with those of policing experts, to make recommendations to improve trust and communication.\nOur paper Understanding Knife Crime and Trust in Police with Young People in East London contains the results of this project, some highlights of which I will summarise here. The paper is accessible to all via gold open access here: https://journals.sagepub.com/doi/10.1177/00111287211029873\nStudy summary Our study sought to understand how young people experience worry about knife crime as they go about their everyday lives, and to map out the mental models which young people have of the issue, and identify any misconceptions and gaps between their views and those of the police.\nTo do this, we worked with young people enrolled in two colleges based in East London. The young people were asked to download an experience sampling mobile application, which they could use to report any instances of worry as they went about their daily activities. We then also interviewed 16 of these young people, and 4 police officials, three of them part of a dedicated Youth Engagement team, and the fourth working within an Emergency Response team, using a mental models framework.\nHighlighted results The experience sampling mobile application collected 85 reports, of which 65 reports were of instances when the young people felt \u0026ldquo;fairly worried\u0026rdquo; or \u0026ldquo;very worried\u0026rdquo;. When asked about what made them feel worried, they provided the following answers:\n 18 reported: “I heard about someone having a knife,” 14 reported: “I saw a knife,” seven reported “I saw someone threaten to use a knife,” seven reported “I saw someone use a knife,” 19 reported “Other,” and 20 were cases where the person did not indicate why they felt worried. “Other” incidents included personal experiences with issues other than knife crime (e.g., “Someone drove past and shouted at me” or “Was being followed”), seeing something that evoked worry (e.g., “Saw guns”), or general knowledge of issues like gangs or drug dealing in the area.\n Interestingly, the locations of these worry incidents were almost all outside of the study area (Hackney). This suggests that young people experience worrying incidents across their entire activity space, rather than in their immediate local areas.\n-Figure 1: Map of young people’s experiences with worry about knife related incidents across London. Hackney borough is highlighted in red.\nRegarding mental models, our mental models revealed a larger overlap between the perceptions of police and young people than initially anticipated.\n-Figure 2: Top level mental model—Expert and young people’s perceptions of knife crime and motives. Singled-lined circles show expert concepts and dash-lined circles the concepts of young people. Squares do not symbolize concepts but they are being used to visualize how concepts are linked with each other and to whom they refer.\nA prominent feature in young people\u0026rsquo;s mental models was the role of deprivation and social marginalization in knife crime, a knowledge they have mostly gained through their personal experiences and those of their peers and communities. Considering how strongly all young people we interviewed felt about this, regardless of their own ethnicity or personal experience, we suggest that those shaping the broader public health approach or work at the local level take this aspect into account. Young people are willing to engage in the knife-crime debate and cooperate in identifying solutions—but only if they feel included in the process and not targeted by it.\nWe found that young people proved extremely knowledgeable, especially regarding risks and motives of knife crime. Both young people and police experts highlight a lack of trust that leads to a lack of cooperation. This increases feelings of unsafety, leading to more knife-carrying. Interestingly, expert and lay interviews describe trust differently. Experts focus on “effectiveness” as both a source of trust and a requirement in “fighting crime” while young people emphasize the need of “fairness” to establish trust.\n- Figure 3: Expert and young people’s perceptions of policing activities and trust. Singled-lined circles show expert concepts and dash-lined circles the concepts of young people. Note that the size of the circle has no semantic interpretation.\nAnother interesting finding from the mental models of young people was that their awareness was not equal across all types of policing activities. While young people know about enforcement-based activities (they are particularly knowledgeable of police enforcement procedures for which they carry negative perceptions, most prominently stop-and-search, “Section 60” and the “Gang Matrix”) they are mostly unaware of community-building initiatives.\nRecommendations Based on our findings, we suggest three approaches to re-establish and build trust.\nFirst, filling in the knowledge gaps. For example, police and other services could try to build young people’s knowledge of non-enforcement policing initiatives and publicize training and other activities which may have a positive impact in local communities.\nSecond, engage young people in developing knife-crime prevention practices in their communities. Their knowledge can be harnessed to make sure these initiatives are well-received. This is in line with similar suggestions made by the All Party Parliamentary Group for Children in the 2014 report and otherresearch.\nAnd third, carry out further research on how young people interpret qualities which promote trust (i.e., professionalism, accountability, transparency, respect, fairness, empathy and protect, and serve attitudes) in specific local contexts. This may be particularly important in promoting a shared understanding and appreciation of these attributes in police forces and could be used to inform training and guidance on the way police-youth interactions are handled.\nThe paper For full details on methodology, data, results, and recommendations, read the paper available in full open access here: https://journals.sagepub.com/doi/10.1177/00111287211029873\n"
    }
,
    {
        "ref": "/blog/2021-05-24-minimizing-misrepresentation-in-spatial-data-visualizations/",
        "title": "Minimizing Misrepresentation in Spatial Data Visualizations",
        "section": "blog",
        "tags": [],
        "date" : "2021.05.24",
        "body": "Early last autumn I was contacted by the team at Sage Research Methods about their project putting together a Data Visualisation Collection of online resources.\nThe idea is to create online teaching material (video tutorials, text entries, datasets, and so on) introducing fundamentals of data and design necessary to create impactful visualizations.\nTo contribute to this resource, I teamed up with Sage to summarise the key learnings from our paper with Dr Sam Langton, Cartograms, hexograms and regular grids: Minimising misrepresentation in spatial data visualisations  in a short video clip.\nThis is now online and abailable here: http://methods.sagepub.com/video/srmpromo/MfaJ2O/minimizing-misrepresentation-in-spatial-data-visualizations\n Screengrab of Sage video  Transforming polygons: a summary The premise of the paper and the video is that in some cases, the size and the shape of the polygons in our data may distract our viewers from our intended key message. For example, looking at Local Authorities across the country makes London look very small:\n Map of Local Authorities in England using Original Boundaries (note London LAs barely visible).  In these cases, we might wish to change this through transforming these polygons. Here we discuss 4 transformations: balanced area cartogram, hexogram, and uniform hexagonal and rectangular grids.\n Cartograms traditionally use a variable (e.g. population size) to distort the geography of our areas. Balanced area cartograms aim to minimise the distorting side-effects of cartograms by pre-defining an ‘interpretability threshold’ which is the smallest legible unit size given the dimensions of the final published map. Hexograms apply the balanced area approach, implementing an iterative binning algorithm which assigns the centroid of the polygons from the balanced cartogram to tessellated hexagons, each representing the original polygons. Doing so maintains spatial accuracy whilst also being uniform in shape and size. Finally, in uniform hexagonal and square tile grids the original geometry is transformed to tessellating hexagons or squares, while minimising the total distance between the centroid of every original geography and its new centroid on the grid. In our paper we implemented this using the geogrid R package using the Hungarian algorithm.  The four transformations result in these augmented maps:\n Transformed maps of Local Authority areas in England using (a) balanced cartogram, (b) hexogram, (c) square grid, (d) hexagonal grid.  After asking a sample of 600+ strangers on the internet to view these maps and either agree or disagree with the statement which we indeneded to communicate with the map overall, we were able to compare whether there were differences between the map transformations in the extent to which people agreed with the intended message.\nWe found that depending on which map we used, people either agreed more or less with the statement, compared with the original geographic boundaries.\nThe key message from this is that these transformations do indeed affect how your audiences will read your map, and the kind of transformation applied will have an effect on the extent and direction of this. Therefore we recommend trying out multiple approaches, and making sure that the chosen approach best represents the message being communicated.\nThe video To watch the video summary, visit the Sage Research Methods website or go directly with the link here: http://methods.sagepub.com/video/srmpromo/MfaJ2O/minimizing-misrepresentation-in-spatial-data-visualizations.\nThe code To learn more about the R code behind the transformations, see our GitHub repository: https://github.com/langtonhugh/EPB_maps or watch my talk at PyData Manchester from last July https://youtu.be/xUWDBQ4wCqU?t=3245\n"
    }
,
    {
        "ref": "/blog/a-response-to-on-the-safety-of-women/",
        "title": "A response to: on the safety of women",
        "section": "blog",
        "tags": [],
        "date" : "2021.03.12",
        "body": "  The murder of Sarah Everard has horrified everyone across the UK, and has re-ignited discussions about women’s safety in public places. While many point out that something so horrific as murder by a stranger is a rare event, one reaction has been an outpouring of shared experiences of harassment and victimisation.\nI wanted to write this post to present the idea of near-misses and highlight that sexual harassment is still something that is hugely underreported. To get a better picture of these crimes and inform prevention measures, we need to get more and better data about them. We need to support women in making these reports.\nBackground: Sarah Everard and the women of Twitter Sarah Everard was last seen on 3rd March, and since then the picture of the violence committed against her has been revealed to the public, cumilating in the arrest of a man in his 40s (see statements here and here).\nAs we learn more about Sarah Everard, women everywhere are coming forward sharing their stories and expressing anger and feelings of unsafety. I find it so striking that in response to such a horrific and extreme event, almost every woman I know or follow on Twitter has an experience they can share, in the fashion of #MeToo.\nTweet by Caroline Criado-Perez at https://twitter.com/CCriadoPerez/status/1369678790289424387\n It seems everyone has a story that fits the profile of a “near-miss” of such an event.\nYesterday, an email was circulated around my department containing a request from the BBC for a piece looking at the safety of women in light of the murder of Sarah Everard. Specifically, the email said:\n We wanted to do a short pre-recorded interview with a criminologist to talk about how rare it is for women to be murdered by strangers and whether women face any more safety risks these days than they have over the past decades.\n This framework of focussing on the most extreme outcome as the “rare event” combined with the sharing of the shared experiences on twitter brings to mind the idea of near-misses.\n Framework: the idea of near-misses In fields such Aviation, Nuclear Power Technology, Military operation and Air Transport, Railway Safety and many others, it is common to consider the reporting and study of near-misses in order to gain insight into otherwise rare events. The underlying assumption (proposed by HW Heinrich in the book Industrial Accident Prevention. A Scientific Approach) is that the same causal mechanisms underlie the near-misses as the serious accidents which result in fatality/injury. So by studying the near-misses, we can eventually understand and ideally prevent the fatalities/injuries.\nI explored the idea of near-misses in my PhD thesis. Working with Camden LGBT forum, we asked peple to report specific incidents of worry about hate crime. People reported situations which they did not consider crimes (although many were) but which made them feel worried. Based on these “near-misses” we identified problematic hotspots where the more rare serious violence incidents may occur.\nApplied here, we could consider the many experiences which the women of Twitter are sharing as these “near-misses” (although many are sharing more serious examples too!). And we should take these seriously in order to:\n understand the extent of the problem  collect information about situational features of the opportunities for these incidents to occur.   But the problem is sexual offences are massively underreported. This is a known issue, and one which campaigns such as TfL’s Report it To Stop It have tried to address. They did this successfully (see our paper on this here (or open access here) which shows how the campaign affected reporting patterns). But even with this, the data we have are unlikely to be comprehensive. However, we can take a look at some patterns.\n Prevalence of sex offences on London transport To explore the prevalence of the types of victimisation the women of Twitter are reporting (specifically cases of sexual harassment on transport) I consider data on 9109 reports of sex offences made to the British Transport Police in London between 2012-01-01 and 2020-11-18 (acquired via a Freedom Of Information request).\nFigure 1 shows that over time, recorded sex offences on London Underground/ DLR/ National Rail in London have been on the increase. (Note an obvious drop once COVID-19 shut down public transport).\n Figure 1: Figure 1: Sex offences over time  While we must keep in mind that some of the change should be attributed to the Report It To Stop It campaign, there is really no way that we could claim that the problem is resolved, or going away.\nI mean, just look specifically at how sexual offences do pick back up again on transport once the transport modes open again after the COVID-19 lockdown (Figure 2):\n Figure 2: Figure 2: Sex offences since lockdown  So even in the post-covid era, passengers on the (supposedly much less crowded?) London Underground/ DLR/ National Rail are reporting on average 10 incidents a week. In the year before COVID, this was 28 incidents per week.\n Is that a big number? Let’s put this into context. The most common crime recorded by British Transport Police in London (taken from open data at police.uk) is “Theft from person (without use or threat of force)”. This is a very widely reported crime, as most of the time when someone’s wallet is stolen, their travel card is stolen with it, and this will need to be addressed when leaving the station. In total, between March 2019 and March 2020, a total of 960 theft from person incidents were reported. This is about 8 times the montly average for sex offences (116 sexual offences reported per month).\nOn the other hand, when we include violence in there by looking at the crime type of robbery, we see an average of 78 cases of robbery per month. This is less common than sex offences (remember that comes to average of 116 offences per month).\n Figure 3: Figure 3: Reported number of sex offences compared with other cirmes  Figure 3 shows we have more recorded incidents of many crimes, including bicycle theft, than we do of sexual offences. (I greyed out the category of “violent and sexual offences” which lumps together far too many crime types, and will include everying I have in the ‘sex offences’ category (in orange)). Does this mean that sexual offences are more infrequent? In the recorded data - yes. In underported experience?\nAccording to the ONS only 17% of people who had experienced rape or assault by penetration (including attempts) reported to the police. I assume the number is lower for the near-misses considered less serious. But let’s try with 17%, if we were to assume that the 116 offences are 17% of what is actually experienced, we would be at an average of 683 offences per month, changing our chart significantly (Figure 4):\n Figure 4: Figure 4: hypothetical reported number accounting for underreporting from CSEW  Clearly there is scope to learn more about women’s experiences.\n Victim profile? Building on the data we do have, we can address some other comments on Twitter about who is victimised.\nOverall in our dataset, or the incidents where the crime was not a Regina Offense (n = 7570) and victim gender was recorded (n = 7570), 7061 of the cases the victim was Female (93.3%), while only 509 (6.7%) cases was the victim Male.\nThe youngest victim in the data set was 7 years old, while the oldest was 96. If we look at the age distribution we can see that it follows a normal distribution, skewed slightly right.\nWe might conclude that younger passengers are more affected, until we look at ridership statistics. This is for all Rail in England, and from 2016, but if you look at the age distribution for women, the pattern should look familiar:\nSource: Rail Passengers Factsheet 2016: https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/590562/rail-passengers-factsheet-2016-revised.pdf\n While I don’t have more demographic data in my data set, I am feeling pretty confident in concluding that the victim profile for sexual offences on transport is: women who use public transport.\n Walking alone late at night…? Okay well what if it has to do with walking around (or taking public transport) alone late at night? We can look at the times of day:\nSurprise surprise, the peak victimisation incidents are happening at rush hour!!! For reference here is a chart of trips made by London residents in London, by start time (hour), weekday only (based on data from london datastore:\nSo not walking alone late at night does not at all guarantee that these crimes will not be experienced!\n What can be done? To prevent the fatal outcomes, however rare they may be, the more frequent similar events, which may be considered near-misses need to be taken seriously.\nIn order to do this though, we need better data. Sexual harassment remains a largely underreported crime. Initiatives like the Report It To Stop It campaign and Project Guardian are a good start as we know they increase reporting.\nOther initiatives, like crowdsourced platforms offer another solution to collect data on many of these near-misses in order to better understand the situational factors associated with crime opportunities. One example is Safecity.in. This could be rolled out widely, and supported in a way that encourages women to report their experiences.\nBy collecting better data we can understand the opportunities better, and support prevention initiatives so women can feel safe when walking, travelling, and generally living their lives.\n "
    }
,
    {
        "ref": "/blog/towards-a-place-based-measure-of-fear-of-crime-a-systematic-review-of-app-based-and-crowdsourcing-approaches/",
        "title": "Towards a Place-based Measure of Fear of Crime: A Systematic Review of App-based and Crowdsourcing Approaches",
        "section": "blog",
        "tags": [],
        "date" : "2020.08.03",
        "body": "  This post is about the paper Towards a Place-based Measure of Fear of Crime: A Systematic Review of App-based and Crowdsourcing Approaches available to read on the journal page or pre-print on https://osf.io/49sv7/\n A place-based and context-specific approach to the study of fear of crime is something that I had been thinking about since my PhD when it seemed to me that among the wealth of work on ‘crime and place’ there seems to be little translation of these situational perspectives to the study of fear of crime.\nTo address this, I developed a mobile application (see more about FOCA here) to allow people to report how they feel in specific time and place. Since then however, more and more scholars have taken this approach, (actually some even before then, just in other fields!) and the time has come to move from showing off this flashy ‘new’ approach and instead to take stock, and establish what is known and what areas need to be further explored by those interested in furthering this field.\nOur newest paper Towards a Place-based Measure of Fear of Crime: A Systematic Review of App-based and Crowdsourcing Approaches, out this week in Environment and Behaviour does exactly that. In this post I will illustrate some highlights from the paper.\nTaking stock To identify what is known (regarding strengths and limitations) about mobile app-based and crowdsourcing approaches to operationalise fear of crime, we conducted a systematic search of 3 major publication databases, and used our pre-defined selection and screening rules to filter studies. The flow chart below illustrates this process.\nWe then extracted and synthesised key limitations and strengths.\nStudy characteristics Before going into strengths and limitations it’s worth noting that we found some very interesting patters in our descriptive exploration of these studies. To me what sticks out the most is the incredibly wide range of ways to measure ‘fear of crime’.\nThis should be familiar to anyone interested in ‘fear of crime’ literature. Already in the 1980s scholars lamented that “the phrase ‘fear of crime’ has acquired so many divergent meanings that its current utility is negligible.”1\nBut even on this new frontied of “apps” and “crowdsourcing” we fail to arrive on a consistent way of measurement and instead we have things like:\n Evaluate the momentary sense of security on a scale from 1 to 5 Green or red tag to indicate locations where they feel comfortable or uncomfortable Choose ‘which place looks safer?’ from two images Choose between two scared/safe emoticons Answer: ‘In this moment, how worried are you about becoming a victim of crime?’  …and many many more!\n Strengths We arrived on the following themes for strengths identified by studies presenting these approaches:\n Capture the spatial-temporal specific nature of attitudes and emotions towards crime. Record data on individual variables and specific types of fear/disorder. Record data on architectural features. Reduced cost of data collection. Oriented to evidence-based policy making/urban planning.  The paper details these and highlights examples, but one really exciting one is the ability to produce point-level maps of people’s experiences, and then associate these with the environmental features of these environments in day and night times.\nFor example, my paper in London, and the study of David Buil-Gil in Elche2 both resulted in such maps:\nThen David went further to explore through Google Street View the environmental context in which these reports were made:\n Limitations We also wanted to highlight what were the weaknesses of the approach identified by the studies we selected, and what sort of future work these might insire to address them. Limitations emerged in the following themes:\n Sample issues Participation inequality. Specifically:  No screening questions. Participation decrease Small sample sizes and low response rates.  Under-representation of certain areas and times. Difficult to interpret results. Limitations to generalise results. Repeatedly asking about fear might increase/cause fear. Lack of temporal variability in some web-based measures.  Again we detail each one of these in the paper in deatail, but I want to highlight one. Sampling is of course a huge issue here, which I know there is work being done to address, but one issue that is not to do with sampling of people, but with sampling of places, presents an interesting problem. One response to this bias in sampling of places (rather than sampling of people) is a study design implemented by Laura Vozmediano and colleagues3 where instead of asking people to self report areas they are in, they tracked people’s movements across the entire study period, resulting in all of the routes. Then these routes were randomly sampled to be shown back to people to ask ‘when you walked this route earlier today, how safe did you feel?’. This is one way to create more representative samples of areas people have been to, rather than asking people to choose where they report.\n  Next steps We conclude by suggesting areas for researchers interested in this area to pursue. Overarchingly, the limitations synthesised in our review identified a need to improve the reliability, validity, and generalisability of these measures. The most prominent theme was around issues with sampling bias and generalisability.\nTo address this some future work ideas are:\n to explore participation motivation, the use of sensors, or interviews or follow-up questionnaires the use of statistical or computational modeling approaches to mitigate bias research on the contextual elements that trigger fear of crime may benefit from the increased use of eye tracking techniques to help address the “difficulty to interpret results” limitation.   Conclusion Overall, the papers in this review all share an approach that allows the understanding of fear of crime as a place-based, contextually specific event, captured in people’s emotional and behavioral responses, that may lend itself to problem-solving approaches. Much like a place-based approach for crime, applying these methodologies to fear of crime make possible its operationalisation in a way that allows such exploration. By building on the strengths and working to address the limitations discussed in this review, we can explore fear of crime as a function of people’s experiences in their immediate environments, and inform evidence-based policy making and urban planning for safer places.\n   Ferraro, K. F., \u0026amp; Grange, R. L. (1987). The measurement of fear of crime. Sociological inquiry, 57(1), 70-97.↩︎\n Buil-Gil, D. (2016). InseguridApp: Estudio piloto de los patrones de distribución espacio-temporal de los enclaves del miedo (al crimen) en Elche a partir de una nueva aplicación móvil [Unpublished master’s thesis]. Miguel Hernández University.↩︎\n Vozmediano, L., Azanza, M., Villamane, M. (2017). Desarrollando y probando una app para analizar la influencia de la seguridad percibida en la movilidad a pie: un trabajo multidisciplinar con profesorado y alumnado de Psicología e Ingeniería. In Proceedings of the Seminar “La educación, base para los Objetivos de Desarrollo Sostenible, Grupo 4 Paz y participación” (p. 13). University of the Basque Country UPV/EHU↩︎\n   "
    }
,
    {
        "ref": "/blog/reading-for-presentation-skills/",
        "title": "Reading for Presentation Skills",
        "section": "blog",
        "tags": [],
        "date" : "2020.06.28",
        "body": " Back in February someone on Twitter recommended the book Pitch Perfect: How to Say It Right the First Time, Every Time by Bill McGowan and Alisa Bowman. It was still in the time before corona (b.c.) so I went to a physical library (wow) and got a physical copy (wow) and read it while on a weekend getaway (wowowow). While it started out a bit slow (a lot of convincing the reader that the book and tools are useful, like sure it would be great if I could afford some one-to-one coaching from the author, but it’s not likely to happen to me unfortunately…) I found there to be many useful pointers, which can help with presentations and interviews, so I will summarise some below.\nreading actual book on actual weekend getaway\n The overall concept: the 7 principles The book is structured around seven key principles, which are then discussed in more detail throughout the book. These are:\nHeadline principle Get attention by starting with your “best materials”. Don’t ease into the point, instead start the presentation/talk with a concise and compelling statement. I feel like academic presentations are particularly bad at applying this principle, so I have started being a bit more active about foregrounding my key findings.\n Scorsese Principle The premise of this principle is that you want to “direct the film” of your presentation or talk with visuals that will grab people’s attention (the book uses the example of Paul Sorvino slicing a close of garlic with a razor in prison). Illustrating your points with descriptions of engaging stories makes your points more memorable and better holds your audience’s attention.\nThis applies to presentations and interviews, with presentations you can use actual images if you have a slide deck, but with interviews it has an effect that means your interviewer might better remember you. “When the interviewer goes back to her team to talk about the twenty different candidates she’s seen that day, it’s the stories that will stick out in her mind.”\n The Pasta-Sauce Principle Don’t be afraid to cut out material. If it’s not essential to include to make your point, you can probably do without it. People’s attention span is short, so don’t waffle because you will lose them. (I am not sure about the naming of this principle… I think it’s about “boiling down” to the core message, like you’d boil down pasta sauce???)\n No-Tailgating Principle As far as I can tell this principle basically amounts to “slow down”. Use a slower pace and deliberate pauses to allow you to think through what you’re about to say, and help avoid saying something silly, or using filler words or sounds like “like”, “umm”, and so on.\n Conviction principle This principle is all about “be confident in your message”. A lot of this is about body language, and I believe it taps into the “power pose” research. Unfortunately, I do believe that this work doesn’t actually replicate :( Anyway there are some tips about how to sit, what to do with your hands, to keep smiling a little bit always, and to maintain eye contact. For example, if you’re uncomfortable with eye contact a handy tip is to actually stare at a man’s sideburns or a woman’s earring - it will apparently seem like you’re looking in their eyes.. (although I just tried this and I’m not convinced…)\n Curiosity principle This is more about listening/conversation skills than presentation skills. The message of this principle is to display genuine interest in what the person talking to you says, both with facial expressions (again eye contact, smiling), but also genuinely giving them your full attention. I think this is just basic decent conversation practice, but McGowan and Bowman mention lots of people are just queueing up the next point they want to say in their head instead of actually listening, and so if you’re doing this, stop! Also get off your phone (unless you’re taking notes…?)\n Draper principle I’ve never watched Mad Men, just not really my jam, but the idea of this principle is to be like the protagonist and steer conversations to stay on points that you are comfortable with. This seems especially relevant to interview situations, where you are asked a question that isn’t quite your comfort zone, the idea is to shift the topic to something that plays to your strengths. “If you don’t like what’s being said, change the conversation”.\n  Practice practice practice This is the other key message of the book, besides the seven principles and I think this is the most important thing. I’ve been practicing my presentations over and over before delivering them ever since I tried to improvise one at an international conference after not presenting publicly at all for over a year. It was horrible, I still cringe thinking back to it. Nothing really went wrong, but I remember tangling words, rushing through slides, and many other things that I’d rather bury deep down. So practicing presentations over and over and over and over and over is something I completely subscribe to.\nBUT McGowan and Bowman suggest also to practice everything - not just presentations but practice for interviews, practice for discussion panel appearances, practice for conference dinner talk (conversation templates I will highlight in a moment as well) and practice everything. He also says to practice applying the 7 principles in everyday situations so it becomes natural.\nAnyway I know it’s not ground-breaking or anything, but really do practice everything and often, this is a point I fully support.\n Conversational templates Another thing I found helpful was the idea of conversational templates. This ties in with the point about practice. With practice you can develop a template for different types of conversation scenarios.\nThis can be useful for interviews (a template for answering certain types of questions that you’re likely to be asked) or or for the post-talk networking type events. This involves preparing some talking points around recent events. Initially this sounded a bit cringey to me, but actually, being someone who struggles in these situations and ends up just talking about nothing to fill silence, I am thinking it might help me. McGowan and Bowman write:\n“Having a conversation template like this can help you avoid putting your foot in your mouth. And if you’re shy there’s an extra benefit: it gives you something to say. It takes the pressure off of having to be engaging on the spur of the moment. The less pressure you feel the more articulate you’re likely to be.”\n Answering questions When you are asked a question, first make sure to listen to it in its entirety. Don’t cut off the person asking the question in order to answer. Wait for the whole question, and then before answering, think through the following 3 points:\n What’s my point?  How will I illustrate it? (an example, a story, some data)  What are the first five words out of my mouth?   Following this three-step process will “…keep you from rambling and meandering your way through some long-winded and redundant speech”.\nSometimes you might get asked a question where it is difficult to show off your skills (in interview) or for you to contribute your experiences (at a dinner party). Here McGowan and Bowman suggest applying the Don Draper principle (see above) of shifting the topic to suit your skills. BUT you want to be very careful here, if you change the topic abruptly, the trust built up with your audience/interviewer might decrese. One approach is to mirror a little of the question in your answer. Try to touch on a broader topic which encompasses both the question and the topic you want to veer into, and make your transition.\nThe book gives this example: “Let’s say you’re at a dinner party and someone is talking about (…) admission process for preschools”. This topic is interesting for parents of preschool-aged children, but his children are in college. So he steers the conversation like so: “I gotta tell you, this kind of admissions stress will follow them for years. My daughter just had an interiew to study abroad…”. Practicing this mirroring and subtle transition might help you in an interview setting to talk a lot about the topics you are comfortable with, while still answering the questions of your interviewer.\n Saying bye Ending your talk/presentation/answer/sentence is an important skill to master as well.\nFor example, McGowan and Bowman suggest that ending your presentation with the usual summary recap is not very strong. Instead you could end with suggestions for your audience to try something new, projecting the future benefits they will reap if they heed the lessons from your talk/presentation/etc. It seems like an applied, future oriented recap. He gives the example of ending his own talks with:\n“So next time you give a speech think about starting with content that’s unpredictable, visual, and anecdotal. I think you will find your audience is more engaged and your message resonates more effectively.”\nRegarding conversations the advice is to end them sooner rather than later. “You want to leave people wanting more. You don’t want to leave them thinking Thank goodness that’s over with.” This applies to answering interview questions as well. You want to give a thorough answer, but to also keep it concise, and not ramble on for 10 minutes, inevitably taking yourself into tangents that may not be relevant to the original question.\nA difficult situation to end a conversation can be when chatting with someone at a party/conference/networking event/whatever. In these cases it is best to be honest. The book suggests saying something like: “Listen, I know you have a ton of people to mingle with and touch base with, so I just wanted to say hi. It’s been so great talking to you, and I know we’re going to see each other at this off-site next week. Maybe we can catch up again there.”\nAnd finally, when I cannot stop talking, and someone needs to jump in, a tactic they might apply is the “Camouflaged Cutoff”. To apply this, just finish my sentence (like we’ve been married 40 years) and then segway into what you wanted to say. The book offers this template: “It’s interesting you bring that up, because that’s really at the heart of another topic I want to get to…” Or if you’re bringing someone else into the conversation: “I think Susan was mentioning something relating to that recently. Susan what was it you were saying about…?”\n Overall The book was an easy read, and I definitely learned some tips from it. It would be great to have actual coaching from the author, but since that’s not something that’s looking likely in my future, the book did a good job of giving pointers. The biggest take-aways for me were to practice practice practice, make conversation templates, think in stories, and take time to listen and talk only when I’m ready.\n "
    }
,
    {
        "ref": "/blog/street-profiles-tutorial/",
        "title": "Street Profile Analysis Tutorial",
        "section": "blog",
        "tags": [],
        "date" : "2020.04.16",
        "body": " A while back I posted this tutorial about hot routes for visualising crime along a network (eg tube line). But I mentioned at the end, it may not always be necessary to include the geographic component of this visualisation. Instead, another approach could be to make use of street profile analysis introduced by Valerie Spicer, Justin Song, Patricia Brantingham, Andrew Park, and Martin Andresen in their 2016 paper ‘Street profile analysis: A new method for mapping crime on major roadways’ published in Applied Geography.\nStreet profile analysis was initially developed as a method for visualising temporal and spatial crime patterns along major roadways in metropolitan areas. The idea is that the geographical location of the street may not actually be as important as the ability to visualise data from multiple years in a comparable way. So the approach is to treat the street in question as the x-axis of a graph, with a start point (A) and end point (B) breaking it into some interval (I think they use 100m intervals). Then, you can visualise how crime is distributed along this street by plotting count or rate of crimes on the y-axis, and have multiple lines for different years for example. You can add context by re-introducing some key intersections, or other points of interest.\nIn their paper, Spicer and colleagues demonstrate this using the example of Kingsway Avenue in Vancouver, BC\nWhen I was working as a crime analyst at TfL, I adapted this method to visualise fare evasion along various bus routes, but instead of breaking up the route into 100m intervals from start to finish, I used bus stops to represent ticks along the axis. Also instead of using crime from different years, I compared data from different sources. It was very easy to adapt street profile analysis in this way, and it proved a simple yet powerful way to tell the story of how fare evasion was distributed along the specific bus line. So for anyone who wants to implement this in their work, I tought it could be helpful to demonstrate in a quick tutorial how to carry out Street Profile Analysis using R.\nOverview Like last time, let’s start with a breakdown of the steps we will need to follow.\n Step 1: Prepare the network layer Step 2. Link crime events to points of interest Step 3: Calculate a rate Step 4: Visualise the results  Much like for the hot routes tutorial, I will use the example dataset of the London Underground network, spatial data for which can be downloaded using the Transport for London API, and crime from British Transport Police for February 2020 available via data.police.uk.\n Step 1: Prepare the network layer This step is the same as last week’s tutorial so I won’t dwell on it here, essentially we get the stops for the bakerloo line from the TfL API, make it an sf object, and create a connected line. I’ve created a function that does this all called getBakerlooLine() and one called getBakerlooStops() which I just run here. If you want to see the function it is on github here but better to have a look at step 1 of the hot routes tutorial for a breakdown and explanation.\nlibrary(rjson) library(dplyr) library(sf) bakerloo_stops \u0026lt;- getBakerlooStops() ## Warning in readLines(\u0026quot;https://api.tfl.gov.uk/line/bakerloo/route/sequence/ ## outbound\u0026quot;): incomplete final line found on \u0026#39;https://api.tfl.gov.uk/line/ ## bakerloo/route/sequence/outbound\u0026#39; bakerloo_line \u0026lt;- getBakerlooLine(bakerloo_stops) We can plot these to make sure everything looks okay:\nlibrary(ggplot2) ggplot()+ geom_sf(data = bakerloo_stops) + geom_sf(data = bakerloo_line) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) Yepp, looking good! In this case we don’t really need to split our line into segments, so that’s it for prepping our network layer!\n Step 2. Link crime events to points of interest For step 2, we start once again with importing some data from the British Transport Police.\ncrimes \u0026lt;- read.csv(\u0026quot;data/2020-02-btp-street.csv\u0026quot;) crimes_sf \u0026lt;- st_as_sf(crimes, coords = c(\u0026quot;Longitude\u0026quot;, \u0026quot;Latitude\u0026quot;), crs = 4326) We also want to subset this larger dataset of crimes to only those that are on or near the Bakerloo line\nbakerloo_line_buff \u0026lt;- st_buffer(bakerloo_line, 0.005) ## Warning in st_buffer.sfc(bakerloo_line, 0.005): st_buffer does not correctly ## buffer longitude/latitude data violent_crimes \u0026lt;- st_intersection(bakerloo_line_buff, crimes_sf %\u0026gt;% filter( Crime.type== \u0026quot;Violence and sexual offences\u0026quot;)) other_crimes \u0026lt;- st_intersection(bakerloo_line_buff, crimes_sf %\u0026gt;% filter( Crime.type != \u0026quot;Violence and sexual offences\u0026quot;)) And make sure it’s all looking good\nggplot()+ geom_sf(data = bakerloo_line_buff) + geom_sf(data = bakerloo_line) + geom_sf(data = bakerloo_stops) + geom_sf(data = other_crimes, col = \u0026quot;blue\u0026quot;) + geom_sf(data = violent_crimes, col = \u0026#39;red\u0026#39;) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) Now we can snap each crime point to the closest tube station using the st_nearest_feature() function. This will return, for each point, the ID of the nearest segment. We can do this once for violent crime and once for other crimes:\nviolent_crimes_closest \u0026lt;- st_nearest_feature(violent_crimes, bakerloo_stops) other_crimes_closest \u0026lt;- st_nearest_feature(other_crimes, bakerloo_stops) If we want, we can have a look at the results. Let’s see for violent crimes:\nviolent_crimes_closest ## [1] 21 21 25 25 13 13 20 15 14 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 23 ## [26] 23 23 23 12 12 12 12 12 17 1 1 1 1 3 9 9 7 7 7 7 7 7 7 7 7 ## [51] 19 19 19 19 19 19 11 11 11 11 8 11 11 2 2 In the results above, we see printed the stop ID for the closest stop to each crime incident.\nNow we need to count the frequency of each stop ID (as this tells us the number of crimes that are close to it, therefore assigned to it), and join this with out stops dataframe:\nlibrary(tidyr) #make list of nearest into df of frequency stops_w_vcrimes \u0026lt;- as.data.frame(table(violent_crimes_closest)) #join to sections object and replace NAs with 0s also rename Freq variable to num_crimes bakerloo_stops \u0026lt;- left_join(bakerloo_stops %\u0026gt;% mutate(stopid = rownames(.)), stops_w_vcrimes, by = c( \u0026quot;stopid\u0026quot; = \u0026quot;violent_crimes_closest\u0026quot;)) %\u0026gt;% mutate(Freq = replace_na(Freq, 0)) %\u0026gt;% rename(num_violent_crimes = Freq) ## Warning: Column `stopid`/`violent_crimes_closest` joining character vector and ## factor, coercing into character vector #do this again for other crimes stops_w_ocrimes \u0026lt;- as.data.frame(table(other_crimes_closest)) bakerloo_stops \u0026lt;- left_join(bakerloo_stops, stops_w_ocrimes, by = c( \u0026quot;stopid\u0026quot; = \u0026quot;other_crimes_closest\u0026quot;)) %\u0026gt;% mutate(Freq = replace_na(Freq, 0)) %\u0026gt;% rename(num_other_crimes = Freq) ## Warning: Column `stopid`/`other_crimes_closest` joining character vector and ## factor, coercing into character vector Great now we have a dataframe of each stop and the number of crimes that happened closest possibly to it. We could make a street profile of it with this count (in fact we will in a moment) but first let’s discuss about rates!\n Step 3: Calculate a rate In order to calculate a crime rate for each station, we need to think about what is an acceptable denominator. How can we best estimate the number of opportunities present for the particular crime type we are interested in, and how might that be captured in some available data set?\nIn an upcoming paper “Alternative denominators in transport crime rates” the pre-print for which is available on OSF here we considered the various types of denominators that may be available to crime analysts focusing on estimating ambient populations to calculate crime risk. These can be:\n residential population in the area surrounding the station (available from the Census) workplace population in the area surrounding the station (also available from the Census) number of lines passing through each station (available from station information, or even a network map) number of trains passing through each station (available from station information, maybe a timetable) number of passengers entering/exiting each station (available from transit authority, maybe via a survey like the Rolling Origin Destination Survey, or smartcard data (eg Oyster card in London))  and possibly some other options that we have not thought of.\nHere, for simplicity we can use the Rolling Origin Destination Survey (RODS) data. We can download this directly from TfL API portal. It is a zip file, so first we download this using download.file() function, and unzip it into a subfolder called “data” using the unzip() function. We delete the zip file after extracting, because we don’t need it anymore using the unlink() function.\n#get rods data from TfL API download.file(\u0026quot;http://tfl.gov.uk/tfl/syndication/feeds/rods.zip\u0026quot;, destfile = \u0026quot;./data/rods.zip\u0026quot;) #unzip zip file unzip(\u0026quot;data/rods.zip\u0026quot;, exdir = \u0026quot;./data\u0026quot;) #delete zip file unlink(\u0026quot;data/rods.zip\u0026quot;) Once we have our data, we want specifically the excel file “Total entries and exits by borough-time of day 2017.xls”. The entry and exit data sets are two separate sheets in this excel file, so we will have to read them in one sheet at a time, using read_xls() function from the readxl package. From having seen the data before, I also know that 1) the variable names are in line 4 so we can skip up to there (the skip= argument in the read_xls() function), 2) that the names are messy, so we clean them up using the clean_names() function in the janitor package, and 3) that for some reason the ‘station’ variable is in two columns, so we have to name these as station number and station name using rename() function in the dplyr library (loaded earlier). It seems like a lot of cleaning of the data, but it could be worse, and there will be more cleaning later…\nlibrary(readxl) library(janitor) #load entries and exits data #first entries rods_entries \u0026lt;- read_xls(\u0026quot;data/RODS_2017/Misc/Total entries and exits by borough-time of day 2017.xls\u0026quot;, sheet = \u0026quot;entries\u0026quot;, skip = 4, col_names = TRUE) %\u0026gt;% clean_names() %\u0026gt;% rename(stn_num = station, stn_name = x3) #then exits rods_exits \u0026lt;- read_xls(\u0026quot;data/RODS_2017/Misc/Total entries and exits by borough-time of day 2017.xls\u0026quot;, sheet = \u0026quot;exits\u0026quot;, skip = 4, col_names = TRUE) %\u0026gt;% clean_names() %\u0026gt;% rename(stn_num = station, stn_name = x3) Then finally we can link the entry and exit data together with left_join() from dplyr, and sum them together to get the whole day’s worth of entries and exits (because we don’t care about time of day for our purposes here) using group_by() and summarise(), then create a final variable called total_pax using the mutate() function. The total_pax variable that tells us how many trips we can expect to go through that station on a given day, giving us a nice denominator to calculate crime rate!\nrods \u0026lt;- rods_exits %\u0026gt;% left_join(., rods_entries) %\u0026gt;% # join up the exit and entry data group_by(borough, stn_num, stn_name) %\u0026gt;% # sum all time periods within the day summarise(total_exiting = sum(number_exiting), total_entering = sum(number_entering)) %\u0026gt;% filter(stn_num != \u0026quot;Total for borough\u0026quot;) %\u0026gt;% # remove the borough totals mutate(total_pax = total_exiting + total_entering) # create grand total of pax trips Now we have our denominator for calculating the crime rate! Let’s join it to our Bakerloo Line stations to do so.\nHere we have another stumbling block (or teachable moment?) To join our data, we need corresponding columns in each one. For example here the denominator dataset (rods) and the stops with the crime data (bakerloo_stops) both contain a column that has station names (stn_name). Normally, we would expect of course that the stn_name variable in both data sets should match; ie that “Elephant \u0026amp; Castle” appears in both data sets, spelled the same way. How naive of us… if we go ahead and try to join our datasets under this assumption, 10 stations will be unmatched. See for yourself:\nbakerloo_stops_try1 \u0026lt;- left_join(bakerloo_stops, rods) Ha-ha this dataset has 10 NA values, that is for 10 stations we did not find a corresponding match! I told you there will be more data cleaning to be doe… Well we can have a look at which stations don’t match for ourselves:\nbakerloo_stops_try1 %\u0026gt;% filter(is.na(total_pax)) %\u0026gt;% select(stn_name) ## Simple feature collection with 10 features and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: -0.194232 ymin: 51.49881 xmax: -0.112315 ymax: 51.53498 ## CRS: EPSG:4326 ## stn_name geometry ## 1 Baker Street Underground Station POINT (-0.15713 51.52288) ## 2 Embankment Underground Station POINT (-0.122666 51.50706) ## 3 Edgware Road (Bakerloo) Underground Station POINT (-0.17015 51.5203) ## 4 Kilburn Park Underground Station POINT (-0.194232 51.53498) ## 5 Lambeth North Underground Station POINT (-0.112315 51.49881) ## 6 Maida Vale Underground Station POINT (-0.185758 51.52978) ## 7 Oxford Circus Underground Station POINT (-0.141903 51.51522) ## 8 Piccadilly Circus Underground Station POINT (-0.133798 51.51005) ## 9 Regent\u0026#39;s Park Underground Station POINT (-0.146444 51.52334) ## 10 Warwick Avenue Underground Station POINT (-0.183783 51.52326) We may notice a pattern, they all seem to have the suffix “Underground Station” in our bakerloo_stops dataset but not in the rods data. Let’s get rid of this, and try to match again:\nbakerloo_stops$stn_name \u0026lt;- gsub(\u0026quot; Underground Station\u0026quot;, \u0026quot;\u0026quot;, bakerloo_stops$stn_name) bakerloo_stops_try2 \u0026lt;- left_join(bakerloo_stops, rods) NEARLY THERE! Now we have only one station which did not join. Which is it?\nbakerloo_stops_try2 %\u0026gt;% filter(is.na(total_pax)) %\u0026gt;% select(stn_name) ## Simple feature collection with 1 feature and 1 field ## geometry type: POINT ## dimension: XY ## bbox: xmin: -0.17015 ymin: 51.5203 xmax: -0.17015 ymax: 51.5203 ## CRS: EPSG:4326 ## stn_name geometry ## 1 Edgware Road (Bakerloo) POINT (-0.17015 51.5203) Ahhh it’s “Edgware Road (Bakerloo)”, which apparently, in the rods dataset is labelled “Edgware Road (Bak)”. *sigh* One last try:\nbakerloo_stops$stn_name \u0026lt;- gsub(\u0026quot;Bakerloo\u0026quot;, \u0026quot;Bak\u0026quot;, bakerloo_stops$stn_name) bakerloo_stops \u0026lt;- left_join(bakerloo_stops, rods) Tadaaa! And now we can calulate the rate by dividing the number of crimes (num_crimes) by the total number of daily passengers (total_pax), and let’s times by 10,000 to get crimes per 10,000 pax for each station.\nbakerloo_stops$violent_crime_rate \u0026lt;- bakerloo_stops$num_violent_crimes/ bakerloo_stops$total_pax * 10000 bakerloo_stops$other_crime_rate \u0026lt;- bakerloo_stops$num_other_crimes/ bakerloo_stops$total_pax * 10000 Great now we have a rate! Our top station for violent and sexual offences is Stonebridge Park, which has 1.42 such crimes per 10,000 passengers, while our lowest are the stations which recorded no such crimes (Kilburn Park, Lambeth North, Maida Vale, Warwick Avenue, Kenton, North Wembley, South Kenton, Willesden Junction). Great, now let’s actually visualise crime along the stations of the Bakerloo Line using Street Profile Analysis!\n Step 4: Visualise the results Now we have (almost) everything we need to visualise crime along the Bakerloo line using Street Profile Analysis. What we want to do, is imagine the Bakerloo line is the x axis, and then use the y axis to show the crime rate. Of course for this, we want to know the order in which the stations follow each other.\nAnnoyingly, the TfL API doesn’t actually have any sort of sequence information with the stops ( see discussion by other users here ) so we have to make our own lookup table:\nbakerloo_order \u0026lt;- data.frame( stop_num = c(1:25), stn_name = c(\u0026quot;Harrow \u0026amp; Wealdstone\u0026quot;, \u0026#39;Kenton\u0026#39;, \u0026#39;South Kenton\u0026#39;, \u0026#39;North Wembley\u0026#39;, \u0026#39;Wembley Central\u0026#39;, \u0026#39;Stonebridge Park\u0026#39;, \u0026#39;Harlesden\u0026#39;, \u0026#39;Willesden Junction\u0026#39;, \u0026#39;Kensal Green\u0026#39;, \u0026quot;Queen\u0026#39;s Park\u0026quot;, \u0026#39;Kilburn Park\u0026#39;, \u0026#39;Maida Vale\u0026#39;, \u0026#39;Warwick Avenue\u0026#39;, \u0026#39;Paddington\u0026#39;, \u0026#39;Edgware Road (Bak)\u0026#39;, \u0026#39;Marylebone\u0026#39;, \u0026#39;Baker Street\u0026#39;, \u0026quot;Regent\u0026#39;s Park\u0026quot;, \u0026#39;Oxford Circus\u0026#39;, \u0026#39;Piccadilly Circus\u0026#39;, \u0026#39;Charing Cross\u0026#39;, \u0026#39;Embankment\u0026#39;, \u0026#39;Waterloo\u0026#39;, \u0026#39;Lambeth North\u0026#39;, \u0026#39;Elephant \u0026amp; Castle\u0026#39;) ) Now we can join this to the original dataframe to have a sequence to order our stops by:\nbakerloo_stops \u0026lt;- left_join(bakerloo_stops, bakerloo_order) And then we can use this to order our stop names, and finally present the Street (or rather Route) Profile for the Bakerloo line considering violent and sexual offences:\nggplot(bakerloo_stops, aes(x = reorder(stn_name, stop_num), y = violent_crime_rate, group = line)) + geom_point() + geom_line() + xlab(\u0026quot;Bakerloo Line\u0026quot;) + ylab(\u0026quot;Violent Crime Rate per 10,000 passengers\u0026quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) Very cool! Now what is great about this is you can compare different things here. For example, we could compare violent crime vs other crimes:\nggplot() + geom_point(data = bakerloo_stops, aes(x = reorder(stn_name, stop_num), y = violent_crime_rate, group = line, col = \u0026quot;Violent and Sexual Offences\u0026quot;)) + geom_line(data = bakerloo_stops, aes(x = reorder(stn_name, stop_num), y = violent_crime_rate, group = line, col = \u0026quot;Violent and Sexual Offences\u0026quot;)) + geom_point(data = bakerloo_stops, aes(x = reorder(stn_name, stop_num), y = other_crime_rate, group = line, col = \u0026quot;Other Crimes\u0026quot;)) + geom_line(data = bakerloo_stops, aes(x = reorder(stn_name, stop_num), y = other_crime_rate, group = line, col = \u0026quot;Other Crimes\u0026quot;)) + xlab(\u0026quot;Bakerloo Line\u0026quot;) + ylab(\u0026quot;Violent vs Other Crime Rate per 10,000 passengers\u0026quot;) + theme_bw() + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_colour_manual(values=c(\u0026quot;red\u0026quot;,\u0026quot;blue\u0026quot;), labels = c(\u0026quot;Violent and Sexual Offences\u0026quot;, \u0026quot;Other Crimes\u0026quot;), guide=\u0026quot;legend\u0026quot;, name=\u0026quot;Crime Type\u0026quot;) We can see they show slightly different patterns along the route, and this may be something we want to investigate further!\nWe could also compare different data sources (for example for the bus routes I compared fare evasion data from three different sources!), or from different times (like the original paper suggests).\n Final remarks This has been one work-through of using Street Profile Analysis to visualise crime rate across a network like a tube line. This could be applied to any sort of route (eg bus route, or street network like the original paper describes) following very similar steps. As always, if there are any comments questions suggestions ammendments just reach out on twitter ( [@r_solymosi](https://twitter.com/r_solymosi) ) or by email reka.solymosi@manchester.ac.uk.\n "
    }
,
    {
        "ref": "/blog/hot-routes-tutorial/",
        "title": "Hot Routes Tutorial",
        "section": "blog",
        "tags": [],
        "date" : "2020.03.29",
        "body": "  Crime concentrates in place and time, and it is the task of the crime analyst to identify where and when these hotspots emerge. This is often achieved by producing density maps. Kernel density estimation is one methdod for producing such maps, which involves applying a function (known as a “kernel”) to each data point, which averages the location of that point with respect to the location of other data points. The surface that results from this model allows us to produce isarithmic maps, also referred to in common parlor as heatmaps (see our crime mapping textbook on GitHub for more on Kernel Density Mapping and a “how-to-in-R” tutorial).\nHowever, often we might be interested in how crime concentrates along a particular network such as along streets, bus routes, train nextworks, and others. In order to map hot spots along a network, we can use a technique called hot routes. It was used as early as 2003 by Andrew Newton to map crime and disorder on the bus network in Merseyside. A useful how-to guide was later produced by Henry Partridge and Lisa Tompson and can be accessed in the associated JDI brief or journal article.\nHot Routes was devised to be a straightforward spatial technique that analyses crime patterns that are associated with a linear network (e.g. streets and other transportation networks). It allows an analyst to map crime concentrations along different segments of the network and visualise this through colour.\nHere I will go through an example of how to apply hot routes in R. We will follow the 4 steps outlined by Henry Partridge and Lisa Tompson in the JDI brief:\nStep 1: Prepare the network layer Step 2. Link crime events to street segments Step 3: Calculate a rate Step 4: Visualise the results\nI will use the example dataset of the London Underground network, spatial data for which can be downloaded using the Transport for London API, and crime from British Transport Police for February 2020 available via data.police.uk.\nStep 1: Prepare the network layer The first step is to prepare our network layer. There are several substeps to this. First we need to acquire a shapefile for our network. Here we will keep it simple for the sake of the tutorial, and use only one line. Let’s go with the Bakerloo line. We can get this data using the TfL API. We can use R to make the API call for us by taking the query URL, and getting the results using the readLines() function in base R, and putting that within the fromJSON() function from therjson package.\nlibrary(rjson) #get json from TfL API api_call \u0026lt;- fromJSON(readLines(\u0026quot;https://api.tfl.gov.uk/line/bakerloo/route/sequence/outbound\u0026quot;)) ## Warning in readLines(\u0026quot;https://api.tfl.gov.uk/line/bakerloo/route/sequence/ ## outbound\u0026quot;): incomplete final line found on \u0026#39;https://api.tfl.gov.uk/line/ ## bakerloo/route/sequence/outbound\u0026#39; When you run the above, you might get a warning messae about incomplete final line, I’m not sure why, but we still get the object api_call in our environment, you can see it’s a large list with 10 elements.\nYou can have a look at this list, it has many interesting bits, but what I want to do here is extract the stops along the bakerloo line and their respective coordinates. There are probably much more efficient ways to do this, but here is mine (this is a tutorial on hot routes, not on parsing json files haha!)\n#parse df of stops and latlongs datalist = list() for (i in 1:length(api_call$stations)) { datalist[[i]] \u0026lt;- data.frame(stn_name = api_call$stations[[i]]$name, stn_lat = api_call$stations[[i]]$lat, stn_lon = api_call$stations[[i]]$lon, line = \u0026quot;bakerloo\u0026quot;) } bakerloo_stops \u0026lt;- do.call(rbind, datalist) Now you can see we have an object called bakerloo_stops which has 25 observations of 4 variables (station name, latitude, longitude, and line).\nIn order to carry out our spatial operations, we will be making use of the sf package. So I will load the sf package and also convert this list of stops to an sf object using st_as_sf() and build the line between the stops using group_by() (from dplyr package), st_union() and st_cast() (from sf):\nlibrary(dplyr) library(sf) bakerloo_stops \u0026lt;- st_as_sf(bakerloo_stops, coords = c(\u0026quot;stn_lon\u0026quot;, \u0026quot;stn_lat\u0026quot;), crs = 4326) bakerloo_line \u0026lt;- bakerloo_stops %\u0026gt;% group_by(line) %\u0026gt;% st_union() %\u0026gt;% st_cast(\u0026quot;LINESTRING\u0026quot;) Now we have a line and the set of stations along it. We can use the ggplot2 package to plot it like so:\nlibrary(ggplot2) ggplot()+ geom_sf(data = bakerloo_stops) + geom_sf(data = bakerloo_line) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) NOTE: I have added theme_void() AND theme(panel.grid.major = element_line(colour = \"white\")) because apprently theme_void() + geom_sf() have some issues together.\nRight, so that’s looking good. But here we have only one line. What we want is to break our line into sections. How you do this depends really much on what you want to show. In this case, it might be meaningful to consider each segment of the line between stops. In this case, we can use the shapefile of the stops (bakerloo_stops) to break up our line (bakerloo_line). However, as Henry and Lisa note, network layers typically contain streets of unequal length. This means that longer segments might show up as hot simply because they have more space to contain more crimes. Therefore in such cases it is advisable in this analysis to use equal length street segments, where possible. In this case however, let’s stick to the stops. To split our linestring (bakerloo_line) into many linestrings using the stops (bakerloo_stops) we can use the st_split() function from the lwgeom package and st_collection_extract() function from sf. Furter, as the st_split() function is expecting a blade argument of length 1, we can use the st_combine() (from sf) function to group our tube stations alltogether:\nlibrary(lwgeom) parts \u0026lt;- st_split(bakerloo_line, st_combine(bakerloo_stops$geometry)) %\u0026gt;% st_collection_extract(\u0026quot;LINESTRING\u0026quot;) parts ## Geometry set for 24 features ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -0.334896 ymin: 51.4945 xmax: -0.099185 ymax: 51.59222 ## Geodetic CRS: WGS 84 ## First 5 geometries: You can see we now have a new object called parts which is a linestring containing 24 features, the segments between our 25 tube stations, all as unique lines. You can also see that this is a geometry set of 24 features, let’s turn it into a simple features collection, and label each of the segments with a number by taking each element of parts and binding it together as a dataframe.\ndatalist = list() for (i in 1:length(parts)) { datalist[[i]] \u0026lt;- st_as_sf(data.frame(section = i), geometry = st_geometry(parts[i])) } bakerloo_sections \u0026lt;- do.call(rbind, datalist)  Step 2: Link crime events to street segments In this step, each crime event needs to be linked to the nearest street segment and the attribute table of the network layer updated with the corresponding count of crime.\nTo achieve this, first let’s get some crime data. I have downloaded all the BTP crime data from the police.uk website, and saved it in my local data folder. From here I can use read.csv() to import it, and similar to the stations, use st_as_sf() to turn it into a sf object:\ncrimes \u0026lt;- read.csv(\u0026quot;data/2020-02-btp-street.csv\u0026quot;) crimes_sf \u0026lt;- st_as_sf(crimes, coords = c(\u0026quot;Longitude\u0026quot;, \u0026quot;Latitude\u0026quot;), crs = 4326) Of course this download includes BTP data for the whole country. We don’t want this. Instead we can think about some threshold within which we care about our crimes. What I mean is, we want to first select all the crimes that we want to arrtibute to our specific network. This will vary with what you are plotting. In the case for example of a street network for the London Borough of Camden, you would want to include all crimes that are within the boundary of Camden. In the case of a bus route or a train route however, you might want to set some buffer, within which you are interested in counting crimes, but outside of which you believe they are too far to be attributable to your network object of interest.\nHow you choose the size of this buffer will depend on things like how accurate you think the geocoding of your data is, or other considerations.\nHere I just went with a coarse buffer of 0.005 arc degrees. The dist argument is assumed to be in decimal degrees (arc_degrees). This buffer distance is a units object, it should be convertible to arc_degree if x has geographic coordinates, and to st_crs(x)$units otherwise.\nSo I build a buffer of 0.005 decimal degrees around the line using st_buffer(), and keep only the crimes that fall within this buffer using st_intersection():\nbakerloo_line_buff \u0026lt;- st_buffer(bakerloo_line, 0.005) ## Warning in st_buffer.sfc(bakerloo_line, 0.005): st_buffer does not correctly ## buffer longitude/latitude data crimes_sf \u0026lt;- st_intersection(bakerloo_line_buff, crimes_sf) I can plot it all to see if it looks good:\nggplot()+ geom_sf(data = bakerloo_line_buff) + geom_sf(data = bakerloo_line) + geom_sf(data = bakerloo_stops) + geom_sf(data = crimes_sf, col = \u0026quot;blue\u0026quot;) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) Great, now what we want to do is snap each one of these crime points to the nearest line section (remember we have the 24 sections in the parts object).\nTo do this, we can use the st_nearest_feature() function. This will return, for each point, the ID of the nearest segment.\nbline_segments \u0026lt;- st_nearest_feature(crimes_sf, bakerloo_sections) bline_segments ## [1] 21 13 13 20 20 20 20 20 20 13 13 23 23 23 23 23 19 19 19 19 19 15 15 15 15 ## [26] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [51] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [76] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 1 ## [101] 1 1 1 1 1 1 1 1 1 1 1 1 1 7 2 2 2 2 2 2 2 2 2 2 2 ## [126] 2 2 2 2 2 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 10 4 4 4 ## [151] 4 16 8 8 8 8 8 8 8 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 ## [176] 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 ## [201] 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 18 18 18 18 18 18 ## [226] 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 ## [251] 18 18 18 18 18 18 18 18 18 18 18 18 18 18 4 4 4 4 4 4 4 4 4 4 4 ## [276] 4 4 4 4 4 4 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 ## [301] 12 12 3 3 3 3 3 3 3 3 3 3 9 9 3 9 3 3 3 9 9 9 3 3 3 ## [326] 3 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 3 9 3 3 22 22 22 22 ## [351] 22 22 22 22 22 22 It is simply a list of the ID numbers of matched line segments for each of the 356 crime points in the crimes_sf object. We can use this to create a frequency table and save this in a dataframe to be joined to the linesegment object. We will also need to replace our missing values with 0s, since in this case the segments which do not appear in our frequency table had 0 crimes snapped to them. We use the replace_na() function from the tidyr package.\nlibrary(tidyr) #make list of nearest into df of frequency sections_freq \u0026lt;- as.data.frame(table(bline_segments)) #make sure id is numeric sections_freq$bline_segments \u0026lt;- as.numeric(as.character(sections_freq$bline_segments)) #join to sections object and replace NAs with 0s bakerloo_sections \u0026lt;- left_join(bakerloo_sections, sections_freq, by = c(\u0026quot;section\u0026quot; = \u0026quot;bline_segments\u0026quot;)) %\u0026gt;% mutate(Freq = replace_na(Freq, 0))  Now we have an sf object with each section labelled with the number of crimes that were snapped to it as they were the nearest segment. So essentially, the number of crimes on (and around, depending on your buffer decisions) each segment. We could map this count line:\nmidpoint_crimes \u0026lt;- mean(bakerloo_sections$Freq) ggplot() + geom_sf(data = bakerloo_sections, aes(colour = Freq), lwd = 2) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Number of crimes\u0026quot;, midpoint = midpoint_crimes, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) But of course we want to account for things like the length of each segment as they are unequal.\n Step 3: Calculate a rate Next, to calculate a rate we need a denominator. In this case, length may be a good one, so the length of each street segment needs to be calculated. We can do this using the st_length() function. Since our data are all in WGS84 projection, this will return the length of each segment in meters.\nbakerloo_sections$length \u0026lt;- st_length(bakerloo_sections) Once we have all the lengths, a new column needs to be created in the network layer to record a crime per metre score. This is calculated by dividing the number of crimes linked to a street segment by its length.\nbakerloo_sections$crime_per_m \u0026lt;- bakerloo_sections$Freq / bakerloo_sections$length We now have our crimes per meter score! On to mapping!\n Step 4: Visualise the results The final step is to thematically shade each street segment with a colour (and line thickness if desired) that corresponds to the range of the rate of crime per metre.\nFor this, let’s convert our crimes per meter (crime_per_m) variable to numeric from a “units” object, and then use ggplot once again.\nbakerloo_sections$crime_per_m \u0026lt;- as.numeric(bakerloo_sections$crime_per_m) midpoint_rates \u0026lt;- mean(bakerloo_sections$crime_per_m) ggplot() + geom_sf(data = bakerloo_sections, aes(colour = crime_per_m), lwd = 2) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Rate of crimes per meter\u0026quot;, midpoint = midpoint_rates, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) And if we wanted to add thickness as well we can by specifying the size argument:\nggplot() + geom_sf(data = bakerloo_sections, aes(colour = crime_per_m, size = crime_per_m), show.legend = \u0026quot;line\u0026quot;) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Rate of crimes per meter (colour)\u0026quot;, midpoint = midpoint_rates, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) + scale_size_continuous(name = \u0026quot;Rate of crimes per meter (width)\u0026quot;)  All done! We have now managed to create a hot routes map of crimes on or near the barkerloo line recorded by British Transport Police in February 2020.\n Final remarks This has been a very simple application of the hot routes technique in R, and I hope this tutorial is helpful and can be applied to many other sets of data! If anyone is following along and gets stuck just get in touch with me on twitter ([@r_solymosi](https://twitter.com/r_solymosi)) or by email reka.solymosi@manchester.ac.uk. Also many of the solutions here are possibly hacky, if someone has better ways of doing this please just let me know, I am happy to improve it. As is always the case there are more sophisticated ways to do this, for example using the spatstat package (for a quick intro see section 6.7 in our crime mapping textbook), and probably many more, but this also does the job…!\nFurther, while hot routes is a really neat way to visualise crime rates along a network for sure, it may be the case that the geographic location of each segment or stop is not so important, and instead it is the ditribution of crime along a route that is the focus. In this case another visualisation technique that may be useful is street profile analysis introduced by Valerie Spicer and colleagues, detailed in this paper. I can always write another tutorial on this one in R, if there’s interest.\nAnyway, happy mapping friends!\n "
    }
,
    {
        "ref": "/blog/exploring-public-engagement-with-missing-person-appeals-on-twitter/",
        "title": "Exploring public engagement with missing person appeals on Twitter",
        "section": "blog",
        "tags": [],
        "date" : "2020.01.29",
        "body": " About this time last year I received funding from the Manchester Statistical Society Campion Grant to carry out some research looking into appeals for informaion made about missing persons on Twitter.\nThe motivation behind this is that police agencies globally are seeing an increase in reports of people going missing. These people are often vulnerable, and their safe and early return can be a key factor in preventing them from coming to serious harm. One approach to quickly find missing people is to disseminate appeals for information using social media. In fact, police, and other agencies, make frequent use of social media (such as Twitter) to send out appeals for information.\nThe goal of this project was to better understand how police accounts tweet appeals for information about missing persons, and how the public engage with these tweets by sharing them.\nTo achieve this goal we analysed 1,008 Tweets made by Greater Manchester Police between the period of 2011 and 2018 in order to investigate what features of the tweet, the twitter account, and the missing person are associated with levels of retweeting.\nIn particular we wanted to look at different features associated with the tweet, the twitter accounts, and the missing person, and any associations with engagement by the public, measured as retweets.\nRelated materials:\n The paper is now published in Policing \u0026amp; Society here: Reka Solymosi, Oana Petcu \u0026amp; Jack Wilkinson (2020) Exploring public engagement with missing person appeals on Twitter, Policing and Society, DOI: 10.1080/10439463.2020.1782409\n A pre-print is availabe on OSF here: https://osf.io/preprints/socarxiv/wugxs\n All of the materials (including data, data dictionary, the paper markdown) are on OSF here: https://osf.io/4w5eg/. The data was anonymised using NETANOS.\n The R package which contains all the code for analysis is available on GitHub here: https://github.com/maczokni/misperTweetsCode\n  Here I will highlight some of the most interesting findings.\nFeatures of interest First we identified features associated with public engagement that might influence engagement from a litearture review. We identified the following:\n Table 1: Table 1: features in the literature  Element Feature    Features of the missing person Race/ ethnic appearance, Gender, Age  Features of the tweet Time and timeliness, Post length, Punctuation and hashtags, Templates, Sentiment, Tone, Useful information, Photo (presence and valence)  Features of the account Number of followers, Age of account, Tweeting activity, Trusted source    We then went through our sample of 1,008 Tweets that were appeals for information about missing persons taken from the 56 GMP Twitter accounts identified for this study, and coded for all of these elements (except age, we abandoned it as this was too messy really…!). Please see the preprint for details on conceptualisation and operationalisation of these variables. Also for the full set of results, as below I will highlight only two of the most interesting ones, the paper itself contains many more!\n Two highlighted results First we looked at the importance of photos, in particular if people engage differently with a custody image versus a regular everyday photo (Figure 1). We found that custory photos are retweeted less than regular photos. Using multiple photos does not seem to matter. The point estimate is the median, and the arms represent the interquartile range.\n Figure 1: Figure 1: Retweets for different image types by gender and ethnic appearance  We also looked at the types of phrasing used in the Tweets, by coding these into different types of templates.\nHere are he list of templates identified through qualitative coding, with an example of each:\n Table 2: Table 2: types of templates present in Tweets by GMP about missing persons  Template No. of tweets Example    call 101 449 “Kristian Rennie, [NUMBER_458], is missing and believed to visit the [OTHER_1521] area. Any information please ring 101.”  original phrasing 255 “Missing for 22 x days, [OTHER_1765] [OTHER_1766] 31 yrs, last seen [OTHER_1767] Rd, Syke, Rochdale. [OTHER_290] sightings ring [OTHER_16] [OTHER_1772] 0161 856 9961”  … are concerned for.. 115 “***Missing*** [OTHER_1479] [OTHER_1477] 67 years! [OTHER_121] sightings please ring 101 we have concerns about [HIS/HER] health! http://t.co/820k9AQQ67”  #missing 83 “#Missing [OTHER_155] is described as being 6’3\" tall, short dark brown hair and stocky build 2/4”  please RT 77 “Another photo of missing person [OTHER_1479] [OTHER_1537] [OTHER_217] [OTHER_1539] [OTHER_1540] are growing #missing http://t.co/8vj8q8drKL”  can you help 72 “Can you help police find missing [OTHER_146] Stokes, 54, last seen in Salford? https://t.co/Dlj8B3cTKI… https://t.co/J7Eil7Wgfn”  have you seen.. 72 “Have you seen missing toddler [OTHER_1615] [OTHER_1616] and [HIS/HER] father Abdullah? [OTHER_1540] are growing https://t.co/5rOzIbWssA https://t.co/4z9vImUodj”  high risk 58 “High [OTHER_7] [OTHER_204] [OTHER_430] area!! [OTHER_1147] [OTHER_1278] 50 yrs old, 5’5’’,white female,blonde hair,brown coat,walking boots. [OTHER_217] RT”  **missing** 56 “*MISSING* [OTHER_1447] are appealing for help to find [OTHER_297] Curran, 48 yrs. [OTHER_297] is 5’10, slim build. [OTHER_297] was last seen in [OTHER_569] 2018. [HE/SHE] is known to frequent [OTHER_405] andPrestwich. Anybody with info should contact police on 101 or [OTHER_41] on 0800 555 111 (ref MP/16/0094141) https://t.co/izvgNU76bq”  link to info 40 “Have you seen this missing person, [OTHER_468] Pal? [OTHER_217] follow the link for more details. http://t.co/UmWZEBYH”  thanks 34 “Missing person…Charlie, who suffers from [OTHER_1476] has left without taking [HIS/HER] medication. Any sightings then please call 101. [OTHER_1446] you.”  … are appealing for.. 26 “Police are appealing for the public’s help to trace a missing [OTHER_115] man. http://t.co/8URE65femf”  urgent appeal 12 “Urgent! plz RT/Share! 9yr [OTHER_1503] [OTHER_1504] missing from [OTHER_1471] area. Small build, short dark hair, [OTHER_652] shorts, Cream/Brown stripe-T, [OTHER_1508] 101” “1013”    We compared the number of retweets between each template (Figure 2). The point estimate is the median, and the arms represent the interquartile range.\n Figure 2: Figure 2: Retweets for different templates   Conclusions These are just two interesting insights gained from exploring these tweets. In the full paper we explore a range of features associated with the appeals for information about missing persons made on Twitter by greater Manchester police.\nIn doing so, we uncover how the police currently construct such appeals, and whether we can infer any structure in the practice. We find that there is some structure, but there is also variation in how these messages are crafted, as well as in other features such as the type and quality of photo used, the phrasing and punctuation used, and the perceived sentiment that results.\nWe further considered how engagement, measured as retweets varies between these differently structured tweets, and draw conclusions about what we think might be important to follow up.\nIn sum, with this paper we provide an insight into how appeals for information for missing people are shared by a major UK police force, and how the public react to these messages. By doing so we serve as a reference point for an issue that is internationally relevant, affecting police and other organisations worldwide, and hope to spark future work in the area, preferably prospective or experimental studies to establish causal relationships between the features identified and engagement.\nRead the full paper here: https://osf.io/preprints/socarxiv/wugxs and do reach out if you have any thoughts/comments/feedback/questions/ideas for future research!\n "
    }
,
    {
        "ref": "/blog/crime-analysis-for-schools/",
        "title": "Introducing data and crime analysis to Wirral Hospitals' School",
        "section": "blog",
        "tags": [],
        "date" : "2020.01.14",
        "body": " On Monday (13th January) I gave a talk introducing pupils of Wirral Hospitals’ School to the world of data analysis, and specifically crime analysis. It was my first time speaking to a younger audience, so I had a bit of a task trying to think about what is the appropriate amount of content to include, and level to pitch to. I didn’t want to be too simplistic and therefore condescending, but I also didn’t want to bore them with jargon and technical details. In the end I think it went well, there was lots of engagement from the pupils, and the head teacher gave me some positive feedback about the content as well as the delivery. So I thought I’d write this up for anyone else looking for resources, thinking about going into similar school settings to talk about data analysis or crime analysis or both!\nPreparation The first thing I had to do was collect possible materials to use. I split this into three groups:\n Content about data analysis Content about crime analysis specifically Activities to engage the students  Content about data analysis and associated activities Firstly, I spent some time going through content which I had curated over the years of teaching and giving talks about data analysis. Some of my fave from this collection are visualisation of spells of Harry Potter. I also have Dear Data. The complaints postcard is a specifically good one to explain because you can immediately start drawing conclusions from it that people can relate to!\nComplaints postcard from Dead Data by GIORGIA LUPI and STEFANIE POSAVEC\n When I started writing the presentation I was actually on Vancouver Island with friends, and asked for advice there, and was recommended to think about #tidytuesday. Of course, this is an excellent resource of fun data sets, and also creative visualisations. Also pudding.cool’s rap artists’ vocabulary was mentioned as something kids would be interested in.\nBut even with such advice I thought I might not be reaching enough examples that are “down with the kids” (is Harry Potter still a thing?! Will it be awkward or funny when I mispronounce all the rap artists names?!) so I reached out to Twitter. Turns out this was an excellent idea, because I was met with a shower of useful resources.\nYou can see the thread of all the suggestions here: https://twitter.com/r_solymosi/status/1214835916637921282\nSo armed with all this, I had a very good selection of things to choose from now regarding data analysis in general. But what about crime analysis?\n Content about crime analysis specifically For this I thought I could introduce an easy-to-grasp framework, and work through two examples using it, one all together, and one as an activity. Specifically, I picked the SARA model to think about crime problems by analysing available information, and then maybe tapping into the students’ creativity for coming up with design solutions to tackle some crime problems.\nFor exercises, I initially thought about getting some data live, from police.uk, but I didn’t know what sort of devides they would have access to, whether or not there would be any sort of good internet connectivity, and so on, so for the sake of safety I decided to go analogue.\nSpecifically I picked two examples. First bicycle theft. For this, I followed the guide POP Centre Guide for Bicycle Theft by I use the popcentre report by SShane D. Johnson, Aiden Sidebottom, and Adam Thorpe as a guide, as well as all the info on http://www.bikeoff.org/. The site is great, especially the gifs explaining how some bikes get stolen, and the lots of creative designd for the response phase.\nThen, for the exercise to do together, I used the example of handbag theft from pubs, modelled after POP Centre Guide for Theft of Customers’ Personal Property in Cafés and Bars by Shane D. Johnson, Kate J. Bowers, Lorraine Gamman, Loreen Mamerow, Anna Warne. Here, I used not only slides to work through this together, but printed out materials, in sealed envelopes labelled as “Scanning”, “Analysis”, and “Response”, handed out to individuals or small groups (“Assessment” was done together as a large group).\nThe idea would be that each “phase” will have some handout materials, and pupils will in groups (and with me helping along as well) will follow along. So first, for the “Scanning”\" phase, I had 3 newspaper articles, all about handbag theft in pubs.\nMaterial for the “Scanning” phase\n Then, for the “Analysis phase”\", we had two conflicting bits of intel from two informants. One usually lies, and one usually tells the truth. So how can we know who is telling the truth or lying? Well we also have a map of the pub, with the number of bags stolen from each table, which we can use to colour in a heat map. Then, with this heatmap we can decide which informant tells the truth.\nPub floorplan for making heatmap of bag theft\n Once we know this, we can open the second analysis envelope, to get some more information from our truthful infomant, who tells us that he’s after bags thrown under chair or hung on the backrest. Now, we get to the response phase, and the envelope here contains two types of chairs and also a table, on which pupils can draw their creative anti-bag-theft designs. I did not manage to come up with a small-group based activity for the “Assessment” phase, so instead we together as a large group discussed the results of the actual evaluation study.\n  The presentation In the end, the presentation went as follows:\nFirst, I spent a few slides introducing myself. When discussing earlier drafts with people, a few people did mention that one of the things the students will be interested in before any of the content is my accent! So I figured I would include a little map to give background on where my accent comes from. Then I talked a bit about my academic and work-life journey to where I am now. Turns out this is really good, because the assembly topic before my talk was about degrees, and education, and career paths, so this fit in very nicely.\nThen I introduced data analysis with a one-liner (“Making sense of relevant data to identify patterns and answer questions”), followed by looking together at a particular data set: the #tidytuesday data set of Seattle’s registered pet names. I showed a screengrab of the data, followed by a chart showing that besides dogs and cats, the good peope of seattle also have pigs and goats registered. Then, I did a little bit of live coding, so that we could play the game “does your pet have a unique name?”. This was really great, because it immediately got many of the kids talking (and some of the staff!!). It was also interesting because it looks like there were pretty much no uniquely named pets, which we didn’t find at least one in the Seattle data base. Only at the end of the session (one girl came up after to ask about her dog, ‘lil marco’) did we find a unique pet name! This was excellend warm-up! Thank you #tidytuesday!\nI then showed some more examples: another #tidytuesday entry on squirrel survey in Central Park, the Harry Potter spells (still relevant!), the rappers vocabulary (they asked about Eminem, perfectly pronouncable!), property prices in the UK (maybe more enjoyed by the staff), and of course Dear Data.\nAfter this came the Skittles exercise, which was one of the recommendations I received from Twitter (thank you @duncanbradley_). This was also really great in getting pupils involved and interested. I got 12 packs of Skittles, handed them out, and asked the pupils if they think that any two between them will be the same. They were pretty convinced that they won’t be. One asked that surely this depends on how many are in each pack (correct!). So we started by counting all the skittles, which varied between 41-45 pieces. Next I asked someone to name their favourite colour skittle (was blue) and got them all to count their blue skittles. There were about 4 with the same number, so we moved onto the next colour, of which we only had a 2-way match, and the third favourite colour had already established that no two packets were the same (phew!). Then skittles were eaten, and we discussed how many packs are needed.\n468 packs of Skittles from possiblywrong.wordpress.com\n Then back to presentation mode, I showed some examples of crime analysis (the Trafford Data Lab’s dashboard for police.uk data was excellent), situational crime prevention examples (ticket gates, shattering pint glass), and then worked through the cycle theft example mentioned above using the SARA model. I asked, just before, how many of them had had a bicycle stolen, and half the room’s hands went up, while the other half seemed to know someone who’s bike was stolen. One girl mentioned she knew about bike marking (yay)! Then I asked who parks their bike on “informal” parking furniture. Again, almost all hands went up (including mine to be fair, we need more cycle infrastructure!!). So when I showed the gif for “lifting”, it got some gasps regarding how easy it is to steal such bikes.\nHow bikes are “lifted”\n Finally it was time for the example of handbag theft from pubs. We opened the Scanning envelope, and had three articles. I emphasised “think about what you think the crime problem is as specifically as possible”. Well, not only did they come up with “theft of handbags from pubs” but a lot of detail, such as that thieves go for flashy expensive looking bags (tapping into some CRAVED characteristics there…!) and that its because the owners are being careless, distracted, or otherwise inadequate guardians (I swear I did not show a crime triangle!!). Having established this specific problem, we reached out to our two informants, receiving some conflicting intel. One says he works near the doors, while the other likes to target the busy dance floor. It was time to colour in the heatmap to establish who is lying and who is telling the truth. This was also pretty easily tackled by the students, and we quickly moved on to trust out informant “B” and get some more information about how he steals handbags, and use this to open our response folder, which had the three pictures of furniture: two chairs and a table in it, which students drew their designs on. There were some really good ideas (one student actually drew the grippa clip, without having seen them before!) and I was impressed that everyone came up with at least one solution!\nWe wrapped up by talking about the real study, the outcome of the evaluation, and then some general questions about crime analysis.\n Thoughts Overall I really enjoyed this experience, and would gladly do it again. The students were really engaging and welcoming and I had a lot of fun listening to their ideas and designs. For anyone thinking about going into your local school, mainstream or non-mainsteam, I strongly encourage this. Especially if you’re early careers! The headteacher said a really nice thing to me at the end, she said that because students perceive me as closer to them in age, they might start to see this sort of career and academic trajectory as open and accessible to them, if it’s not something they would have thought about before. Of course many of these students are exceptionally smart, and are going to go on to great things without any input from me, but if I could give a bit of inspiration to one person, I’m pretty happy about it.\nAnd if you reading this are interested to go give a similar talk in a school, please feel free to use my materials, and just let me know if you want to talk about my experience at all!\nThe slides are available here: https://www.dropbox.com/s/gap5ibu3sc3lsjy/wirralschool.pptx?dl=0\nAnd the handouts are available here: https://www.dropbox.com/s/scb2ccjqtzrxy4m/handouts.pptx?dl=0\n "
    }
,
    {
        "ref": "/blog/asc-2019/",
        "title": "ASC 2019",
        "section": "blog",
        "tags": [],
        "date" : "2019.11.18",
        "body": " Last week I was at the 2019 annual meeting of the American Society of Criminology (ASC). It was a great opportunity to see friends and colleagues, learn about new developments in the field, and present my paper on environmental features associated with sexual harassment.\nThis was my 3rd ASC (2nd in sunny San Francisco) but my first without a “crime and place“ stream. This meant that instead of sitting in one room and waiting for the presentations to come to me, I had to curate my own experience. With 1,301 sessions (according to the programme) there were definitely a lot of options (4.110534247 times 10 to the power of 3488 possible alternate conference experiences to have, to be exact…!), but here I will summarise my version of this conference and highlight some interesting (to me) papers, breaking things down into themes.\nFear of Crime There were quite a few presentations about fear of crime, with many emphasising the importance of the situational context in which fear is experienced. It is so exciting to now see fear of crime research back on the map (har har); I am noticing a bit of a boom in this area with more and more scholars getting excited about perception and safety in the environment.\nI particularly liked Valerie Spicer’s (Simon Fraser University, Canada) presentation about using street intercept surveys to measure changing perceptions of safety in specific locations. Street intercept surveys use a location-based sampling approach (stopping people to ask questions on specific street intersections). This allows for the mapping of responses, and through a longitudinal design, we can see maps of perception changing over time. For example, they noticed a shift in the location of fear hotspots with changes in the environment (for example the opening of a new transit hub). This method also allows for the sampling of people possibly not captured by other traditional sampling frames, such as homeless people who may have very different fear patterns to other users of public spaces. I particularly liked that the research team asked not only about fear of crime, but also about community strengths, focusing on the positive experiences people have with public spaces as well. The paper is available here. The below image is one of the maps from the paper showing location-specific measures of fear of crime:\nmap of perceived safetymeasured with street intercept survey from Song \u0026amp; Spicer\n Since building a mobile application to measure fear of crime (see paper here) my thing is really apps for fear of crime research (even have a review of all these for which you can read the preprint here). So of course I was very excited to see Michael Chataway’s paper (Queensland University of Technology, Australia), presented in the same session, which used a mobile app, and similarly to Valerie Spicer’s paper emphasised this shift in focus to perception of safety instead of unsafety. He outlined that absence of fear is not necessarily the same as feeling safe. He suggests to ask about safe places, in order to explore their characteristics, and see how they differ from places perceived as unsafe. For example, he found that a feeling of “at homeness” was strongly associated with feeling safe. This is mirrored in my PhD research finding that “familiarity with an area” is associated with reduced odds of feeling worried about crime. I look forward to seeing more work develop in this area. This paper is available here\nPs: if anyone is interested in apps for social science research, Michael and I have a webinar available hosted by the UK Data Service here\nIn another fear of crime presentation, James Hurst (Universiry of Arkansas Little Rock,USA) carried out location-based retrospective surveys to ask about people’s perception of safety in specific university campuses. He originally asked about feeling unsafe at different times of the day, but actually found that there was almost no variation in perception between the different times. This is contrary to what I found in my mobile-app based study in London (see above-mentioned paper) , where time of day actually did matter. I would be really interested in finding out more about whether this is an artefact of the measurement tool used, or something else may be going on there. I cannot currently find a link but have asked James and will update here.\n Transport crime Many of the transport presentations could technically fit under the fear of crime label as well, but I’m separating them out anyways, because transport research also merits its own section due to the volume of emerging work.\nIt was great to see the presentations about the international study on college students’ experiences of fear and victimisation on public transport across the world, coming from the book of Vania Ceccato (KTH, Sweden) and Anastasia Loukatou-Sideris (UCLA, USA). The book is called “Transit Crime and Sexual Violence in Cities: Internatoinal Evidences and Prevention” and will be published by Routlegde next year, keep an eye out for it my transport nerdy friends! Here, four of the chapters were presented, Stockholm, Bogotá, San José, and Lagos. It was interesting to see similarities (and differences) in students experiences across these vastly different contexts. Andrew Newton (Nottingham Trent University, UK) and I have a chapter in said book about London and it was nice to see how other researchers approached the same issues worldwide.\nIn another session Elenice De Sousa (Montclair State University, USA) also explored fear of crime on different types of transport in Belo Horizonte, which, opposite to the presentations mentioned above, actually found that lower fear was not associated with frequent ridership. The case of Bogotá is further interesting due to having two very different bus routes one with many security features and another without, creating very different environments for passenger safety. I cannot find a link to this paper, but Elenice has written about bus crimes in Brazil before, so if you want an idea, that paper is available here\nI also liked the presentation by young scholar William Johnson from George Mason University who is working with Barak Ariel (Cambridge, UK) looking into the crime preventative impacts of platform edge doors. They had data from London Underground and considered the role that PEDs can play in preventing many of the crimes (and non crimes) which occurred. He also coined my favourite americanism of the conference by calling the BTP “British Transit Police”, which I liked because if only the letters weren’t capital it would be absolutely accurate. Anyway it is always so good to see more young scholars working in the area of transport crime! I believe this paper is still forthcoming, so I’ll keep an eye out for it.\n Methodological advancements Papers that emphasise interesting methods are often my fave at conferences because they offer some new way to approach a topic, and have the potential to further understanding in so many areas of criminology and wider social sciences. There were a few neat things like this presented at ASC.\nGoing back to perception research, I’ve been working on putting together a project with Emily Moir (Griffith University, Australia) to use virtual reality (VR) scenarios to explore guardianship in public places, and how different interventions are perceived by the target being protected. So I was very happy to see Jean-Louis van Gelder (University Twente) present a paper about using VR to study offender decision making by comparing people’s responses to text based vignettes and VR scenarios of a bar fight capturing various related outcomes. This paper is already published here. The below image is from this paper showing the filming process of the “bar fight” scenario:\nman wearing 360 degree camera to film bar fight vr scenario from van Gelder et al.\n There was also a paper introducing the importance of considering local level relationships between outcomes and their covariates, rather than focusing on the global, by using geographically weighted regressions in crime research by Martin Andresen (Griffith University, Australia), and a paper using causal mediation analysis by Krisztian Poch (LSE, UK) to study normative and non normative approaches to duty to obey (paper currently under review for Journal of Experimental Criminology). Both are methods I now want to learn more about.\n Academic integrity Finally, the session I was most excited about actually was not really any paper, but the ASC Forum on Academic Integrity. I’ve been really interested in this topic since Kim Rossmo advised to read “Rigor Mortis: How Sloppy Science Creates Worthless Cures, Crushes Hope, and Wastes Billions” by Richard F Harris. I super strongly recommend this book to anyone interested in how to improve science. The forum was set up in response to a recent (very well publicised) case of a retraction request of a paper by one of the co authors. If you are not familiar with the case, there is a medium article for you to read here and Justin Pickett’s (University of Albany, USA) piece on OSF available here\nThe session consisted of an opening by the panel (Sally Simpson and Laura Dugan (University of Maryland), and Daniel Nagin (Carnegie Mellon University)) where they outlined that they will not be discussing the case specifically but are looking for suggestions for improvement and to address questions and concerns. The main outcome so far seems to be that ASC journals are now subscribed to the Committee On Publication Ethics (COPE). Beyond that they are seeking suggestions, so if anyone has ideas now seems like a good time to reach out.\nThe discussions were across a range of issues, from the rights of whistleblowers, to citation cartels, to whether we trust internal investigations (many criminologists seem not to when thinking about the police so why don’t we apply the same standards here?). A few people mentioned to consider who is affected, not only the authors and editors and academic institutions, but also consumers of these outputs; practitioners who wish to implement research findings, and study participants or members of the communities studied who may benefit from the research should all be able to trust in them.\nThe session was recorded, so it will be possible to see all this in case you’ve missed it. I will update here when I have found access to it with a link. It was overall great that a discussion is starting to take place and personally I was really honoured to meet Justin Pickett and shake his hand for being so brave and driving change in criminology ahead.\n Final thoughts Overall it was a great ASC. I saw many more inspiring and engaging sessions (eg on guardianship with great papers by Emily Moir, Danielle Reynald, and Zharina Vakhitova (all Griffith University, Australia), and a paper on a randomised experiment introducing super intense flood lights to the streets of New York City to reduce crime by Aaron Chalfin from University of Pennsylvania, USA) and got to reconnect with colleagues from across the globe, which all makes me feel so so lucky to be part of an exciting and evolving field of research. I hope to return again soon.\n "
    }
,
    {
        "ref": "/blog/halloween-mcr/",
        "title": "Halloween MCR",
        "section": "blog",
        "tags": [],
        "date" : "2019.10.30",
        "body": " On 29 October 2019 I gave a short presentation at PyData Manchester and Open Data Manchester joint meetup on the topic of Data Horror Stories. My talk was a data-driven exploration of the massive inflatable Halloween monsters of Manchester.\nHalloween is the best  Boys and girls of every age Wouldn’t you like to see something strange Come with us and you will see, This our town of Halloween - The Nightmare Before Christmas\n I friggin’ love Halloween! I get really into it. Last year, we organised a Halloween themed all day #rstats conference/meetup/thing, ( you can read all about that here), which involved some fantastic pumpkin carvings:\n…and me dressed up as the broom package\n(far right, with a broom and some functions taped to me…!)\nWhen I first moved to the UK 9 years ago, it was not really a thing at all, but more recently, it has become adopted as a fun and spooky holiday for all the age groups. In Manchester for example there are now all sorts of Halloween related activities. One of these is a set of inflatable monsters which are dotted around the city centre. You can download a map to hunt for them all here.\nNaturally, the monster walk was something very appealing to me, and we set out to walk the monster walk, take nice photos, and then pick our fave monsters over some beers. In our household, the hands down winner was “Blob”:\nblob\n But what about the rest of the city? Which was Manchester’s favourite monster? This was the question I set out to answer in this talk\nimage credit: [@OpenDataManchester](https://twitter.com/opendatamcr/status/1189253327550406657/photo/1)\n MCR Monsters One way to gauge what monsters people are photographing and sharing is to look at Instagram. I found two key hashtags that were relevant:\n #MCRMonsters and #HalloweenMCR  I wanted to select posts that used both hashtags, because a lot of what was coming up with just one or the other was actually not monster related content (at least not in the sense that I was after).\nTo acquire a set of photos with these hashtags and some of their metadata, I used the instaloader tool. Specifically to get only posts that had both hashtags, I modified this bit of code by aandergr. My version can be found on github here: https://github.com/maczokni/halloweenMCR. This was the only bit of Python I used however, and then I swiftly read my retreived JSON into R.\nAfter some cleaning I had a nice bit of data with some Monster photos and associated metadata. However, none of these told me which monster is in each photo. So this required some manual coding, where I looked at each photo, and coded what monster I saw.\nFinally, after all this was done, the results could be considered\n And the winner is…  I was working in the lab, late one night When my eyes beheld an eerie sight For my monster from his slab, began to rise -Bobby Pickett - Monster Mash\n So finally we can get to some results.\nFirst I considered number of posts:\nWell it seems like this round has been won by the dragon who lives atop the Printworks. Okay…\nprintworks dragon\n What about the most likes?\nYess, Blob back in the lead!\nBut this measure is still weighting the number of photos taken, as more photos mean more likes. What about likes per photo?\nWhat is this?! Well in this case it seems I get as winner something I tagged as “fake”. While it is definitely not a current monster (currently in its place are the “orange eyes”), after further investigation, I think maybe it is not fake but an image from last year. This is the image in question.\nIn either case, since I filtered for images in 2019 only, it should not be there, and is therefore DISQUALIFIED.\nSo intead the winner is….\n…BLOB! What a champ\nblob\n  All is well that ends well In conclusion, it has been a fun exercise to play a bit with the Instragram API and see what sorts of information I can get out of it. Number of likes, also replies, and the URL to the photos. I want to explore more.\nI also noted that there is this “may contain” feature, which has some sort of image recognition application to help describe posts for those with visual impairment. I used this to query some dog photos for example (everyone loves dog photos!). A simple string contains search and boom, I hav dog + halloween monster photos!\ndog_pics \u0026lt;- tagged_monsters[grepl(\u0026#39;dog\u0026#39;, tagged_monsters$may_contain),]  See the two results I got back here and here.\nThere is much more to explore though; originally I was hoping to get information about what filters people use, based on this paper about instagram filter choice being able to diagnose depression, but I did not get this info with instaloader. I guess I will keep exploring what is out there.\nFor anyone interested, all my code for this is on my github page.\nHappy halloween!!\n "
    }
,
    {
        "ref": "/blog/webscraping-and-some-sampling-mapping-in-r/",
        "title": "Webscraping and some sampling \u0026 mapping in R",
        "section": "blog",
        "tags": [],
        "date" : "2019.08.10",
        "body": " Presenting at R Sheffield Last week I had the pleasure to present at the Sheffield R User Group alongside former PhD colleague (and roommate) Joanna Hill who is currently based in Uganda, working remotely at Rutgers University. It was a great event, set in a meeting room upstairs in The Red Deer pub, which gave it a nice informal feel. The attendees were all welcoming, knowedgeable, and very engaged and engaging, so it was a great experience and I recommend to anyone in the area. The one negative was that trains weren’t running due to floods so I had to drive and therefore not fully immerse the presenting-in-a-pub-with-a-pint aesthetic. Oh well, there is always next time, and at least we got to see some nice peak district views.\nPeak views (photo by Jo Hill)\n  !!!DISCLAIMER!!! I am no expert on scaping data from the web. In that I really only engage with this problem when I find an interesting source of data, then scarpe this, and then abandon it forever. So DISCLAIMER: there are probably much better, more effective/efficient ways of doing this.\nIn fact someone in the meetup mentioned rvest. I haven’t looked into it yet but an initial look is already super exciting, and I urge anyone interested in webscrping to check that out, as it’s probably loads more useful than my hack-together approach.\nThat said, I do use my hacky approach to get myself exciting data sets sometimes, such as data from FixMyStreet, and people have asked me before about how I do this, so I thought this could be a good chance to share that. So what follows is a cliffnote of the talk:\n Finding some data about experiences of sexual harassment Sexual harassment is a widespread global issue. 75% of all women worldwide (~ 2 billion) have experienced sexual harassment. 35% of all women worldwise (~ 930 million) have experienced some form of sexual/physical violence (source: World Health Organisation, 2017).\nOne approach to tackle sexual harassment is to use a Crime Science framework. Crime science\n applies scientific method to the study of crime and security problems with the aim of reducing harm  (source: Cockbain, E., \u0026amp; Laycock, G. (2017). Crime Science. Oxford Research Encyclopedia of Criminology)\nHowever one issue with sexual harassment is that sexual harassment and violence are massively underreported:\n in England and Wales, 1 in 6 people who had experienced rape or assault by penetration (17%) had told the police  (source: Office of National Statistics)  in India, fewer than 1.5% of victims of sexual violence report their assaults to police  (source: McDougal, Krumholz, Bhan, Bharadwaj, \u0026amp; Raj, 2018)   This means that we struggle to gain information about the situational features associated with sexual harassment and violence.\nOne solution can be to make use of crowdsourced data, as I have done in previous projects looking at fix my street reports and spatial behaviour of those who report. In particular, there is an online platform called Safecity.\nhome screen of safecity.in\n Safecity is a platform that crowdsources personal stories of sexual harassment and abuse in public spaces. This data which maybe anonymous, gets aggregated as hot spots on a map indicating trends at a local level. The idea is to make this data useful for individuals, local communities and local administration to identify factors that causes behavior that leads to violence and work on strategies for solutions (source: Safecity).\nTo submit a report, you click on a map and find the location where the incident took place. Then you fill out a short form that asks for a title, a description, the time of the incident.\nform for submitting report\n These reports are then displayed on the website.\na screenshot of one report\n These reports are viewable one by one, but also, lucky for the potential data-hungry researcher, their URLs are sequential. What I mean is that, if one report is safectiy.in/.../report/12345 then the next ones are safectiy.in/.../report/12346 and safectiy.in/.../report/12347 etc. So, if we can write a script to open each page, and take the data we need, and then move on to the next one, we can iterate through each report, from first to last, to build a dataframe of these reports.\n Extracting the data we need So how to extract the data we need? Well as a first step we need to think about the variables of interest. One good starting point is the form that someone reporting an incident would have to fill out. We can see it has for example a ‘title’. Great so let’s get the title for each report. To do this, we will need to see what html ‘tags’ this title is demarcated by. So fir this, first view the page source by right clicking somewhere on the page, and selecting “View Page Source”. So here I have the source for the report http://maps.safecity.in/reports/view/11679\nview page source screenshot\n My approach here is to start searching for the tag in this source. For exaple, I can see that the tag here is \"report-title\".\nscreenshot of report-title in source\n So I make a note of this, and any other tags that I will need to locate the variables that I want to scrape into my datasets.\nMy approach to selecting the variables required is to start by downloading the entire page. I do this with readLines(), which grabs all the lines into a list object. Here I use the url() function to get all the lines directly from the web page for which I have the url address. In this case that address is http://maps.safecity.in/reports/view/11679. Let’s grab all the lines for this into an object I will call all_lines.\nall_lines \u0026lt;- readLines(url(\u0026quot;http://maps.safecity.in/reports/view/11679\u0026quot;)) This all_lines object now contains all the lines of html that we could see earlier when we used the “View Source” option in our web browser. If interested we can print this to the console just to see…\nAnyway, usually we are not interested, instead we want to select the lines of interest. So for example, we want to select the line which has the title of the report that is displayed on this page. But how to find this line?\nOne approach is to use grepl(). This function uses pattern matching to return TRUE where a pattern is found in a string. For example, grepl(\"a\", \"abc\") returns TRUE, while grepl(\"z\", \"abc\") returns FALSE. So, using the tag we identified earlier, “report-title”, we can use grepl to find the line where it is present.\nWe can then use subsetting (here I’m using square brackets) and the which() function, which returns the TRUE indices of a logical object, allowing for array indices. For example, which(grepl(\"a\", c(\"abc\", \"xyz\"))) will return 1, and which(grepl(\"z\", c(\"abc\", \"xyz\"))) will return 2.\nGoing back to the case of extracting the title of the report from our webpage, we can create an object called title, and use subsetting and grepl() and which() to assign to it the line which has the “report-title” tag.\ntitle \u0026lt;- all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))] This is nice, but you can see it returns the entire line, html tags and all:\ntitle ## [1] \u0026quot;\\t\\t\u0026lt;h1 class=\\\u0026quot;report-title\\\u0026quot;\u0026gt;Stalking\u0026lt;/h1\u0026gt;\u0026quot; To clean out the HTML tags I make use of a function (obviously lifted from Stackoverflow):\ncleanFun \u0026lt;- function(htmlString) { return(gsub(\u0026quot;\u0026lt;.*?\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, htmlString)) } Unfortunately for me, this has not removed the tabs:\ncleanFun(title)  ## [1] \u0026quot;\\t\\tStalking\u0026quot; Now again, there are probably better ways to do this but I am too lazy to look this up so I use more pattern matching with the gsub() function to get rid of those:\ngsub(\u0026quot;\\\\t\u0026quot;, \u0026quot;\u0026quot;, cleanFun(title)) ## [1] \u0026quot;Stalking\u0026quot;  Using the above to build a dataframe Okay so that should give you an idea of how to extract a variable of interest from one page. We did this for title but you could do it again easily for other features of interest, such as the description, or the longitude/latitude for mapping. And more importantly, you want to do this for multiple reports, to append them all together into a dataframe.\nI mentioned before that the URLs for these reports are sequential, in that report 1234 is followed by report 1235, 1236, 1237, and so on. You may guess where I’m going with this: it is possible to write a loop that will repeat this extraction action for all the urls in a list. Again, I know for loops are evil in R, but this is where I’m at.\nSo first things first, I create an empty list to save all my dataframes into. I’ll call ic (creatively) datalist. I also need to create a counter j here. (Note: I only create it here because I don’t start my loop from 1. Mostly because this is a toy example. If I were getting all the reports (in fact when I got all the reports) I would build my loop with 1:number of reports, so I could simply use i to index my list of dataframes. )\ndatalist \u0026lt;- list() j \u0026lt;- 1 Now that I have these essential items, I write a loop, which will count from 11676 to 11679 to iterate through 4 reports (http://maps.safecity.in/reports/view/11676, http://maps.safecity.in/reports/view/11677, http://maps.safecity.in/reports/view/11678, http://maps.safecity.in/reports/view/11679) and for each one, repeat the steps discussed above to extract a title (and also duplicate for description), and save into my list object called datalist.\nfor (i in 11676:11679) { all_lines \u0026lt;- readLines(url(paste0(\u0026#39;http://maps.safecity.in/reports/view/\u0026#39;, i))) datalist[[j]] \u0026lt;- data.frame( title = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))])), description = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;Description\u0026quot;, all_lines)) + 1])) ) j \u0026lt;- j+1 } (Note: I also increase my index j there, again if you start from report 1, this is not necessary).\nWhen this is all finished, I am left with a list of dataframes, so my remaining task is to bind the list of data frames. Because I’ve been so hacky with everything I want to make up for it and inject some tidyverse into the mix, so let’s use the bind_rows() function from dplyr to do this.\nlibrary(dplyr) safecity_data \u0026lt;- bind_rows(datalist) Now we have a dataframe called safecity_data which we can have a look at here:\n  title description    TOUCHINGS The girl was being touched by her classmates who are boys on her buttocks.  TOUCHINGS A teacher is touching girls on their buttocks and canning their buttocks too.  Stalking A man kept following me.. It was scary.. He kept saying something  Stalking A man kept following me.. It was scary.. He kept saying something    As I mentioned, this is a toy example, but it should provide you with a good idea about how you can go about replicating this for more variables, and across more URLs. One thing I did not mention is error handling. It is likely that not all URLs will lead to a valid page, for example reports may get removed, or for other reasons. For such cases it is important that the code you run has a way to handle such errors. In my work I used tryCatch(), which worked excellently.\n Map and sample the reports Once you had all your variables (including spatial data such as Longitude and Latitude) and a sizeable data set, it is possible to put these reports on a map, and use spatial information to sample from these reports.\nThe first step to take for this is to make the data spatial. Currently, while the data may have a Longitude and Latitude column, these are not recognised as a geometry. To achieve this, you can use the sf package. Sf stands for simple features. Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them ( source: R Spatial). Check out Lovelace, R., Nowosad, J., \u0026amp; Muenchow, J. (2019). Geocomputation with R. CRC Press. for a great resource on all things spatial in R with sf.\nFor this example here, I’ve got a larger data set from my earlier webscraping work so let’s use the function st_as_sf() from the sf library to turn the long and lat columns into geometries:\nlibrary(sf) safecity_sf \u0026lt;- st_as_sf(safecity, coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;), crs = 4326) Having done this, we turn the flat dataframe into an sf object, and it becomes incredibly smooth to map the data, using our trusty old ggplot:\nlibrary(ggplot2) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + theme_bw() By turning our data into an sf spatial object, and using geom_sf() it becomes possible to plot our data as a ggplot. But the above isn’t giving us loads of context really. We can possibly guess that the blob of points is India, but unless we’re great geograpgers we may encounter trouble trying to guess where our out-of-India points are…\nOne way to quickly give some context and explore the background is to make use of the shapefiles in the rnaturalearth package, an R package to hold and facilitate interaction with Natural Earth map data. Read more about its usage here.\nThen, we can use the function ne_countries() to request a vector shapefile of all the country outlines across the world. In the parameters we specity that we want the resulting object to be of class sf, as well as set out the fill and outline colours.\nlibrary(rnaturalearth) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() This time we can see better that yes all our points are in India, but we seem to have some reports from the USA and the UK as well.\nWhich brings us to the sampling option. What if I wanted only those reports that were made in India, and to exclude the other ones? Well we can make use of the specific shapefile from the rnaturalearth package to subset our point list to only those which intersect with the India polygon. So let’s create a sf object for India:\nindia \u0026lt;- ne_states(country = \u0026#39;india\u0026#39;, returnclass = \u0026#39;sf\u0026#39;) To then select only reports made in India, we can use the st_intersects() function from the sf package, which will return TRUE for all points which intersect our polygon of interest. Then we can use that set of points labelled with TRUE to subset our original dataframe. Like so:\nindia_safecity_int \u0026lt;- st_intersects(india, safecity_sf) india_safecity_sf \u0026lt;- safecity_sf[unlist(india_safecity_int),] Now we can make sure we did everything right, and map our India only reports\nggplot() + geom_sf(data = india_safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() We now have a fantastic, spatially explicit data set of people’s experiences with victimisation from sexual harassment across India. It can be now used to perform other mapping exercises, and in my presentation I mentioned tmap for creating thematic maps smoothly, and sppt for running various spatial point pattern tests to compare different point sets.\n Wrapping up Overall I hope the above is useful as a bit of a guide into thinking about scraping some data from the web that may fill a gap in knowledge or understanding around a specific problem, which may help gain further insight and achieve good outcomes (possibly reduced prevalence of sexual harassment, or other societal ills). As I mentioned, there are probably other better/more efficient ways of doing this, but I thought I would share what I have been doing here.\nI am actually writing up a paper from the data, so I will share this later on here as well, for anyone interested. I will be presenting a version of this paper at the ASC so if anyone will be there then come see what we got up to with all these data!\nOn a final note, thank you to Anna Krystalli for inviting me to speak at R Sheffield, I really do recommend anyone in the area to attend this meetup, and if you have something to share to get in touch with Anna. I hope that I can attend another meetup soon, hopefully when the trains are back up and running so I can make up on that lost pint…!\n "
    }
,
    {
        "ref": "/blog/data-visualisation-summer-school/",
        "title": "Data Visualisation Summer School",
        "section": "blog",
        "tags": [],
        "date" : "2019.07.15",
        "body": " Learning to visualise data Last week I had the opportunity to take a one-week course by Andy Kirk about data visualisation, hosted by Methods at Manchester as part of the Summer School programme. It was a fantastic experience to be a student again, and I learned a lot about practical considerations that go into producing effective visualisations that are trustworthy, accessible, and elegant.\nI will not (and cannot) summarise the course here, I recomment anyone interested in creating data visuailisations to get in touch with Andy for courses. But I did want to quickly summarise some key take aways, list some really awesome tools, and show off some creations from the course.\n Take aways I really liked how the course was software agnostic, instead of teaching how to create particular plots with R or Tableu or something specific, the aim was to teach design thinking, and to take time to consider everything that goes into the data visualisation process.\nThe most useful thing for me to structure my approach to visualisation I think came from the 4-step breakdown of the visualisation process. It really made organising my thinking about how to get from question to visualisation easier. The steps are:\nFormulate a brief - why are you doing this? what/who is it for? what question is it answering? Work with the data - think about what data is required and also what format the data need to be in to allow the visualisation to be created Establish editorial thinking - really focus on what you want to say, to guide how to say it. we talked about the angle of the approach, and framing in relation to this. Develop design solution - finally think about the key decisions that go into the visualisation design: data representation, interactivity, annotation, colour, and composition, and how these all contribute to creating a trustworthy, elegant, and accessible piece.  Finally, many of the exercises involved looking at existing pieces of visualisation and really thinking about what I like and don’t like about them. This turned out to be a really good starting point to think about what is a good and not so good way to develop my own visualisations. Not only that, but using a google sheet to collect class responses to answers to “how much they like/dislike” some elements really got a discussion going and good engagement - something I might try in my teaching going forward.\n Resources Andy provided loads of great resources for people to use and refer back to when creating visualisation projects.\nOn his site visualisingdata.com there is a resources tab which lists a pretty much never-ending list of tools. Some which stood out to me where:\n D3.js - I feel like this is the ultimate visualisation tool and I’ve dabbled with it here and there but I cannot find the time to buckle down and get to grips with Javascript. Not that I haven’t tried, I do remember trying to start a book club going through Eloquent Javascript but it does require an inital time investment, and when the good people of open source are wrapping all these javascript libraries into R packages, then my main incentives are removed. But it is definitely on the todo list.\n R - R is so good for data visualisation, and so much easier to learn than Javascript (to me anyway) and also deals so well with all the data manipulation side of things, so it’s a 10/10 from me.\n Flourish - this tool has lots of pre-programmed charting options, and is free to use for public data. I think if you need to keep the data private it begins to cost though…\n RAWgraph - seemed to be this GUI for creating D3 visualisations. So if you’re interested in creating non-standard chart types, this look like a super easy way to do so, and is free.\n Gephi - useful for network visualisation \u0026amp; easy to use and also totally free.\n  In addition to these, during one of the group exercises our group discovered word art - no, not the late 90s MS Word 3D rainbow coloured clipart thing, but a tool to make a wordcloud take the shape of any image you want. I know, I know, word clouds are “the mulletts of the internet”, but we did use this to produce some neat visualisations of text, so hey, mulletts can be useful too. For our challenge visualising data about art collections, we looked at words used to describe representations of women in modern art and egyptian art:\nAnother useful tool was the Chartmaker Directory a crowdsourced collection of all the different charts you could think to use for your data, and a set of all the tools that you can make them in. And if there is something missing which you think should be there, you can submit for it to be added. Very useful tool to help you create your descired visualisation no matter what tool you use.\n Exercises The really neat part about being a student again was to explore cool and totally not-related-to-my-work data sets for visualisation. The first one of these was the scripts for the original Star Wars trilogies. The task was to think about what we want to visualise from the data and how. The dataset was split over 3 sheets in Excel (one for each film) and only had 3 variables, sequence (the lines numbered 1-n from first and last line spoken in each film), name of the character speaking, and the line which they said. Inspired by the New York Times visualisation about Peyton Manning’s Touchdowns I decided to see who speaks the most in the films. I used R, so I’ll include the code for the graph here too. I don’t know if the data is up for sharing, but you can easily find transcribed films online, so could reproduce with such data:\nlibrary(readxl) library(ggplot2) library(tidyr) library(dplyr) library(stringr) library(janitor) #read in each sheet, create a variable to tag film, and merge into one dataframe newhope \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeIV\u0026quot;) %\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) newhope$film \u0026lt;- \u0026quot;SW_Episode_IV\u0026quot; newhope$X__3 \u0026lt;- NULL empire \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeV\u0026quot;)%\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) empire$film \u0026lt;- \u0026quot;SW_Episode_V\u0026quot; jedi \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeVI\u0026quot;) %\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) jedi$film \u0026lt;- \u0026quot;SW_Episode_VI\u0026quot; all_sw \u0026lt;- rbind(newhope, empire) all_sw \u0026lt;- rbind(all_sw, jedi) #create new requence to paste together all 3 films all_sw$pos \u0026lt;- 1:nrow(all_sw) #create new variable that counts the number of words in each line all_sw$nwords \u0026lt;- sapply(strsplit(all_sw$dialogue, \u0026quot; \u0026quot;), length) #get cumulative words spoken at each line for all characters talk_vol \u0026lt;- all_sw %\u0026gt;% select(character, nwords, pos, line_number) test \u0026lt;- talk_vol %\u0026gt;% spread(character, nwords) %\u0026gt;% replace(is.na(.), 0) %\u0026gt;% gather(\u0026quot;who\u0026quot;, \u0026quot;num_chars\u0026quot;, -pos, -line_number) test$csum \u0026lt;- ave(test$num_chars, test$who, FUN=cumsum) #get the top 10 speakers to highlight them in the chart top10 \u0026lt;- test %\u0026gt;% group_by(who) %\u0026gt;% summarise(talks = max(csum)) %\u0026gt;% arrange(desc(talks)) %\u0026gt;% head(n = 10) %\u0026gt;% pull(who) #make points pts_test \u0026lt;- test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)) #plot ggplot() + geom_vline(xintercept = 1, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1011, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1850, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_text(aes(x=1, label=\u0026quot;New Hope\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1011, label=\u0026quot;Empire Strikes Back\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1850, label=\u0026quot;Return of the Jedi\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_line(data = test, aes(x = test$pos, y = test$csum, group = test$who), alpha = .4) + geom_line(data = test %\u0026gt;% filter(who %in% top10), aes(x = pos, y = csum, colour = who)) + geom_point(data = test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, colour = who)) + geom_text(data = test %\u0026gt;% filter(who %in% top10 \u0026amp; who != \u0026quot;BEN\u0026quot; ) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust=0.5, size = 4.5) + geom_text(data = test %\u0026gt;% filter(who == \u0026quot;BEN\u0026quot;) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust= 0, size = 4.5) + theme_minimal() + theme(legend.position=\u0026quot;none\u0026quot;, text = element_text(size = 16), axis.text.x=element_blank(), axis.ticks.x=element_blank(), plot.margin = unit(c(1,0.5,0,0.5), \u0026quot;lines\u0026quot;)) + labs(title=\u0026quot;Cumulative number of words spoken \\n by characters in original Star Wars trilogy\u0026quot;, x =\u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;) + xlim(c(0,3000)) + ylim(c(0,4700)) + scale_colour_brewer(palette = \u0026quot;Paired\u0026quot;) We also got to work with data from the Manchester Museum, Withworth Gallery, and Manchester Art Gallery. Together with another classmate, we used these data (which I am more sure I probably shouldn’t share, but I am sure that interested people could get in touch with the Manchester Museum Group to ask) to visualise gender representation in the Manchester Museum’s Egypt collection, and the Manchester Art Gallery. Our final product looked like this:\nMy colleague Vibhuti made the sanky diagram using Flourish mentioned above, and represents the types of artifacts that gods vs goddessess were represented with. I made the floor plan of the Manchester Art Gallery, with each room shaded by the proportion of paintings painted by male v female artists using the waffle package and the gridExtra package in R. We assembled everything in MS Publisher. Overall it was good fun and we got to present the results to members of the Manchester Museums Group, so very useful.\n A note on accessibility There was a lot of talk about accessibility of charts and this was I think a really important thing to always keep in mind. We discussed accessibility as in is the chart usable, is it suitably understandable, and therefore accessible to the audience but also discussed accessibility in terms of considering colourblind users for example. Some resources for this:\n colororacle.org gives a way to check if your colour scheme is colourblind colorbrewer has colourbling friendly pallette suggestions and in our final project we used coolors.co which generates a colour palette for you, and allows you to check if its colourblind friendly with different types of colourblindess simulated.  One thing we didn’t talk about (and I appreciate may be out of scope for the course for now) is accessibility of visualisations for those people with visual impairments who would use for example a screen reader to interpret our charts. It would be interesting to learn more about this, and if anyone knows some best practice on making charts even more accessible, I would welcome any tips and links to resources.\n "
    }
]
