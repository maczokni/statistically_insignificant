[
    {
        "ref": "https://rekadata.site/blog/hot-routes-tutorial/",
        "title": "Hot Routes Tutorial",
        "section": "blog",
        "date" : "2020.03.29",
        "body": " Crime concentrates in place and time, and it is the task of the crime analyst to identify where and when these hotspots emerge. This is often achieved by producing density maps. Kernel density estimation is one methdod for producing such maps, which involves applying a function (known as a “kernel”) to each data point, which averages the location of that point with respect to the location of other data points. The surface that results from this model allows us to produce isarithmic maps, also referred to in common parlor as heatmaps (see our crime mapping textbook on GitHub for more on Kernel Density Mapping and a “how-to-in-R” tutorial).\nHowever, often we might be interested in how crime concentrates along a particular network such as along streets, bus routes, train nextworks, and others. In order to map hot spots along a network, we can use a technique called hot routes. It was used as early as 2003 by Andrew Newton to map crime and disorder on the bus network in Merseyside. A useful how-to guide was later produced by Henry Partridge and Lisa Tompson and can be accessed in the associated JDI brief or journal article.\nHot Routes was devised to be a straightforward spatial technique that analyses crime patterns that are associated with a linear network (e.g. streets and other transportation networks). It allows an analyst to map crime concentrations along different segments of the network and visualise this through colour.\nHere I will go through an example of how to apply hot routes in R. We will follow the 4 steps outlined by Henry Partridge and Lisa Tompson in the JDI brief:\nStep 1: Prepare the network layer Step 2. Link crime events to street segments Step 3: Calculate a rate Step 4: Visualise the results\nI will use the example dataset of the London Underground network, spatial data for which can be downloaded using the Transport for London API, and crime from British Transport Police for February 2020 available via data.police.uk.\nStep 1: Prepare the network layer The first step is to prepare our network layer. There are several substeps to this. First we need to acquire a shapefile for our network. Here we will keep it simple for the sake of the tutorial, and use only one line. Let’s go with the Bakerloo line. We can get this data using the TfL API. We can use R to make the API call for us by taking the query URL, and getting the results using the readLines() function in base R, and putting that within the fromJSON() function from therjson package.\nlibrary(rjson) #get json from TfL API api_call \u0026lt;- fromJSON(readLines(\u0026quot;https://api.tfl.gov.uk/line/bakerloo/route/sequence/outbound\u0026quot;)) ## Warning in readLines(\u0026quot;https://api.tfl.gov.uk/line/bakerloo/route/sequence/ ## outbound\u0026quot;): incomplete final line found on \u0026#39;https://api.tfl.gov.uk/line/ ## bakerloo/route/sequence/outbound\u0026#39; When you run the above, you might get a warning messae about incomplete final line, I’m not sure why, but we still get the object api_call in our environment, you can see it’s a large list with 10 elements.\nYou can have a look at this list, it has many interesting bits, but what I want to do here is extract the stops along the bakerloo line and their respective coordinates. There are probably much more efficient ways to do this, but here is mine (this is a tutorial on hot routes, not on parsing json files haha!)\n#parse df of stops and latlongs datalist = list() for (i in 1:length(api_call$stations)) { datalist[[i]] \u0026lt;- data.frame(stn_name = api_call$stations[[i]]$name, stn_lat = api_call$stations[[i]]$lat, stn_lon = api_call$stations[[i]]$lon, line = \u0026quot;bakerloo\u0026quot;) } bakerloo_stops \u0026lt;- do.call(rbind, datalist) Now you can see we have an object called bakerloo_stops which has 25 observations of 4 variables (station name, latitude, longitude, and line).\nIn order to carry out our spatial operations, we will be making use of the sf package. So I will load the sf package and also convert this list of stops to an sf object using st_as_sf() and build the line between the stops using group_by() (from dplyr package), st_union() and st_cast() (from sf):\nlibrary(dplyr) ## Warning: package \u0026#39;dplyr\u0026#39; was built under R version 3.5.2 library(sf) bakerloo_stops \u0026lt;- st_as_sf(bakerloo_stops, coords = c(\u0026quot;stn_lon\u0026quot;, \u0026quot;stn_lat\u0026quot;), crs = 4326) bakerloo_line \u0026lt;- bakerloo_stops %\u0026gt;% group_by(line) %\u0026gt;% st_union() %\u0026gt;% st_cast(\u0026quot;LINESTRING\u0026quot;) Now we have a line and the set of stations along it. We can use the ggplot2 package to plot it like so:\nlibrary(ggplot2) ggplot()+ geom_sf(data = bakerloo_stops) + geom_sf(data = bakerloo_line) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) NOTE: I have added theme_void() AND theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) because apprently theme_void() + geom_sf() have some issues together.\nRight, so that’s looking good. But here we have only one line. What we want is to break our line into sections. How you do this depends really much on what you want to show. In this case, it might be meaningful to consider each segment of the line between stops. In this case, we can use the shapefile of the stops (bakerloo_stops) to break up our line (bakerloo_line). However, as Henry and Lisa note, network layers typically contain streets of unequal length. This means that longer segments might show up as hot simply because they have more space to contain more crimes. Therefore in such cases it is advisable in this analysis to use equal length street segments, where possible. In this case however, let’s stick to the stops. To split our linestring (bakerloo_line) into many linestrings using the stops (bakerloo_stops) we can use the st_split() function from the lwgeom package and st_collection_extract() function from sf. Furter, as the st_split() function is expecting a blade argument of length 1, we can use the st_combine() (from sf) function to group our tube stations alltogether:\nlibrary(lwgeom) parts \u0026lt;- st_split(bakerloo_line, st_combine(bakerloo_stops$geometry)) %\u0026gt;% st_collection_extract(\u0026quot;LINESTRING\u0026quot;) parts ## Geometry set for 24 features ## geometry type: LINESTRING ## dimension: XY ## bbox: xmin: -0.334896 ymin: 51.4945 xmax: -0.099185 ymax: 51.59222 ## epsg (SRID): 4326 ## proj4string: +proj=longlat +datum=WGS84 +no_defs ## First 5 geometries: You can see we now have a new object called parts which is a linestring containing 24 features, the segments between our 25 tube stations, all as unique lines. You can also see that this is a geometry set of 24 features, let’s turn it into a simple features collection, and label each of the segments with a number by taking each element of parts and binding it together as a dataframe.\ndatalist = list() for (i in 1:length(parts)) { datalist[[i]] \u0026lt;- st_as_sf(data.frame(section = i), geometry = st_geometry(parts[i])) } bakerloo_sections \u0026lt;- do.call(rbind, datalist)  Step 2: Link crime events to street segments In this step, each crime event needs to be linked to the nearest street segment and the attribute table of the network layer updated with the corresponding count of crime.\nTo achieve this, first let’s get some crime data. I have downloaded all the BTP crime data from the police.uk website, and saved it in my local data folder. From here I can use read.csv() to import it, and similar to the stations, use st_as_sf() to turn it into a sf object:\ncrimes \u0026lt;- read.csv(\u0026quot;data/2020-02-btp-street.csv\u0026quot;) crimes_sf \u0026lt;- st_as_sf(crimes, coords = c(\u0026quot;Longitude\u0026quot;, \u0026quot;Latitude\u0026quot;), crs = 4326) Of course this download includes BTP data for the whole country. We don’t want this. Instead we can think about some threshold within which we care about our crimes. What I mean is, we want to first select all the crimes that we want to arrtibute to our specific network. This will vary with what you are plotting. In the case for example of a street network for the London Borough of Camden, you would want to include all crimes that are within the boundary of Camden. In the case of a bus route or a train route however, you might want to set some buffer, within which you are interested in counting crimes, but outside of which you believe they are too far to be attributable to your network object of interest.\nHow you choose the size of this buffer will depend on things like how accurate you think the geocoding of your data is, or other considerations.\nHere I just went with a coarse buffer of 0.005 arc degrees. The dist argument is assumed to be in decimal degrees (arc_degrees). This buffer distance is a units object, it should be convertible to arc_degree if x has geographic coordinates, and to st_crs(x)$units otherwise.\nSo I build a buffer of 0.005 decimal degrees around the line using st_buffer(), and keep only the crimes that fall within this buffer using st_intersection():\nbakerloo_line_buff \u0026lt;- st_buffer(bakerloo_line, 0.005) ## Warning in st_buffer.sfc(bakerloo_line, 0.005): st_buffer does not ## correctly buffer longitude/latitude data crimes_sf \u0026lt;- st_intersection(bakerloo_line_buff, crimes_sf) I can plot it all to see if it looks good:\nggplot()+ geom_sf(data = bakerloo_line_buff) + geom_sf(data = bakerloo_line) + geom_sf(data = bakerloo_stops) + geom_sf(data = crimes_sf, col = \u0026quot;blue\u0026quot;) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) Great, now what we want to do is snap each one of these crime points to the nearest line section (remember we have the 24 sections in the parts object).\nTo do this, we can use the st_nearest_feature() function. This will return, for each point, the ID of the nearest segment.\nbline_segments \u0026lt;- st_nearest_feature(crimes_sf, bakerloo_sections) bline_segments ## [1] 21 13 13 20 20 20 20 20 20 13 13 23 23 23 23 23 19 19 19 19 19 15 15 ## [24] 15 15 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [47] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [70] 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 6 ## [93] 6 6 6 6 6 6 6 1 1 1 1 1 1 1 1 1 1 1 1 1 1 7 2 ## [116] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 10 10 10 10 10 10 10 10 ## [139] 10 10 10 10 10 10 10 10 10 4 4 4 4 16 8 8 8 8 8 8 8 9 9 ## [162] 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 ## [185] 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 ## [208] 9 9 9 9 9 9 9 9 9 9 9 9 18 18 18 18 18 18 18 18 18 18 18 ## [231] 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 18 ## [254] 18 18 18 18 18 18 18 18 18 18 18 4 4 4 4 4 4 4 4 4 4 4 4 ## [277] 4 4 4 4 4 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 12 ## [300] 12 12 12 3 3 3 3 3 3 3 3 3 3 9 9 3 9 3 3 3 9 9 9 ## [323] 3 3 3 3 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 9 3 9 3 ## [346] 3 22 22 22 22 22 22 22 22 22 22 It is simply a list of the ID numbers of matched line segments for each of the 356 crime points in the crimes_sf object. We can use this to create a frequency table and save this in a dataframe to be joined to the linesegment object. We will also need to replace our missing values with 0s, since in this case the segments which do not appear in our frequency table had 0 crimes snapped to them. We use the replace_na() function from the tidyr package.\nlibrary(tidyr) #make list of nearest into df of frequency sections_freq \u0026lt;- as.data.frame(table(bline_segments)) #make sure id is numeric sections_freq$bline_segments \u0026lt;- as.numeric(as.character(sections_freq$bline_segments)) #join to sections object and replace NAs with 0s bakerloo_sections \u0026lt;- left_join(bakerloo_sections, sections_freq, by = c(\u0026quot;section\u0026quot; = \u0026quot;bline_segments\u0026quot;)) %\u0026gt;% mutate(Freq = replace_na(Freq, 0))  Now we have an sf object with each section labelled with the number of crimes that were snapped to it as they were the nearest segment. So essentially, the number of crimes on (and around, depending on your buffer decisions) each segment. We could map this count line:\nmidpoint_crimes \u0026lt;- mean(bakerloo_sections$Freq) ggplot() + geom_sf(data = bakerloo_sections, aes(colour = Freq), lwd = 2) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Number of crimes\u0026quot;, midpoint = midpoint_crimes, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) But of course we want to account for things like the length of each segment as they are unequal.\n Step 3: Calculate a rate Next, to calculate a rate we need a denominator. In this case, length may be a good one, so the length of each street segment needs to be calculated. We can do this using the st_length() function. Since our data are all in WGS84 projection, this will return the length of each segment in meters.\nbakerloo_sections$length \u0026lt;- st_length(bakerloo_sections) Once we have all the lengths, a new column needs to be created in the network layer to record a crime per metre score. This is calculated by dividing the number of crimes linked to a street segment by its length.\nbakerloo_sections$crime_per_m \u0026lt;- bakerloo_sections$Freq / bakerloo_sections$length We now have our crimes per meter score! On to mapping!\n Step 4: Visualise the results The final step is to thematically shade each street segment with a colour (and line thickness if desired) that corresponds to the range of the rate of crime per metre.\nFor this, let’s convert our crimes per meter (crime_per_m) variable to numeric from a “units” object, and then use ggplot once again.\nbakerloo_sections$crime_per_m \u0026lt;- as.numeric(bakerloo_sections$crime_per_m) midpoint_rates \u0026lt;- mean(bakerloo_sections$crime_per_m) ggplot() + geom_sf(data = bakerloo_sections, aes(colour = crime_per_m), lwd = 2) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Rate of crimes per meter\u0026quot;, midpoint = midpoint_rates, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) And if we wanted to add thickness as well we can by specifying the size argument:\nggplot() + geom_sf(data = bakerloo_sections, aes(colour = crime_per_m, size = crime_per_m), show.legend = \u0026quot;line\u0026quot;) + geom_sf(data = bakerloo_stops) + theme_void() + theme(panel.grid.major = element_line(colour = \u0026quot;white\u0026quot;)) + #theme void is buggy with geom_sf() so need this too scale_colour_gradient2(name = \u0026quot;Rate of crimes per meter (colour)\u0026quot;, midpoint = midpoint_rates, low = \u0026quot;#ffffcc\u0026quot;, mid = \u0026quot;#fd8d3c\u0026quot;, high = \u0026quot;#800026\u0026quot;) + scale_size_continuous(name = \u0026quot;Rate of crimes per meter (width)\u0026quot;)  All done! We have now managed to create a hot routes map of crimes on or near the barkerloo line recorded by British Transport Police in February 2020.\n Final remarks This has been a very simple application of the hot routes technique in R, and I hope this tutorial is helpful and can be applied to many other sets of data! If anyone is following along and gets stuck just get in touch with me on twitter ([@r_solymosi](https://twitter.com/r_solymosi)) or by email reka.solymosi@manchester.ac.uk. Also many of the solutions here are possibly hacky, if someone has better ways of doing this please just let me know, I am happy to improve it. As is always the case there are more sophisticated ways to do this, for example using the spatstat package (for a quick intro see section 6.7 in our crime mapping textbook), and probably many more, but this also does the job…!\nFurther, while hot routes is a really neat way to visualise crime rates along a network for sure, it may be the case that the geographic location of each segment or stop is not so important, and instead it is the ditribution of crime along a route that is the focus. In this case another visualisation technique that may be useful is street profile analysis introduced by Valerie Spicer and colleagues, detailed in this paper. I can always write another tutorial on this one in R, if there’s interest.\nAnyway, happy mapping friends!\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/exploring-public-engagement-with-missing-person-appeals-on-twitter/",
        "title": "Exploring public engagement with missing person appeals on Twitter",
        "section": "blog",
        "date" : "2020.01.29",
        "body": " About this time last year I received funding from the Manchester Statistical Society Campion Grant to carry out some research looking into appeals for informaion made about missing persons on Twitter.\nThe motivation behind this is that police agencies globally are seeing an increase in reports of people going missing. These people are often vulnerable, and their safe and early return can be a key factor in preventing them from coming to serious harm. One approach to quickly find missing people is to disseminate appeals for information using social media. In fact, police, and other agencies, make frequent use of social media (such as Twitter) to send out appeals for information.\nThe goal of this project was to better understand how police accounts tweet appeals for information about missing persons, and how the public engage with these tweets by sharing them.\nTo achieve this goal we analysed 1,008 Tweets made by Greater Manchester Police between the period of 2011 and 2018 in order to investigate what features of the tweet, the twitter account, and the missing person are associated with levels of retweeting.\nIn particular we wanted to look at different features associated with the tweet, the twitter accounts, and the missing person, and any associations with engagement by the public, measured as retweets.\nRelated materials:\n The paper is available as a pre-print on OSF here: https://osf.io/preprints/socarxiv/wugxs\n All of the materials (including data, data dictionary, the paper markdown) are on OSF here: https://osf.io/4w5eg/. The data was anonymised using NETANOS.\n The R package which contains all the code for analysis is available on GitHub here: https://github.com/maczokni/misperTweetsCode\n  Here I will highlight some of the most interesting findings.\nFeatures of interest First we identified features associated with public engagement that might influence engagement from a litearture review. We identified the following:\n Table 1: Table 1: features in the literature  Element Feature    Features of the missing person Race/ ethnic appearance, Gender, Age  Features of the tweet Time and timeliness, Post length, Punctuation and hashtags, Templates, Sentiment, Tone, Useful information, Photo (presence and valence)  Features of the account Number of followers, Age of account, Tweeting activity, Trusted source    We then went through our sample of 1,008 Tweets that were appeals for information about missing persons taken from the 56 GMP Twitter accounts identified for this study, and coded for all of these elements (except age, we abandoned it as this was too messy really…!). Please see the preprint for details on conceptualisation and operationalisation of these variables. Also for the full set of results, as below I will highlight only two of the most interesting ones, the paper itself contains many more!\n Two highlighted results First we looked at the importance of photos, in particular if people engage differently with a custody image versus a regular everyday photo (Figure 1). We found that custory photos are retweeted less than regular photos. Using multiple photos does not seem to matter. The point estimate is the median, and the arms represent the interquartile range.\n Figure 1: Figure 1: Retweets for different image types by gender and ethnic appearance  We also looked at the types of phrasing used in the Tweets, by coding these into different types of templates.\nHere are he list of templates identified through qualitative coding, with an example of each:\n Table 2: Table 2: types of templates present in Tweets by GMP about missing persons  Template No. of tweets Example    call 101 449 “*** [OTHER_841] [OTHER_7] [OTHER_204] *** Joshua [OTHER_57] 13 years old is missing from the [OTHER_224] area, if you have any information please call 101. https://t.co/h9S4kMbcmn”  original phrasing 255 “Please take a look at this. [OTHER_519] [OTHER_353] [OTHER_217] Lingard: http://t.co/oMdYHMZr”  … are concerned for.. 115 “Pt1 - [OTHER_23] are becoming increasingly concerned for the welfare of a 46-year-old man from [OTHER_264] [OTHER_1743] thought to be missing in Wigan”  #missing 83 “… grey cardigan, peach top, blue jeans peach pumps. [OTHER_121] sightings pls call [OTHER_86] on 101… pls RT #missing”  please RT 77 “PLEASE [COMPANY_29] [OTHER_668] [OTHER_669] [OTHER_670] [OTHER_671] [OTHER_672] [OTHER_668] [OTHER_674] [OTHER_675] [OTHER_26] [OTHER_677] 21/5/13 [HE/SHE] [OTHER_678] 14 http://t.co/EUmrRL4wEd”  can you help 72 “Can you help us find this woman, [OTHER_446] Bleu, who has gone missing from the MRI? http://t.co/6ymS0h57Td”  have you seen.. 72 “Have you seen 47-year-old [OTHER_1163] from Derbyshire? Hayley Poynton, 47, was last seen at around 10.55pm on [OTHER_330] 10 [OTHER_1167] 2018 at [OTHER_83] [OTHER_792] Hospital. https://t.co/wjSmnldtN4 https://t.co/q4H0m116aD”  high risk 58 “*High [OTHER_7] Missing*: [OTHER_91] male, 1[NUMBER_374]s, [OTHER_1223] build, approx 6ft, [OTHER_10] short hair with quiff, wearing a black tracksuit 1/2”  **missing** 56 “*MISSING* [OTHER_2101] [OTHER_2104] 13yrs, Pendleton. Desc - white, 5’4\u0026quot;, slim, blue eyes, shaven hair, possible shaven eyebrows. http://t.co/H9KuNoT1A6”  link to info 40 “MISSING: [OTHER_1983] [OTHER_1984] Moore, 40. [HE/SHE] has links to the city center, wythenshawe and Stockport/Bramhall areas of Manchester. Please share. https://t.co/NEulhR3cYu”  thanks 34 “Help us find [OTHER_1025] [OTHER_833] [OTHER_519] [OTHER_989] [OTHER_163] if seen dont approach ring 999 and quote [COMPANY_41] 379 070314. please [OTHER_210] Thanks”  … are appealing for.. 26 “Police are appealing for information to trace 2 sisters who are missing from home. Lana \u0026amp; [OTHER_428] [PERSON_103] were last seen in [OTHER_429] [OTHER_89] 22/07”  urgent appeal 12 “MISSING [OTHER_22] – [OTHER_492] O’Leary 42 years. [OTHER_1540] for [OTHER_492] who requires urgent medical attention. [OTHER_121] info phone 101, log 1347 26/08/17 https://t.co/jcy7I25Gtt”    We compared the number of retweets between each template (Figure 2). The point estimate is the median, and the arms represent the interquartile range.\n Figure 2: Figure 2: Retweets for different templates   Conclusions These are just two interesting insights gained from exploring these tweets. In the full paper we explore a range of features associated with the appeals for information about missing persons made on Twitter by greater Manchester police.\nIn doing so, we uncover how the police currently construct such appeals, and whether we can infer any structure in the practice. We find that there is some structure, but there is also variation in how these messages are crafted, as well as in other features such as the type and quality of photo used, the phrasing and punctuation used, and the perceived sentiment that results.\nWe further considered how engagement, measured as retweets varies between these differently structured tweets, and draw conclusions about what we think might be important to follow up.\nIn sum, with this paper we provide an insight into how appeals for information for missing people are shared by a major UK police force, and how the public react to these messages. By doing so we serve as a reference point for an issue that is internationally relevant, affecting police and other organisations worldwide, and hope to spark future work in the area, preferably prospective or experimental studies to establish causal relationships between the features identified and engagement.\nRead the full paper here: https://osf.io/preprints/socarxiv/wugxs and do reach out if you have any thoughts/comments/feedback/questions/ideas for future research!\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/crime-analysis-for-schools/",
        "title": "Introducing data and crime analysis to Wirral Hospitals' School",
        "section": "blog",
        "date" : "2020.01.14",
        "body": " On Monday (13th January) I gave a talk introducing pupils of Wirral Hospitals’ School to the world of data analysis, and specifically crime analysis. It was my first time speaking to a younger audience, so I had a bit of a task trying to think about what is the appropriate amount of content to include, and level to pitch to. I didn’t want to be too simplistic and therefore condescending, but I also didn’t want to bore them with jargon and technical details. In the end I think it went well, there was lots of engagement from the pupils, and the head teacher gave me some positive feedback about the content as well as the delivery. So I thought I’d write this up for anyone else looking for resources, thinking about going into similar school settings to talk about data analysis or crime analysis or both!\nPreparation The first thing I had to do was collect possible materials to use. I split this into three groups:\n Content about data analysis Content about crime analysis specifically Activities to engage the students  Content about data analysis and associated activities Firstly, I spent some time going through content which I had curated over the years of teaching and giving talks about data analysis. Some of my fave from this collection are visualisation of spells of Harry Potter. I also have Dear Data. The complaints postcard is a specifically good one to explain because you can immediately start drawing conclusions from it that people can relate to!\nComplaints postcard from Dead Data by GIORGIA LUPI and STEFANIE POSAVEC\n When I started writing the presentation I was actually on Vancouver Island with friends, and asked for advice there, and was recommended to think about #tidytuesday. Of course, this is an excellent resource of fun data sets, and also creative visualisations. Also pudding.cool’s rap artists’ vocabulary was mentioned as something kids would be interested in.\nBut even with such advice I thought I might not be reaching enough examples that are “down with the kids” (is Harry Potter still a thing?! Will it be awkward or funny when I mispronounce all the rap artists names?!) so I reached out to Twitter. Turns out this was an excellent idea, because I was met with a shower of useful resources.\nYou can see the thread of all the suggestions here: https://twitter.com/r_solymosi/status/1214835916637921282\nSo armed with all this, I had a very good selection of things to choose from now regarding data analysis in general. But what about crime analysis?\n Content about crime analysis specifically For this I thought I could introduce an easy-to-grasp framework, and work through two examples using it, one all together, and one as an activity. Specifically, I picked the SARA model to think about crime problems by analysing available information, and then maybe tapping into the students’ creativity for coming up with design solutions to tackle some crime problems.\nFor exercises, I initially thought about getting some data live, from police.uk, but I didn’t know what sort of devides they would have access to, whether or not there would be any sort of good internet connectivity, and so on, so for the sake of safety I decided to go analogue.\nSpecifically I picked two examples. First bicycle theft. For this, I followed the guide POP Centre Guide for Bicycle Theft by I use the popcentre report by SShane D. Johnson, Aiden Sidebottom, and Adam Thorpe as a guide, as well as all the info on http://www.bikeoff.org/. The site is great, especially the gifs explaining how some bikes get stolen, and the lots of creative designd for the response phase.\nThen, for the exercise to do together, I used the example of handbag theft from pubs, modelled after POP Centre Guide for Theft of Customers’ Personal Property in Cafés and Bars by Shane D. Johnson, Kate J. Bowers, Lorraine Gamman, Loreen Mamerow, Anna Warne. Here, I used not only slides to work through this together, but printed out materials, in sealed envelopes labelled as “Scanning”, “Analysis”, and “Response”, handed out to individuals or small groups (“Assessment” was done together as a large group).\nThe idea would be that each “phase” will have some handout materials, and pupils will in groups (and with me helping along as well) will follow along. So first, for the “Scanning”\u0026quot; phase, I had 3 newspaper articles, all about handbag theft in pubs.\nMaterial for the “Scanning” phase\n Then, for the “Analysis phase”\u0026quot;, we had two conflicting bits of intel from two informants. One usually lies, and one usually tells the truth. So how can we know who is telling the truth or lying? Well we also have a map of the pub, with the number of bags stolen from each table, which we can use to colour in a heat map. Then, with this heatmap we can decide which informant tells the truth.\nPub floorplan for making heatmap of bag theft\n Once we know this, we can open the second analysis envelope, to get some more information from our truthful infomant, who tells us that he’s after bags thrown under chair or hung on the backrest. Now, we get to the response phase, and the envelope here contains two types of chairs and also a table, on which pupils can draw their creative anti-bag-theft designs. I did not manage to come up with a small-group based activity for the “Assessment” phase, so instead we together as a large group discussed the results of the actual evaluation study.\n  The presentation In the end, the presentation went as follows:\nFirst, I spent a few slides introducing myself. When discussing earlier drafts with people, a few people did mention that one of the things the students will be interested in before any of the content is my accent! So I figured I would include a little map to give background on where my accent comes from. Then I talked a bit about my academic and work-life journey to where I am now. Turns out this is really good, because the assembly topic before my talk was about degrees, and education, and career paths, so this fit in very nicely.\nThen I introduced data analysis with a one-liner (“Making sense of relevant data to identify patterns and answer questions”), followed by looking together at a particular data set: the #tidytuesday data set of Seattle’s registered pet names. I showed a screengrab of the data, followed by a chart showing that besides dogs and cats, the good peope of seattle also have pigs and goats registered. Then, I did a little bit of live coding, so that we could play the game “does your pet have a unique name?”. This was really great, because it immediately got many of the kids talking (and some of the staff!!). It was also interesting because it looks like there were pretty much no uniquely named pets, which we didn’t find at least one in the Seattle data base. Only at the end of the session (one girl came up after to ask about her dog, ‘lil marco’) did we find a unique pet name! This was excellend warm-up! Thank you #tidytuesday!\nI then showed some more examples: another #tidytuesday entry on squirrel survey in Central Park, the Harry Potter spells (still relevant!), the rappers vocabulary (they asked about Eminem, perfectly pronouncable!), property prices in the UK (maybe more enjoyed by the staff), and of course Dear Data.\nAfter this came the Skittles exercise, which was one of the recommendations I received from Twitter (thank you @duncanbradley_). This was also really great in getting pupils involved and interested. I got 12 packs of Skittles, handed them out, and asked the pupils if they think that any two between them will be the same. They were pretty convinced that they won’t be. One asked that surely this depends on how many are in each pack (correct!). So we started by counting all the skittles, which varied between 41-45 pieces. Next I asked someone to name their favourite colour skittle (was blue) and got them all to count their blue skittles. There were about 4 with the same number, so we moved onto the next colour, of which we only had a 2-way match, and the third favourite colour had already established that no two packets were the same (phew!). Then skittles were eaten, and we discussed how many packs are needed.\n468 packs of Skittles from possiblywrong.wordpress.com\n Then back to presentation mode, I showed some examples of crime analysis (the Trafford Data Lab’s dashboard for police.uk data was excellent), situational crime prevention examples (ticket gates, shattering pint glass), and then worked through the cycle theft example mentioned above using the SARA model. I asked, just before, how many of them had had a bicycle stolen, and half the room’s hands went up, while the other half seemed to know someone who’s bike was stolen. One girl mentioned she knew about bike marking (yay)! Then I asked who parks their bike on “informal” parking furniture. Again, almost all hands went up (including mine to be fair, we need more cycle infrastructure!!). So when I showed the gif for “lifting”, it got some gasps regarding how easy it is to steal such bikes.\nHow bikes are “lifted”\n Finally it was time for the example of handbag theft from pubs. We opened the Scanning envelope, and had three articles. I emphasised “think about what you think the crime problem is as specifically as possible”. Well, not only did they come up with “theft of handbags from pubs” but a lot of detail, such as that thieves go for flashy expensive looking bags (tapping into some CRAVED characteristics there…!) and that its because the owners are being careless, distracted, or otherwise inadequate guardians (I swear I did not show a crime triangle!!). Having established this specific problem, we reached out to our two informants, receiving some conflicting intel. One says he works near the doors, while the other likes to target the busy dance floor. It was time to colour in the heatmap to establish who is lying and who is telling the truth. This was also pretty easily tackled by the students, and we quickly moved on to trust out informant “B” and get some more information about how he steals handbags, and use this to open our response folder, which had the three pictures of furniture: two chairs and a table in it, which students drew their designs on. There were some really good ideas (one student actually drew the grippa clip, without having seen them before!) and I was impressed that everyone came up with at least one solution!\nWe wrapped up by talking about the real study, the outcome of the evaluation, and then some general questions about crime analysis.\n Thoughts Overall I really enjoyed this experience, and would gladly do it again. The students were really engaging and welcoming and I had a lot of fun listening to their ideas and designs. For anyone thinking about going into your local school, mainstream or non-mainsteam, I strongly encourage this. Especially if you’re early careers! The headteacher said a really nice thing to me at the end, she said that because students perceive me as closer to them in age, they might start to see this sort of career and academic trajectory as open and accessible to them, if it’s not something they would have thought about before. Of course many of these students are exceptionally smart, and are going to go on to great things without any input from me, but if I could give a bit of inspiration to one person, I’m pretty happy about it.\nAnd if you reading this are interested to go give a similar talk in a school, please feel free to use my materials, and just let me know if you want to talk about my experience at all!\nThe slides are available here: https://www.dropbox.com/s/gap5ibu3sc3lsjy/wirralschool.pptx?dl=0\nAnd the handouts are available here: https://www.dropbox.com/s/scb2ccjqtzrxy4m/handouts.pptx?dl=0\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/asc-2019/",
        "title": "ASC 2019",
        "section": "blog",
        "date" : "2019.11.18",
        "body": " Last week I was at the 2019 annual meeting of the American Society of Criminology (ASC). It was a great opportunity to see friends and colleagues, learn about new developments in the field, and present my paper on environmental features associated with sexual harassment.\nThis was my 3rd ASC (2nd in sunny San Francisco) but my first without a “crime and place“ stream. This meant that instead of sitting in one room and waiting for the presentations to come to me, I had to curate my own experience. With 1,301 sessions (according to the programme) there were definitely a lot of options (4.110534247 times 10 to the power of 3488 possible alternate conference experiences to have, to be exact…!), but here I will summarise my version of this conference and highlight some interesting (to me) papers, breaking things down into themes.\nFear of Crime There were quite a few presentations about fear of crime, with many emphasising the importance of the situational context in which fear is experienced. It is so exciting to now see fear of crime research back on the map (har har); I am noticing a bit of a boom in this area with more and more scholars getting excited about perception and safety in the environment.\nI particularly liked Valerie Spicer’s (Simon Fraser University, Canada) presentation about using street intercept surveys to measure changing perceptions of safety in specific locations. Street intercept surveys use a location-based sampling approach (stopping people to ask questions on specific street intersections). This allows for the mapping of responses, and through a longitudinal design, we can see maps of perception changing over time. For example, they noticed a shift in the location of fear hotspots with changes in the environment (for example the opening of a new transit hub). This method also allows for the sampling of people possibly not captured by other traditional sampling frames, such as homeless people who may have very different fear patterns to other users of public spaces. I particularly liked that the research team asked not only about fear of crime, but also about community strengths, focusing on the positive experiences people have with public spaces as well. The paper is available here. The below image is one of the maps from the paper showing location-specific measures of fear of crime:\nmap of perceived safetymeasured with street intercept survey from Song \u0026amp; Spicer\n Since building a mobile application to measure fear of crime (see paper here) my thing is really apps for fear of crime research (even have a review of all these for which you can read the preprint here). So of course I was very excited to see Michael Chataway’s paper (Queensland University of Technology, Australia), presented in the same session, which used a mobile app, and similarly to Valerie Spicer’s paper emphasised this shift in focus to perception of safety instead of unsafety. He outlined that absence of fear is not necessarily the same as feeling safe. He suggests to ask about safe places, in order to explore their characteristics, and see how they differ from places perceived as unsafe. For example, he found that a feeling of “at homeness” was strongly associated with feeling safe. This is mirrored in my PhD research finding that “familiarity with an area” is associated with reduced odds of feeling worried about crime. I look forward to seeing more work develop in this area. This paper is available here\nPs: if anyone is interested in apps for social science research, Michael and I have a webinar available hosted by the UK Data Service here\nIn another fear of crime presentation, James Hurst (Universiry of Arkansas Little Rock,USA) carried out location-based retrospective surveys to ask about people’s perception of safety in specific university campuses. He originally asked about feeling unsafe at different times of the day, but actually found that there was almost no variation in perception between the different times. This is contrary to what I found in my mobile-app based study in London (see above-mentioned paper) , where time of day actually did matter. I would be really interested in finding out more about whether this is an artefact of the measurement tool used, or something else may be going on there. I cannot currently find a link but have asked James and will update here.\n Transport crime Many of the transport presentations could technically fit under the fear of crime label as well, but I’m separating them out anyways, because transport research also merits its own section due to the volume of emerging work.\nIt was great to see the presentations about the international study on college students’ experiences of fear and victimisation on public transport across the world, coming from the book of Vania Ceccato (KTH, Sweden) and Anastasia Loukatou-Sideris (UCLA, USA). The book is called “Transit Crime and Sexual Violence in Cities: Internatoinal Evidences and Prevention” and will be published by Routlegde next year, keep an eye out for it my transport nerdy friends! Here, four of the chapters were presented, Stockholm, Bogotá, San José, and Lagos. It was interesting to see similarities (and differences) in students experiences across these vastly different contexts. Andrew Newton (Nottingham Trent University, UK) and I have a chapter in said book about London and it was nice to see how other researchers approached the same issues worldwide.\nIn another session Elenice De Sousa (Montclair State University, USA) also explored fear of crime on different types of transport in Belo Horizonte, which, opposite to the presentations mentioned above, actually found that lower fear was not associated with frequent ridership. The case of Bogotá is further interesting due to having two very different bus routes one with many security features and another without, creating very different environments for passenger safety. I cannot find a link to this paper, but Elenice has written about bus crimes in Brazil before, so if you want an idea, that paper is available here\nI also liked the presentation by young scholar William Johnson from George Mason University who is working with Barak Ariel (Cambridge, UK) looking into the crime preventative impacts of platform edge doors. They had data from London Underground and considered the role that PEDs can play in preventing many of the crimes (and non crimes) which occurred. He also coined my favourite americanism of the conference by calling the BTP “British Transit Police”, which I liked because if only the letters weren’t capital it would be absolutely accurate. Anyway it is always so good to see more young scholars working in the area of transport crime! I believe this paper is still forthcoming, so I’ll keep an eye out for it.\n Methodological advancements Papers that emphasise interesting methods are often my fave at conferences because they offer some new way to approach a topic, and have the potential to further understanding in so many areas of criminology and wider social sciences. There were a few neat things like this presented at ASC.\nGoing back to perception research, I’ve been working on putting together a project with Emily Moir (Griffith University, Australia) to use virtual reality (VR) scenarios to explore guardianship in public places, and how different interventions are perceived by the target being protected. So I was very happy to see Jean-Louis van Gelder (University Twente) present a paper about using VR to study offender decision making by comparing people’s responses to text based vignettes and VR scenarios of a bar fight capturing various related outcomes. This paper is already published here. The below image is from this paper showing the filming process of the “bar fight” scenario:\nman wearing 360 degree camera to film bar fight vr scenario from van Gelder et al.\n There was also a paper introducing the importance of considering local level relationships between outcomes and their covariates, rather than focusing on the global, by using geographically weighted regressions in crime research by Martin Andresen (Griffith University, Australia), and a paper using causal mediation analysis by Krisztian Poch (LSE, UK) to study normative and non normative approaches to duty to obey (paper currently under review for Journal of Experimental Criminology). Both are methods I now want to learn more about.\n Academic integrity Finally, the session I was most excited about actually was not really any paper, but the ASC Forum on Academic Integrity. I’ve been really interested in this topic since Kim Rossmo advised to read “Rigor Mortis: How Sloppy Science Creates Worthless Cures, Crushes Hope, and Wastes Billions” by Richard F Harris. I super strongly recommend this book to anyone interested in how to improve science. The forum was set up in response to a recent (very well publicised) case of a retraction request of a paper by one of the co authors. If you are not familiar with the case, there is a medium article for you to read here and Justin Pickett’s (University of Albany, USA) piece on OSF available here\nThe session consisted of an opening by the panel (Sally Simpson and Laura Dugan (University of Maryland), and Daniel Nagin (Carnegie Mellon University)) where they outlined that they will not be discussing the case specifically but are looking for suggestions for improvement and to address questions and concerns. The main outcome so far seems to be that ASC journals are now subscribed to the Committee On Publication Ethics (COPE). Beyond that they are seeking suggestions, so if anyone has ideas now seems like a good time to reach out.\nThe discussions were across a range of issues, from the rights of whistleblowers, to citation cartels, to whether we trust internal investigations (many criminologists seem not to when thinking about the police so why don’t we apply the same standards here?). A few people mentioned to consider who is affected, not only the authors and editors and academic institutions, but also consumers of these outputs; practitioners who wish to implement research findings, and study participants or members of the communities studied who may benefit from the research should all be able to trust in them.\nThe session was recorded, so it will be possible to see all this in case you’ve missed it. I will update here when I have found access to it with a link. It was overall great that a discussion is starting to take place and personally I was really honoured to meet Justin Pickett and shake his hand for being so brave and driving change in criminology ahead.\n Final thoughts Overall it was a great ASC. I saw many more inspiring and engaging sessions (eg on guardianship with great papers by Emily Moir, Danielle Reynald, and Zharina Vakhitova (all Griffith University, Australia), and a paper on a randomised experiment introducing super intense flood lights to the streets of New York City to reduce crime by Aaron Chalfin from University of Pennsylvania, USA) and got to reconnect with colleagues from across the globe, which all makes me feel so so lucky to be part of an exciting and evolving field of research. I hope to return again soon.\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/halloween-mcr/",
        "title": "Halloween MCR",
        "section": "blog",
        "date" : "2019.10.30",
        "body": " On 29 October 2019 I gave a short presentation at PyData Manchester and Open Data Manchester joint meetup on the topic of Data Horror Stories. My talk was a data-driven exploration of the massive inflatable Halloween monsters of Manchester.\nHalloween is the best  Boys and girls of every age Wouldn’t you like to see something strange Come with us and you will see, This our town of Halloween - The Nightmare Before Christmas\n I friggin’ love Halloween! I get really into it. Last year, we organised a Halloween themed all day #rstats conference/meetup/thing, ( you can read all about that here), which involved some fantastic pumpkin carvings:\n…and me dressed up as the broom package\n(far right, with a broom and some functions taped to me…!)\nWhen I first moved to the UK 9 years ago, it was not really a thing at all, but more recently, it has become adopted as a fun and spooky holiday for all the age groups. In Manchester for example there are now all sorts of Halloween related activities. One of these is a set of inflatable monsters which are dotted around the city centre. You can download a map to hunt for them all here.\nNaturally, the monster walk was something very appealing to me, and we set out to walk the monster walk, take nice photos, and then pick our fave monsters over some beers. In our household, the hands down winner was “Blob”:\nblob\n But what about the rest of the city? Which was Manchester’s favourite monster? This was the question I set out to answer in this talk\nimage credit: [@OpenDataManchester](https://twitter.com/opendatamcr/status/1189253327550406657/photo/1)\n MCR Monsters One way to gauge what monsters people are photographing and sharing is to look at Instagram. I found two key hashtags that were relevant:\n #MCRMonsters and #HalloweenMCR  I wanted to select posts that used both hashtags, because a lot of what was coming up with just one or the other was actually not monster related content (at least not in the sense that I was after).\nTo acquire a set of photos with these hashtags and some of their metadata, I used the instaloader tool. Specifically to get only posts that had both hashtags, I modified this bit of code by aandergr. My version can be found on github here: https://github.com/maczokni/halloweenMCR. This was the only bit of Python I used however, and then I swiftly read my retreived JSON into R.\nAfter some cleaning I had a nice bit of data with some Monster photos and associated metadata. However, none of these told me which monster is in each photo. So this required some manual coding, where I looked at each photo, and coded what monster I saw.\nFinally, after all this was done, the results could be considered\n And the winner is…  I was working in the lab, late one night When my eyes beheld an eerie sight For my monster from his slab, began to rise -Bobby Pickett - Monster Mash\n So finally we can get to some results.\nFirst I considered number of posts:\nWell it seems like this round has been won by the dragon who lives atop the Printworks. Okay…\nprintworks dragon\n What about the most likes?\nYess, Blob back in the lead!\nBut this measure is still weighting the number of photos taken, as more photos mean more likes. What about likes per photo?\nWhat is this?! Well in this case it seems I get as winner something I tagged as “fake”. While it is definitely not a current monster (currently in its place are the “orange eyes”), after further investigation, I think maybe it is not fake but an image from last year. This is the image in question.\nIn either case, since I filtered for images in 2019 only, it should not be there, and is therefore DISQUALIFIED.\nSo intead the winner is….\n…BLOB! What a champ\nblob\n  All is well that ends well In conclusion, it has been a fun exercise to play a bit with the Instragram API and see what sorts of information I can get out of it. Number of likes, also replies, and the URL to the photos. I want to explore more.\nI also noted that there is this “may contain” feature, which has some sort of image recognition application to help describe posts for those with visual impairment. I used this to query some dog photos for example (everyone loves dog photos!). A simple string contains search and boom, I hav dog + halloween monster photos!\ndog_pics \u0026lt;- tagged_monsters[grepl(\u0026#39;dog\u0026#39;, tagged_monsters$may_contain),]  See the two results I got back here and here.\nThere is much more to explore though; originally I was hoping to get information about what filters people use, based on this paper about instagram filter choice being able to diagnose depression, but I did not get this info with instaloader. I guess I will keep exploring what is out there.\nFor anyone interested, all my code for this is on my github page.\nHappy halloween!!\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/webscraping-and-some-sampling-mapping-in-r/",
        "title": "Webscraping and some sampling \u0026 mapping in R",
        "section": "blog",
        "date" : "2019.08.10",
        "body": " Presenting at R Sheffield Last week I had the pleasure to present at the Sheffield R User Group alongside former PhD colleague (and roommate) Joanna Hill who is currently based in Uganda, working remotely at Rutgers University. It was a great event, set in a meeting room upstairs in The Red Deer pub, which gave it a nice informal feel. The attendees were all welcoming, knowedgeable, and very engaged and engaging, so it was a great experience and I recommend to anyone in the area. The one negative was that trains weren’t running due to floods so I had to drive and therefore not fully immerse the presenting-in-a-pub-with-a-pint aesthetic. Oh well, there is always next time, and at least we got to see some nice peak district views.\nPeak views (photo by Jo Hill)\n  !!!DISCLAIMER!!! I am no expert on scaping data from the web. In that I really only engage with this problem when I find an interesting source of data, then scarpe this, and then abandon it forever. So DISCLAIMER: there are probably much better, more effective/efficient ways of doing this.\nIn fact someone in the meetup mentioned rvest. I haven’t looked into it yet but an initial look is already super exciting, and I urge anyone interested in webscrping to check that out, as it’s probably loads more useful than my hack-together approach.\nThat said, I do use my hacky approach to get myself exciting data sets sometimes, such as data from FixMyStreet, and people have asked me before about how I do this, so I thought this could be a good chance to share that. So what follows is a cliffnote of the talk:\n Finding some data about experiences of sexual harassment Sexual harassment is a widespread global issue. 75% of all women worldwide (~ 2 billion) have experienced sexual harassment. 35% of all women worldwise (~ 930 million) have experienced some form of sexual/physical violence (source: World Health Organisation, 2017).\nOne approach to tackle sexual harassment is to use a Crime Science framework. Crime science\n applies scientific method to the study of crime and security problems with the aim of reducing harm  (source: Cockbain, E., \u0026amp; Laycock, G. (2017). Crime Science. Oxford Research Encyclopedia of Criminology)\nHowever one issue with sexual harassment is that sexual harassment and violence are massively underreported:\n in England and Wales, 1 in 6 people who had experienced rape or assault by penetration (17%) had told the police  (source: Office of National Statistics)  in India, fewer than 1.5% of victims of sexual violence report their assaults to police  (source: McDougal, Krumholz, Bhan, Bharadwaj, \u0026amp; Raj, 2018)   This means that we struggle to gain information about the situational features associated with sexual harassment and violence.\nOne solution can be to make use of crowdsourced data, as I have done in previous projects looking at fix my street reports and spatial behaviour of those who report. In particular, there is an online platform called Safecity.\nhome screen of safecity.in\n Safecity is a platform that crowdsources personal stories of sexual harassment and abuse in public spaces. This data which maybe anonymous, gets aggregated as hot spots on a map indicating trends at a local level. The idea is to make this data useful for individuals, local communities and local administration to identify factors that causes behavior that leads to violence and work on strategies for solutions (source: Safecity).\nTo submit a report, you click on a map and find the location where the incident took place. Then you fill out a short form that asks for a title, a description, the time of the incident.\nform for submitting report\n These reports are then displayed on the website.\na screenshot of one report\n These reports are viewable one by one, but also, lucky for the potential data-hungry researcher, their URLs are sequential. What I mean is that, if one report is safectiy.in/.../report/12345 then the next ones are safectiy.in/.../report/12346 and safectiy.in/.../report/12347 etc. So, if we can write a script to open each page, and take the data we need, and then move on to the next one, we can iterate through each report, from first to last, to build a dataframe of these reports.\n Extracting the data we need So how to extract the data we need? Well as a first step we need to think about the variables of interest. One good starting point is the form that someone reporting an incident would have to fill out. We can see it has for example a ‘title’. Great so let’s get the title for each report. To do this, we will need to see what html ‘tags’ this title is demarcated by. So fir this, first view the page source by right clicking somewhere on the page, and selecting “View Page Source”. So here I have the source for the report http://maps.safecity.in/reports/view/11679\nview page source screenshot\n My approach here is to start searching for the tag in this source. For exaple, I can see that the tag here is \u0026quot;report-title\u0026quot;.\nscreenshot of report-title in source\n So I make a note of this, and any other tags that I will need to locate the variables that I want to scrape into my datasets.\nMy approach to selecting the variables required is to start by downloading the entire page. I do this with readLines(), which grabs all the lines into a list object. Here I use the url() function to get all the lines directly from the web page for which I have the url address. In this case that address is http://maps.safecity.in/reports/view/11679. Let’s grab all the lines for this into an object I will call all_lines.\nall_lines \u0026lt;- readLines(url(\u0026quot;http://maps.safecity.in/reports/view/11679\u0026quot;)) This all_lines object now contains all the lines of html that we could see earlier when we used the “View Source” option in our web browser. If interested we can print this to the console just to see…\nAnyway, usually we are not interested, instead we want to select the lines of interest. So for example, we want to select the line which has the title of the report that is displayed on this page. But how to find this line?\nOne approach is to use grepl(). This function uses pattern matching to return TRUE where a pattern is found in a string. For example, grepl(\u0026quot;a\u0026quot;, \u0026quot;abc\u0026quot;) returns TRUE, while grepl(\u0026quot;z\u0026quot;, \u0026quot;abc\u0026quot;) returns FALSE. So, using the tag we identified earlier, “report-title”, we can use grepl to find the line where it is present.\nWe can then use subsetting (here I’m using square brackets) and the which() function, which returns the TRUE indices of a logical object, allowing for array indices. For example, which(grepl(\u0026quot;a\u0026quot;, c(\u0026quot;abc\u0026quot;, \u0026quot;xyz\u0026quot;))) will return 1, and which(grepl(\u0026quot;z\u0026quot;, c(\u0026quot;abc\u0026quot;, \u0026quot;xyz\u0026quot;))) will return 2.\nGoing back to the case of extracting the title of the report from our webpage, we can create an object called title, and use subsetting and grepl() and which() to assign to it the line which has the “report-title” tag.\ntitle \u0026lt;- all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))] This is nice, but you can see it returns the entire line, html tags and all:\ntitle ## [1] \u0026quot;\\t\\t\u0026lt;h1 class=\\\u0026quot;report-title\\\u0026quot;\u0026gt;Stalking\u0026lt;/h1\u0026gt;\u0026quot; To clean out the HTML tags I make use of a function (obviously lifted from Stackoverflow):\ncleanFun \u0026lt;- function(htmlString) { return(gsub(\u0026quot;\u0026lt;.*?\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, htmlString)) } Unfortunately for me, this has not removed the tabs:\ncleanFun(title)  ## [1] \u0026quot;\\t\\tStalking\u0026quot; Now again, there are probably better ways to do this but I am too lazy to look this up so I use more pattern matching with the gsub() function to get rid of those:\ngsub(\u0026quot;\\\\t\u0026quot;, \u0026quot;\u0026quot;, cleanFun(title)) ## [1] \u0026quot;Stalking\u0026quot;  Using the above to build a dataframe Okay so that should give you an idea of how to extract a variable of interest from one page. We did this for title but you could do it again easily for other features of interest, such as the description, or the longitude/latitude for mapping. And more importantly, you want to do this for multiple reports, to append them all together into a dataframe.\nI mentioned before that the URLs for these reports are sequential, in that report 1234 is followed by report 1235, 1236, 1237, and so on. You may guess where I’m going with this: it is possible to write a loop that will repeat this extraction action for all the urls in a list. Again, I know for loops are evil in R, but this is where I’m at.\nSo first things first, I create an empty list to save all my dataframes into. I’ll call ic (creatively) datalist. I also need to create a counter j here. (Note: I only create it here because I don’t start my loop from 1. Mostly because this is a toy example. If I were getting all the reports (in fact when I got all the reports) I would build my loop with 1:number of reports, so I could simply use i to index my list of dataframes. )\ndatalist \u0026lt;- list() j \u0026lt;- 1 Now that I have these essential items, I write a loop, which will count from 11676 to 11679 to iterate through 4 reports (http://maps.safecity.in/reports/view/11676, http://maps.safecity.in/reports/view/11677, http://maps.safecity.in/reports/view/11678, http://maps.safecity.in/reports/view/11679) and for each one, repeat the steps discussed above to extract a title (and also duplicate for description), and save into my list object called datalist.\nfor (i in 11676:11679) { all_lines \u0026lt;- readLines(url(paste0(\u0026#39;http://maps.safecity.in/reports/view/\u0026#39;, i))) datalist[[j]] \u0026lt;- data.frame( title = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))])), description = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;Description\u0026quot;, all_lines)) + 1])) ) j \u0026lt;- j+1 } (Note: I also increase my index j there, again if you start from report 1, this is not necessary).\nWhen this is all finished, I am left with a list of dataframes, so my remaining task is to bind the list of data frames. Because I’ve been so hacky with everything I want to make up for it and inject some tidyverse into the mix, so let’s use the bind_rows() function from dplyr to do this.\nlibrary(dplyr) safecity_data \u0026lt;- bind_rows(datalist) Now we have a dataframe called safecity_data which we can have a look at here:\n  title description    TOUCHINGS The girl was being touched by her classmates who are boys on her buttocks.  TOUCHINGS A teacher is touching girls on their buttocks and canning their buttocks too.  Stalking A man kept following me.. It was scary.. He kept saying something  Stalking A man kept following me.. It was scary.. He kept saying something    As I mentioned, this is a toy example, but it should provide you with a good idea about how you can go about replicating this for more variables, and across more URLs. One thing I did not mention is error handling. It is likely that not all URLs will lead to a valid page, for example reports may get removed, or for other reasons. For such cases it is important that the code you run has a way to handle such errors. In my work I used tryCatch(), which worked excellently.\n Map and sample the reports Once you had all your variables (including spatial data such as Longitude and Latitude) and a sizeable data set, it is possible to put these reports on a map, and use spatial information to sample from these reports.\nThe first step to take for this is to make the data spatial. Currently, while the data may have a Longitude and Latitude column, these are not recognised as a geometry. To achieve this, you can use the sf package. Sf stands for simple features. Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them ( source: R Spatial). Check out Lovelace, R., Nowosad, J., \u0026amp; Muenchow, J. (2019). Geocomputation with R. CRC Press. for a great resource on all things spatial in R with sf.\nFor this example here, I’ve got a larger data set from my earlier webscraping work so let’s use the function st_as_sf() from the sf library to turn the long and lat columns into geometries:\nlibrary(sf) safecity_sf \u0026lt;- st_as_sf(safecity, coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;), crs = 4326) Having done this, we turn the flat dataframe into an sf object, and it becomes incredibly smooth to map the data, using our trusty old ggplot:\nlibrary(ggplot2) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + theme_bw() By turning our data into an sf spatial object, and using geom_sf() it becomes possible to plot our data as a ggplot. But the above isn’t giving us loads of context really. We can possibly guess that the blob of points is India, but unless we’re great geograpgers we may encounter trouble trying to guess where our out-of-India points are…\nOne way to quickly give some context and explore the background is to make use of the shapefiles in the rnaturalearth package, an R package to hold and facilitate interaction with Natural Earth map data. Read more about its usage here.\nThen, we can use the function ne_countries() to request a vector shapefile of all the country outlines across the world. In the parameters we specity that we want the resulting object to be of class sf, as well as set out the fill and outline colours.\nlibrary(rnaturalearth) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() This time we can see better that yes all our points are in India, but we seem to have some reports from the USA and the UK as well.\nWhich brings us to the sampling option. What if I wanted only those reports that were made in India, and to exclude the other ones? Well we can make use of the specific shapefile from the rnaturalearth package to subset our point list to only those which intersect with the India polygon. So let’s create a sf object for India:\nindia \u0026lt;- ne_states(country = \u0026#39;india\u0026#39;, returnclass = \u0026#39;sf\u0026#39;) To then select only reports made in India, we can use the st_intersects() function from the sf package, which will return TRUE for all points which intersect our polygon of interest. Then we can use that set of points labelled with TRUE to subset our original dataframe. Like so:\nindia_safecity_int \u0026lt;- st_intersects(india, safecity_sf) india_safecity_sf \u0026lt;- safecity_sf[unlist(india_safecity_int),] Now we can make sure we did everything right, and map our India only reports\nggplot() + geom_sf(data = india_safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() We now have a fantastic, spatially explicit data set of people’s experiences with victimisation from sexual harassment across India. It can be now used to perform other mapping exercises, and in my presentation I mentioned tmap for creating thematic maps smoothly, and sppt for running various spatial point pattern tests to compare different point sets.\n Wrapping up Overall I hope the above is useful as a bit of a guide into thinking about scraping some data from the web that may fill a gap in knowledge or understanding around a specific problem, which may help gain further insight and achieve good outcomes (possibly reduced prevalence of sexual harassment, or other societal ills). As I mentioned, there are probably other better/more efficient ways of doing this, but I thought I would share what I have been doing here.\nI am actually writing up a paper from the data, so I will share this later on here as well, for anyone interested. I will be presenting a version of this paper at the ASC so if anyone will be there then come see what we got up to with all these data!\nOn a final note, thank you to Anna Krystalli for inviting me to speak at R Sheffield, I really do recommend anyone in the area to attend this meetup, and if you have something to share to get in touch with Anna. I hope that I can attend another meetup soon, hopefully when the trains are back up and running so I can make up on that lost pint…!\n "
    }
,
    {
        "ref": "https://rekadata.site/blog/data-visualisation-summer-school/",
        "title": "Data Visualisation Summer School",
        "section": "blog",
        "date" : "2019.07.15",
        "body": " Learning to visualise data Last week I had the opportunity to take a one-week course by Andy Kirk about data visualisation, hosted by Methods at Manchester as part of the Summer School programme. It was a fantastic experience to be a student again, and I learned a lot about practical considerations that go into producing effective visualisations that are trustworthy, accessible, and elegant.\nI will not (and cannot) summarise the course here, I recomment anyone interested in creating data visuailisations to get in touch with Andy for courses. But I did want to quickly summarise some key take aways, list some really awesome tools, and show off some creations from the course.\n Take aways I really liked how the course was software agnostic, instead of teaching how to create particular plots with R or Tableu or something specific, the aim was to teach design thinking, and to take time to consider everything that goes into the data visualisation process.\nThe most useful thing for me to structure my approach to visualisation I think came from the 4-step breakdown of the visualisation process. It really made organising my thinking about how to get from question to visualisation easier. The steps are:\nFormulate a brief - why are you doing this? what/who is it for? what question is it answering? Work with the data - think about what data is required and also what format the data need to be in to allow the visualisation to be created Establish editorial thinking - really focus on what you want to say, to guide how to say it. we talked about the angle of the approach, and framing in relation to this. Develop design solution - finally think about the key decisions that go into the visualisation design: data representation, interactivity, annotation, colour, and composition, and how these all contribute to creating a trustworthy, elegant, and accessible piece.  Finally, many of the exercises involved looking at existing pieces of visualisation and really thinking about what I like and don’t like about them. This turned out to be a really good starting point to think about what is a good and not so good way to develop my own visualisations. Not only that, but using a google sheet to collect class responses to answers to “how much they like/dislike” some elements really got a discussion going and good engagement - something I might try in my teaching going forward.\n Resources Andy provided loads of great resources for people to use and refer back to when creating visualisation projects.\nOn his site visualisingdata.com there is a resources tab which lists a pretty much never-ending list of tools. Some which stood out to me where:\n D3.js - I feel like this is the ultimate visualisation tool and I’ve dabbled with it here and there but I cannot find the time to buckle down and get to grips with Javascript. Not that I haven’t tried, I do remember trying to start a book club going through Eloquent Javascript but it does require an inital time investment, and when the good people of open source are wrapping all these javascript libraries into R packages, then my main incentives are removed. But it is definitely on the todo list.\n R - R is so good for data visualisation, and so much easier to learn than Javascript (to me anyway) and also deals so well with all the data manipulation side of things, so it’s a 10/10 from me.\n Flourish - this tool has lots of pre-programmed charting options, and is free to use for public data. I think if you need to keep the data private it begins to cost though…\n RAWgraph - seemed to be this GUI for creating D3 visualisations. So if you’re interested in creating non-standard chart types, this look like a super easy way to do so, and is free.\n Gephi - useful for network visualisation \u0026amp; easy to use and also totally free.\n  In addition to these, during one of the group exercises our group discovered word art - no, not the late 90s MS Word 3D rainbow coloured clipart thing, but a tool to make a wordcloud take the shape of any image you want. I know, I know, word clouds are “the mulletts of the internet”, but we did use this to produce some neat visualisations of text, so hey, mulletts can be useful too. For our challenge visualising data about art collections, we looked at words used to describe representations of women in modern art and egyptian art:\nAnother useful tool was the Chartmaker Directory a crowdsourced collection of all the different charts you could think to use for your data, and a set of all the tools that you can make them in. And if there is something missing which you think should be there, you can submit for it to be added. Very useful tool to help you create your descired visualisation no matter what tool you use.\n Exercises The really neat part about being a student again was to explore cool and totally not-related-to-my-work data sets for visualisation. The first one of these was the scripts for the original Star Wars trilogies. The task was to think about what we want to visualise from the data and how. The dataset was split over 3 sheets in Excel (one for each film) and only had 3 variables, sequence (the lines numbered 1-n from first and last line spoken in each film), name of the character speaking, and the line which they said. Inspired by the New York Times visualisation about Peyton Manning’s Touchdowns I decided to see who speaks the most in the films. I used R, so I’ll include the code for the graph here too. I don’t know if the data is up for sharing, but you can easily find transcribed films online, so could reproduce with such data:\nlibrary(readxl) library(ggplot2) library(tidyr) library(dplyr) library(stringr) library(janitor) #read in each sheet, create a variable to tag film, and merge into one dataframe newhope \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeIV\u0026quot;) %\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) newhope$film \u0026lt;- \u0026quot;SW_Episode_IV\u0026quot; newhope$X__3 \u0026lt;- NULL empire \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeV\u0026quot;)%\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) empire$film \u0026lt;- \u0026quot;SW_Episode_V\u0026quot; jedi \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeVI\u0026quot;) %\u0026gt;% clean_names() %\u0026gt;% select(line_number, character, dialogue) jedi$film \u0026lt;- \u0026quot;SW_Episode_VI\u0026quot; all_sw \u0026lt;- rbind(newhope, empire) all_sw \u0026lt;- rbind(all_sw, jedi) #create new requence to paste together all 3 films all_sw$pos \u0026lt;- 1:nrow(all_sw) #create new variable that counts the number of words in each line all_sw$nwords \u0026lt;- sapply(strsplit(all_sw$dialogue, \u0026quot; \u0026quot;), length) #get cumulative words spoken at each line for all characters talk_vol \u0026lt;- all_sw %\u0026gt;% select(character, nwords, pos, line_number) test \u0026lt;- talk_vol %\u0026gt;% spread(character, nwords) %\u0026gt;% replace(is.na(.), 0) %\u0026gt;% gather(\u0026quot;who\u0026quot;, \u0026quot;num_chars\u0026quot;, -pos, -line_number) test$csum \u0026lt;- ave(test$num_chars, test$who, FUN=cumsum) #get the top 10 speakers to highlight them in the chart top10 \u0026lt;- test %\u0026gt;% group_by(who) %\u0026gt;% summarise(talks = max(csum)) %\u0026gt;% arrange(desc(talks)) %\u0026gt;% head(n = 10) %\u0026gt;% pull(who) #make points pts_test \u0026lt;- test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)) #plot ggplot() + geom_vline(xintercept = 1, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1011, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1850, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_text(aes(x=1, label=\u0026quot;New Hope\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1011, label=\u0026quot;Empire Strikes Back\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1850, label=\u0026quot;Return of the Jedi\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_line(data = test, aes(x = test$pos, y = test$csum, group = test$who), alpha = .4) + geom_line(data = test %\u0026gt;% filter(who %in% top10), aes(x = pos, y = csum, colour = who)) + geom_point(data = test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, colour = who)) + geom_text(data = test %\u0026gt;% filter(who %in% top10 \u0026amp; who != \u0026quot;BEN\u0026quot; ) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust=0.5, size = 4.5) + geom_text(data = test %\u0026gt;% filter(who == \u0026quot;BEN\u0026quot;) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust= 0, size = 4.5) + theme_minimal() + theme(legend.position=\u0026quot;none\u0026quot;, text = element_text(size = 16), axis.text.x=element_blank(), axis.ticks.x=element_blank(), plot.margin = unit(c(1,0.5,0,0.5), \u0026quot;lines\u0026quot;)) + labs(title=\u0026quot;Cumulative number of words spoken \\n by characters in original Star Wars trilogy\u0026quot;, x =\u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;) + xlim(c(0,3000)) + ylim(c(0,4700)) + scale_colour_brewer(palette = \u0026quot;Paired\u0026quot;) We also got to work with data from the Manchester Museum, Withworth Gallery, and Manchester Art Gallery. Together with another classmate, we used these data (which I am more sure I probably shouldn’t share, but I am sure that interested people could get in touch with the Manchester Museum Group to ask) to visualise gender representation in the Manchester Museum’s Egypt collection, and the Manchester Art Gallery. Our final product looked like this:\nMy colleague Vibhuti made the sanky diagram using Flourish mentioned above, and represents the types of artifacts that gods vs goddessess were represented with. I made the floor plan of the Manchester Art Gallery, with each room shaded by the proportion of paintings painted by male v female artists using the waffle package and the gridExtra package in R. We assembled everything in MS Publisher. Overall it was good fun and we got to present the results to members of the Manchester Museums Group, so very useful.\n A note on accessibility There was a lot of talk about accessibility of charts and this was I think a really important thing to always keep in mind. We discussed accessibility as in is the chart usable, is it suitably understandable, and therefore accessible to the audience but also discussed accessibility in terms of considering colourblind users for example. Some resources for this:\n colororacle.org gives a way to check if your colour scheme is colourblind colorbrewer has colourbling friendly pallette suggestions and in our final project we used coolors.co which generates a colour palette for you, and allows you to check if its colourblind friendly with different types of colourblindess simulated.  One thing we didn’t talk about (and I appreciate may be out of scope for the course for now) is accessibility of visualisations for those people with visual impairments who would use for example a screen reader to interpret our charts. It would be interesting to learn more about this, and if anyone knows some best practice on making charts even more accessible, I would welcome any tips and links to resources.\n "
    }
]
