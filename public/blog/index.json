[
    {
        "ref": "/blog/webscraping-and-some-sampling-mapping-in-r/",
        "title": "Webscraping and some sampling \u0026 mapping in R",
        "section": "blog",
        "date" : "2019.08.10",
        "body": " Presenting at R Sheffield Last week I had the pleasure to present at the Sheffield R User Group alongside former PhD colleague (and roommate) Joanna Hill who is currently based in Uganda, working remotely at Rutgers University. It was a great event, set in a meeting room upstairs in The Red Deer pub, which gave it a nice informal feel. The attendees were all welcoming, knowedgeable, and very engaged and engaging, so it was a great experience and I recommend to anyone in the area. The one negative was that trains weren’t running due to floods so I had to drive and therefore not fully immerse the presenting-in-a-pub-with-a-pint aesthetic. Oh well, there is always next time, and at least we got to see some nice peak district views.\nPeak views (photo by Jo Hill)\n  !!!DISCLAIMER!!! I am no expert on scaping data from the web. In that I really only engage with this problem when I find an interesting source of data, then scarpe this, and then abandon it forever. So DISCLAIMER: there are probably much better, more effective/efficient ways of doing this.\nIn fact someone in the meetup mentioned rvest. I haven’t looked into it yet but an initial look is already super exciting, and I urge anyone interested in webscrping to check that out, as it’s probably loads more useful than my hack-together approach.\nThat said, I do use my hacky approach to get myself exciting data sets sometimes, such as data from FixMyStreet, and people have asked me before about how I do this, so I thought this could be a good chance to share that. So what follows is a cliffnote of the talk:\n Finding some data about experiences of sexual harassment Sexual harassment is a widespread global issue. 75% of all women worldwide (~ 2 billion) have experienced sexual harassment. 35% of all women worldwise (~ 930 million) have experienced some form of sexual/physical violence (source: World Health Organisation, 2017).\nOne approach to tackle sexual harassment is to use a Crime Science framework. Crime science\n applies scientific method to the study of crime and security problems with the aim of reducing harm  (source: Cockbain, E., \u0026amp; Laycock, G. (2017). Crime Science. Oxford Research Encyclopedia of Criminology)\nHowever one issue with sexual harassment is that sexual harassment and violence are massively underreported:\n in England and Wales, 1 in 6 people who had experienced rape or assault by penetration (17%) had told the police  (source: Office of National Statistics)  in India, fewer than 1.5% of victims of sexual violence report their assaults to police  (source: McDougal, Krumholz, Bhan, Bharadwaj, \u0026amp; Raj, 2018)   This means that we struggle to gain information about the situational features associated with sexual harassment and violence.\nOne solution can be to make use of crowdsourced data, as I have done in previous projects looking at fix my street reports and spatial behaviour of those who report. In particular, there is an online platform called Safecity.\nhome screen of safecity.in\n Safecity is a platform that crowdsources personal stories of sexual harassment and abuse in public spaces. This data which maybe anonymous, gets aggregated as hot spots on a map indicating trends at a local level. The idea is to make this data useful for individuals, local communities and local administration to identify factors that causes behavior that leads to violence and work on strategies for solutions (source: Safecity).\nTo submit a report, you click on a map and find the location where the incident took place. Then you fill out a short form that asks for a title, a description, the time of the incident.\nform for submitting report\n These reports are then displayed on the website.\na screenshot of one report\n These reports are viewable one by one, but also, lucky for the potential data-hungry researcher, their URLs are sequential. What I mean is that, if one report is safectiy.in/.../report/12345 then the next ones are safectiy.in/.../report/12346 and safectiy.in/.../report/12347 etc. So, if we can write a script to open each page, and take the data we need, and then move on to the next one, we can iterate through each report, from first to last, to build a dataframe of these reports.\n Extracting the data we need So how to extract the data we need? Well as a first step we need to think about the variables of interest. One good starting point is the form that someone reporting an incident would have to fill out. We can see it has for example a ‘title’. Great so let’s get the title for each report. To do this, we will need to see what html ‘tags’ this title is demarcated by. So fir this, first view the page source by right clicking somewhere on the page, and selecting “View Page Source”. So here I have the source for the report http://maps.safecity.in/reports/view/11679\nview page source screenshot\n My approach here is to start searching for the tag in this source. For exaple, I can see that the tag here is \u0026quot;report-title\u0026quot;.\nscreenshot of report-title in source\n So I make a note of this, and any other tags that I will need to locate the variables that I want to scrape into my datasets.\nMy approach to selecting the variables required is to start by downloading the entire page. I do this with readLines(), which grabs all the lines into a list object. Here I use the url() function to get all the lines directly from the web page for which I have the url address. In this case that address is http://maps.safecity.in/reports/view/11679. Let’s grab all the lines for this into an object I will call all_lines.\nall_lines \u0026lt;- readLines(url(\u0026quot;http://maps.safecity.in/reports/view/11679\u0026quot;)) This all_lines object now contains all the lines of html that we could see earlier when we used the “View Source” option in our web browser. If interested we can print this to the console just to see…\nAnyway, usually we are not interested, instead we want to select the lines of interest. So for example, we want to select the line which has the title of the report that is displayed on this page. But how to find this line?\nOne approach is to use grepl(). This function uses pattern matching to return TRUE where a pattern is found in a string. For example, grepl(\u0026quot;a\u0026quot;, \u0026quot;abc\u0026quot;) returns TRUE, while grepl(\u0026quot;z\u0026quot;, \u0026quot;abc\u0026quot;) returns FALSE. So, using the tag we identified earlier, “report-title”, we can use grepl to find the line where it is present.\nWe can then use subsetting (here I’m using square brackets) and the which() function, which returns the TRUE indices of a logical object, allowing for array indices. For example, which(grepl(\u0026quot;a\u0026quot;, c(\u0026quot;abc\u0026quot;, \u0026quot;xyz\u0026quot;))) will return 1, and which(grepl(\u0026quot;z\u0026quot;, c(\u0026quot;abc\u0026quot;, \u0026quot;xyz\u0026quot;))) will return 2.\nGoing back to the case of extracting the title of the report from our webpage, we can create an object called title, and use subsetting and grepl() and which() to assign to it the line which has the “report-title” tag.\ntitle \u0026lt;- all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))] This is nice, but you can see it returns the entire line, html tags and all:\ntitle ## [1] \u0026quot;\\t\\t\u0026lt;h1 class=\\\u0026quot;report-title\\\u0026quot;\u0026gt;Stalking\u0026lt;/h1\u0026gt;\u0026quot; To clean out the HTML tags I make use of a function (obviously lifted from Stackoverflow):\ncleanFun \u0026lt;- function(htmlString) { return(gsub(\u0026quot;\u0026lt;.*?\u0026gt;\u0026quot;, \u0026quot;\u0026quot;, htmlString)) } Unfortunately for me, this has not removed the tabs:\ncleanFun(title)  ## [1] \u0026quot;\\t\\tStalking\u0026quot; Now again, there are probably better ways to do this but I am too lazy to look this up so I use more pattern matching with the gsub() function to get rid of those:\ngsub(\u0026quot;\\\\t\u0026quot;, \u0026quot;\u0026quot;, cleanFun(title)) ## [1] \u0026quot;Stalking\u0026quot;  Using the above to build a dataframe Okay so that should give you an idea of how to extract a variable of interest from one page. We did this for title but you could do it again easily for other features of interest, such as the description, or the longitude/latitude for mapping. And more importantly, you want to do this for multiple reports, to append them all together into a dataframe.\nI mentioned before that the URLs for these reports are sequential, in that report 1234 is followed by report 1235, 1236, 1237, and so on. You may guess where I’m going with this: it is possible to write a loop that will repeat this extraction action for all the urls in a list. Again, I know for loops are evil in R, but this is where I’m at.\nSo first things first, I create an empty list to save all my dataframes into. I’ll call ic (creatively) datalist. I also need to create a counter j here. (Note: I only create it here because I don’t start my loop from 1. Mostly because this is a toy example. If I were getting all the reports (in fact when I got all the reports) I would build my loop with 1:number of reports, so I could simply use i to index my list of dataframes. )\ndatalist \u0026lt;- list() j \u0026lt;- 1 Now that I have these essential items, I write a loop, which will count from 11676 to 11679 to iterate through 4 reports (http://maps.safecity.in/reports/view/11676, http://maps.safecity.in/reports/view/11677, http://maps.safecity.in/reports/view/11678, http://maps.safecity.in/reports/view/11679) and for each one, repeat the steps discussed above to extract a title (and also duplicate for description), and save into my list object called datalist.\nfor (i in 11676:11679) { all_lines \u0026lt;- readLines(url(paste0(\u0026#39;http://maps.safecity.in/reports/view/\u0026#39;, i))) datalist[[j]] \u0026lt;- data.frame( title = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;report-title\u0026quot;, all_lines))])), description = gsub(\u0026quot;\\\\t\u0026quot;,\u0026quot;\u0026quot;,cleanFun(all_lines[which(grepl(\u0026quot;Description\u0026quot;, all_lines)) + 1])) ) j \u0026lt;- j+1 } (Note: I also increase my index j there, again if you start from report 1, this is not necessary).\nWhen this is all finished, I am left with a list of dataframes, so my remaining task is to bind the list of data frames. Because I’ve been so hacky with everything I want to make up for it and inject some tidyverse into the mix, so let’s use the bind_rows() function from dplyr to do this.\nlibrary(dplyr) safecity_data \u0026lt;- bind_rows(datalist) Now we have a dataframe called safecity_data which we can have a look at here:\n  title description    TOUCHINGS The girl was being touched by her classmates who are boys on her buttocks.  TOUCHINGS A teacher is touching girls on their buttocks and canning their buttocks too.  Stalking A man kept following me.. It was scary.. He kept saying something  Stalking A man kept following me.. It was scary.. He kept saying something    As I mentioned, this is a toy example, but it should provide you with a good idea about how you can go about replicating this for more variables, and across more URLs. One thing I did not mention is error handling. It is likely that not all URLs will lead to a valid page, for example reports may get removed, or for other reasons. For such cases it is important that the code you run has a way to handle such errors. In my work I used tryCatch(), which worked excellently.\n Map and sample the reports Once you had all your variables (including spatial data such as Longitude and Latitude) and a sizeable data set, it is possible to put these reports on a map, and use spatial information to sample from these reports.\nThe first step to take for this is to make the data spatial. Currently, while the data may have a Longitude and Latitude column, these are not recognised as a geometry. To achieve this, you can use the sf package. Sf stands for simple features. Simple features or simple feature access refers to a formal standard (ISO 19125-1:2004) that describes how objects in the real world can be represented in computers, with emphasis on the spatial geometry of these objects. It also describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them ( source: R Spatial). Check out Lovelace, R., Nowosad, J., \u0026amp; Muenchow, J. (2019). Geocomputation with R. CRC Press. for a great resource on all things spatial in R with sf.\nFor this example here, I’ve got a larger data set from my earlier webscraping work so let’s use the function st_as_sf() from the sf library to turn the long and lat columns into geometries:\nlibrary(sf) safecity_sf \u0026lt;- st_as_sf(safecity, coords = c(\u0026quot;longitude\u0026quot;, \u0026quot;latitude\u0026quot;), crs = 4326) Having done this, we turn the flat dataframe into an sf object, and it becomes incredibly smooth to map the data, using our trusty old ggplot:\nlibrary(ggplot2) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + theme_bw() By turning our data into an sf spatial object, and using geom_sf() it becomes possible to plot our data as a ggplot. But the above isn’t giving us loads of context really. We can possibly guess that the blob of points is India, but unless we’re great geograpgers we may encounter trouble trying to guess where our out-of-India points are…\nOne way to quickly give some context and explore the background is to make use of the shapefiles in the rnaturalearth package, an R package to hold and facilitate interaction with Natural Earth map data. Read more about its usage here.\nThen, we can use the function ne_countries() to request a vector shapefile of all the country outlines across the world. In the parameters we specity that we want the resulting object to be of class sf, as well as set out the fill and outline colours.\nlibrary(rnaturalearth) ggplot() + geom_sf(data = safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() This time we can see better that yes all our points are in India, but we seem to have some reports from the USA and the UK as well.\nWhich brings us to the sampling option. What if I wanted only those reports that were made in India, and to exclude the other ones? Well we can make use of the specific shapefile from the rnaturalearth package to subset our point list to only those which intersect with the India polygon. So let’s create a sf object for India:\nindia \u0026lt;- ne_states(country = \u0026#39;india\u0026#39;, returnclass = \u0026#39;sf\u0026#39;) To then select only reports made in India, we can use the st_intersects() function from the sf package, which will return TRUE for all points which intersect our polygon of interest. Then we can use that set of points labelled with TRUE to subset our original dataframe. Like so:\nindia_safecity_int \u0026lt;- st_intersects(india, safecity_sf) india_safecity_sf \u0026lt;- safecity_sf[unlist(india_safecity_int),] Now we can make sure we did everything right, and map our India only reports\nggplot() + geom_sf(data = india_safecity_sf, colour = \u0026#39;blue\u0026#39;) + geom_sf(data = ne_countries(returnclass = \u0026#39;sf\u0026#39;), fill = \u0026#39;transparent\u0026#39;, colour = \u0026#39;black\u0026#39;) + theme_bw() We now have a fantastic, spatially explicit data set of people’s experiences with victimisation from sexual harassment across India. It can be now used to perform other mapping exercises, and in my presentation I mentioned tmap for creating thematic maps smoothly, and sppt for running various spatial point pattern tests to compare different point sets.\n Wrapping up Overall I hope the above is useful as a bit of a guide into thinking about scraping some data from the web that may fill a gap in knowledge or understanding around a specific problem, which may help gain further insight and achieve good outcomes (possibly reduced prevalence of sexual harassment, or other societal ills). As I mentioned, there are probably other better/more efficient ways of doing this, but I thought I would share what I have been doing here.\nI am actually writing up a paper from the data, so I will share this later on here as well, for anyone interested. I will be presenting a version of this paper at the ASC so if anyone will be there then come see what we got up to with all these data!\nOn a final note, thank you to Anna Krystalli for inviting me to speak at R Sheffield, I really do recommend anyone in the area to attend this meetup, and if you have something to share to get in touch with Anna. I hope that I can attend another meetup soon, hopefully when the trains are back up and running so I can make up on that lost pint…!\n "
    }
,
    {
        "ref": "/blog/data-visualisation-summer-school/",
        "title": "Data Visualisation Summer School",
        "section": "blog",
        "date" : "2019.07.15",
        "body": " Learning to visualise data Last week I had the opportunity to take a one-week course by Andy Kirk about data visualisation, hosted by Methods at Manchester as part of the Summer School programme. It was a fantastic experience to be a student again, and I learned a lot about practical considerations that go into producing effective visualisations that are trustworthy, accessible, and elegant.\nI will not (and cannot) summarise the course here, I recomment anyone interested in creating data visuailisations to get in touch with Andy for courses. But I did want to quickly summarise some key take aways, list some really awesome tools, and show off some creations from the course.\n Take aways I really liked how the course was software agnostic, instead of teaching how to create particular plots with R or Tableu or something specific, the aim was to teach design thinking, and to take time to consider everything that goes into the data visualisation process.\nThe most useful thing for me to structure my approach to visualisation I think came from the 4-step breakdown of the visualisation process. It really made organising my thinking about how to get from question to visualisation easier. The steps are:\nFormulate a brief - why are you doing this? what/who is it for? what question is it answering? Work with the data - think about what data is required and also what format the data need to be in to allow the visualisation to be created Establish editorial thinking - really focus on what you want to say, to guide how to say it. we talked about the angle of the approach, and framing in relation to this. Develop design solution - finally think about the key decisions that go into the visualisation design: data representation, interactivity, annotation, colour, and composition, and how these all contribute to creating a trustworthy, elegant, and accessible piece.  Finally, many of the exercises involved looking at existing pieces of visualisation and really thinking about what I like and don’t like about them. This turned out to be a really good starting point to think about what is a good and not so good way to develop my own visualisations. Not only that, but using a google sheet to collect class responses to answers to “how much they like/dislike” some elements really got a discussion going and good engagement - something I might try in my teaching going forward.\n Resources Andy provided loads of great resources for people to use and refer back to when creating visualisation projects.\nOn his site visualisingdata.com there is a resources tab which lists a pretty much never-ending list of tools. Some which stood out to me where:\n D3.js - I feel like this is the ultimate visualisation tool and I’ve dabbled with it here and there but I cannot find the time to buckle down and get to grips with Javascript. Not that I haven’t tried, I do remember trying to start a book club going through Eloquent Javascript but it does require an inital time investment, and when the good people of open source are wrapping all these javascript libraries into R packages, then my main incentives are removed. But it is definitely on the todo list.\n R - R is so good for data visualisation, and so much easier to learn than Javascript (to me anyway) and also deals so well with all the data manipulation side of things, so it’s a 10/10 from me.\n Flourish - this tool has lots of pre-programmed charting options, and is free to use for public data. I think if you need to keep the data private it begins to cost though…\n RAWgraph - seemed to be this GUI for creating D3 visualisations. So if you’re interested in creating non-standard chart types, this look like a super easy way to do so, and is free.\n Gephi - useful for network visualisation \u0026amp; easy to use and also totally free.\n  In addition to these, during one of the group exercises our group discovered word art - no, not the late 90s MS Word 3D rainbow coloured clipart thing, but a tool to make a wordcloud take the shape of any image you want. I know, I know, word clouds are “the mulletts of the internet”, but we did use this to produce some neat visualisations of text, so hey, mulletts can be useful too. For our challenge visualising data about art collections, we looked at words used to describe representations of women in modern art and egyptian art:\nAnother useful tool was the Chartmaker Directory a crowdsourced collection of all the different charts you could think to use for your data, and a set of all the tools that you can make them in. And if there is something missing which you think should be there, you can submit for it to be added. Very useful tool to help you create your descired visualisation no matter what tool you use.\n Exercises The really neat part about being a student again was to explore cool and totally not-related-to-my-work data sets for visualisation. The first one of these was the scripts for the original Star Wars trilogies. The task was to think about what we want to visualise from the data and how. The dataset was split over 3 sheets in Excel (one for each film) and only had 3 variables, sequence (the lines numbered 1-n from first and last line spoken in each film), name of the character speaking, and the line which they said. Inspired by the New York Times visualisation about Peyton Manning’s Touchdowns I decided to see who speaks the most in the films. I used R, so I’ll include the code for the graph here too. I don’t know if the data is up for sharing, but you can easily find transcribed films online, so could reproduce with such data:\nlibrary(readxl) library(ggplot2) library(tidyr) library(dplyr) library(stringr) #read in each sheet, create a variable to tag film, and merge into one dataframe newhope \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeIV\u0026quot;) newhope$film \u0026lt;- \u0026quot;SW_Episode_IV\u0026quot; newhope$X__3 \u0026lt;- NULL empire \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeV\u0026quot;) empire$film \u0026lt;- \u0026quot;SW_Episode_V\u0026quot; jedi \u0026lt;- read_excel(\u0026quot;data/2.OriginalStarWarsScripts.xlsx\u0026quot;, sheet = \u0026quot;SW_EpisodeVI\u0026quot;) jedi$film \u0026lt;- \u0026quot;SW_Episode_VI\u0026quot; all_sw \u0026lt;- rbind(newhope, empire) all_sw \u0026lt;- rbind(all_sw, jedi) #create new requence to paste together all 3 films all_sw$pos \u0026lt;- 1:nrow(all_sw) #create new variable that counts the number of words in each line all_sw$nwords \u0026lt;- sapply(strsplit(all_sw$Dialogue, \u0026quot; \u0026quot;), length) #get cumulative words spoken at each line for all characters talk_vol \u0026lt;- all_sw %\u0026gt;% select(Character, nwords, pos, `Line Number`) test \u0026lt;- talk_vol %\u0026gt;% spread(Character, nwords) %\u0026gt;% replace(is.na(.), 0) %\u0026gt;% gather(\u0026quot;who\u0026quot;, \u0026quot;num_chars\u0026quot;, -pos, -`Line Number`) test$csum \u0026lt;- ave(test$num_chars, test$who, FUN=cumsum) #get the top 10 speakers to highlight them in the chart top10 \u0026lt;- test %\u0026gt;% group_by(who) %\u0026gt;% summarise(talks = max(csum)) %\u0026gt;% arrange(desc(talks)) %\u0026gt;% head(n = 10) %\u0026gt;% pull(who) #make points pts_test \u0026lt;- test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)) #plot ggplot() + geom_vline(xintercept = 1, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1011, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_vline(xintercept = 1850, colour=\u0026quot;#A9A9A9\u0026quot;, linetype=\u0026quot;dashed\u0026quot;) + geom_text(aes(x=1, label=\u0026quot;New Hope\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1011, label=\u0026quot;Empire Strikes Back\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_text(aes(x=1850, label=\u0026quot;Return of the Jedi\u0026quot;, y=4600), colour=\u0026quot;#A9A9A9\u0026quot;, hjust = -0.1) + geom_line(data = test, aes(x = test$pos, y = test$csum, group = test$who), alpha = .4) + geom_line(data = test %\u0026gt;% filter(who %in% top10), aes(x = pos, y = csum, colour = who)) + geom_point(data = test %\u0026gt;% filter(who %in% top10) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, colour = who)) + geom_text(data = test %\u0026gt;% filter(who %in% top10 \u0026amp; who != \u0026quot;BEN\u0026quot; ) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust=0.5, size = 4.5) + geom_text(data = test %\u0026gt;% filter(who == \u0026quot;BEN\u0026quot;) %\u0026gt;% group_by(who) %\u0026gt;% summarise(max_char = max(csum), max_pos = max(pos)), aes(x = max_pos, y = max_char, label=str_to_title(who), colour = who),hjust= -0.1, vjust= 0, size = 4.5) + theme_minimal() + theme(legend.position=\u0026quot;none\u0026quot;, text = element_text(size = 16), axis.text.x=element_blank(), axis.ticks.x=element_blank(), plot.margin = unit(c(1,0.5,0,0.5), \u0026quot;lines\u0026quot;)) + labs(title=\u0026quot;Cumulative number of words spoken \\n by characters in original Star Wars trilogy\u0026quot;, x =\u0026quot;\u0026quot;, y = \u0026quot;\u0026quot;) + xlim(c(0,3000)) + ylim(c(0,4700)) + scale_colour_brewer(palette = \u0026quot;Paired\u0026quot;) We also got to work with data from the Manchester Museum, Withworth Gallery, and Manchester Art Gallery. Together with another classmate, we used these data (which I am more sure I probably shouldn’t share, but I am sure that interested people could get in touch with the Manchester Museum Group to ask) to visualise gender representation in the Manchester Museum’s Egypt collection, and the Manchester Art Gallery. Our final product looked like this:\nMy colleague Vibhuti made the sanky diagram using Flourish mentioned above, and represents the types of artifacts that gods vs goddessess were represented with. I made the floor plan of the Manchester Art Gallery, with each room shaded by the proportion of paintings painted by male v female artists using the waffle package and the gridExtra package in R. We assembled everything in MS Publisher. Overall it was good fun and we got to present the results to members of the Manchester Museums Group, so very useful.\n A note on accessibility There was a lot of talk about accessibility of charts and this was I think a really important thing to always keep in mind. We discussed accessibility as in is the chart usable, is it suitably understandable, and therefore accessible to the audience but also discussed accessibility in terms of considering colourblind users for example. Some resources for this:\n colororacle.org gives a way to check if your colour scheme is colourblind colorbrewer has colourbling friendly pallette suggestions and in our final project we used coolors.co which generates a colour palette for you, and allows you to check if its colourblind friendly with different types of colourblindess simulated.  One thing we didn’t talk about (and I appreciate may be out of scope for the course for now) is accessibility of visualisations for those people with visual impairments who would use for example a screen reader to interpret our charts. It would be interesting to learn more about this, and if anyone knows some best practice on making charts even more accessible, I would welcome any tips and links to resources.\n "
    }
]
